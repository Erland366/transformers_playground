{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "sdtDsu1Y0EqL"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/vast/users/qirong.ho/miniforge3/envs/flame/lib/python3.11/site-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'repr' attribute with value False was provided to the `Field()` function, which has no effect in the context it was used. 'repr' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n",
      "  warnings.warn(\n",
      "/vast/users/qirong.ho/miniforge3/envs/flame/lib/python3.11/site-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'frozen' attribute with value True was provided to the `Field()` function, which has no effect in the context it was used. 'frozen' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n",
      "  warnings.warn(\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /vast/users/qirong.ho/.netrc\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33merlandpg\u001b[0m to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wandb initialized successfully\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import math\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "from torch.nn.attention.flex_attention import flex_attention, create_block_mask\n",
    "import triton\n",
    "import triton.language as tl\n",
    "from tqdm.notebook import tqdm\n",
    "from dotenv import load_dotenv\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython import display\n",
    "import wandb\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "use_wandb = True # set to False to disable wandb logging\n",
    "try:\n",
    "    import wandb\n",
    "    wandb_api_key = os.getenv('WANDB_API_KEY')\n",
    "    if wandb_api_key:  \n",
    "        wandb.login(key=wandb_api_key)\n",
    "        use_wandb = use_wandb and True\n",
    "        print(\"wandb initialized successfully\")\n",
    "    else:\n",
    "        print(\"WANDB_API_KEY not found - wandb logging disabled\")\n",
    "except ImportError:\n",
    "    print(\"wandb not installed - wandb logging disabled\")\n",
    "    wandb = None\n",
    "\n",
    "torch.manual_seed(69)\n",
    "torch.set_printoptions(profile=\"short\", sci_mode=False, linewidth=100000)\n",
    "torch.set_float32_matmul_precision('high')\n",
    "# this script is configured to run on a RTX 3060 12GB GPU. you'll want to adjust the model sizes and batch sizes for other devices\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# device = torch.device('cpu')\n",
    "plt.rcParams['figure.figsize'] = [8, 6]\n",
    "plt.rcParams['figure.dpi'] = 50\n",
    "plt.rcParams['axes.grid'] = True\n",
    "plt.rcParams['xtick.minor.visible'] = True\n",
    "plt.rcParams['ytick.minor.visible'] = True\n",
    "\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ANr5dn7W0EqR",
    "outputId": "a00cbe8d-e1c6-454b-b552-4ffd17ce17e5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of dataset in characters:  39526018\n"
     ]
    }
   ],
   "source": [
    "# we use this 40mb file of concatenated anime subtitles as our dataset\n",
    "# just the right size for toy experiments like this I think\n",
    "with open('animesubs.txt', 'r', encoding='latin') as f:\n",
    "    text = f.read()\n",
    "print(\"length of dataset in characters: \", len(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pANiObIZ0EqU",
    "outputId": "98f70469-1c89-4341-89e8-c142a14331b1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Open your mind. Open your mind.\n",
      "\n",
      "Far beyond the deep blue Earth, you and I shall meet...\n",
      "\n",
      "AH! MY GODDESS\n",
      "\n",
      "A snow-white feather comes fluttering down, swaying gently in the air.\n",
      "\n",
      "Without holding back, I want to envelope you, my one and only love.\n",
      "\n",
      "I know I have the power to protect the one I love, right here in my hands.\n",
      "\n",
      "Open your mind. Just as I've always dreamed.\n",
      "\n",
      "Let the wind carry off your hopes, faraway.\n",
      "\n",
      "I have wings nobody can see. Look, you have them, too.\n",
      "\n",
      "They'll take us to where we ca\n"
     ]
    }
   ],
   "source": [
    "print(text[:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "WvM5h6_i3KsM"
   },
   "outputs": [],
   "source": [
    "# remove japanese characters\n",
    "text = ''.join(filter(lambda character:ord(character) < 0x3000, text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "U7SOcWJM0EqW",
    "outputId": "a82a4da8-a8ed-47bf-cfb5-2e4c59dee2cf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unique characters: 86 \n",
      " !'()*+,-./0123456789:;<=>?@ABCDEFGHIJKLMNOPQRSTUVWXYZ[]_abcdefghijklmnopqrstuvwxyz|\n"
     ]
    }
   ],
   "source": [
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "print(\"unique characters:\", vocab_size, ''.join(chars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yobmmaeK0EqX",
    "outputId": "26669f38-4b26-4b53-f642-ee7543e7c578"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'\\n': 0, ' ': 1, '!': 2, \"'\": 3, '(': 4, ')': 5, '*': 6, '+': 7, ',': 8, '-': 9, '.': 10, '/': 11, '0': 12, '1': 13, '2': 14, '3': 15, '4': 16, '5': 17, '6': 18, '7': 19, '8': 20, '9': 21, ':': 22, ';': 23, '<': 24, '=': 25, '>': 26, '?': 27, '@': 28, 'A': 29, 'B': 30, 'C': 31, 'D': 32, 'E': 33, 'F': 34, 'G': 35, 'H': 36, 'I': 37, 'J': 38, 'K': 39, 'L': 40, 'M': 41, 'N': 42, 'O': 43, 'P': 44, 'Q': 45, 'R': 46, 'S': 47, 'T': 48, 'U': 49, 'V': 50, 'W': 51, 'X': 52, 'Y': 53, 'Z': 54, '[': 55, ']': 56, '_': 57, 'a': 58, 'b': 59, 'c': 60, 'd': 61, 'e': 62, 'f': 63, 'g': 64, 'h': 65, 'i': 66, 'j': 67, 'k': 68, 'l': 69, 'm': 70, 'n': 71, 'o': 72, 'p': 73, 'q': 74, 'r': 75, 's': 76, 't': 77, 'u': 78, 'v': 79, 'w': 80, 'x': 81, 'y': 82, 'z': 83, '|': 84, '\\x94': 85, '': 86}\n",
      "{0: '\\n', 1: ' ', 2: '!', 3: \"'\", 4: '(', 5: ')', 6: '*', 7: '+', 8: ',', 9: '-', 10: '.', 11: '/', 12: '0', 13: '1', 14: '2', 15: '3', 16: '4', 17: '5', 18: '6', 19: '7', 20: '8', 21: '9', 22: ':', 23: ';', 24: '<', 25: '=', 26: '>', 27: '?', 28: '@', 29: 'A', 30: 'B', 31: 'C', 32: 'D', 33: 'E', 34: 'F', 35: 'G', 36: 'H', 37: 'I', 38: 'J', 39: 'K', 40: 'L', 41: 'M', 42: 'N', 43: 'O', 44: 'P', 45: 'Q', 46: 'R', 47: 'S', 48: 'T', 49: 'U', 50: 'V', 51: 'W', 52: 'X', 53: 'Y', 54: 'Z', 55: '[', 56: ']', 57: '_', 58: 'a', 59: 'b', 60: 'c', 61: 'd', 62: 'e', 63: 'f', 64: 'g', 65: 'h', 66: 'i', 67: 'j', 68: 'k', 69: 'l', 70: 'm', 71: 'n', 72: 'o', 73: 'p', 74: 'q', 75: 'r', 76: 's', 77: 't', 78: 'u', 79: 'v', 80: 'w', 81: 'x', 82: 'y', 83: 'z', 84: '|', 85: '\\x94', 86: ''}\n",
      "encoded: [43, 73, 62, 71, 1, 82, 72, 78, 75, 1, 70, 66, 71, 61, 10, 1, 43, 73, 62, 71]\n",
      "decoded: Open your mind. Open\n",
      "vocab size: 87\n"
     ]
    }
   ],
   "source": [
    "# yes, all language models will be character level, which isn't ideal but it's good for simplicity\n",
    "# very simple tokenizer\n",
    "stoi = {ch:i for i,ch in enumerate(chars)}\n",
    "itos = {i:ch for i,ch in enumerate(chars)}\n",
    "# add special token for padding\n",
    "stoi[''] = len(stoi)\n",
    "itos[len(itos)] = ''\n",
    "print(stoi)\n",
    "print(itos)\n",
    "encode = lambda s: [stoi[ch] for ch in s]\n",
    "decode = lambda l: ''.join([itos[i] for i in l])\n",
    "print(\"encoded:\", encode(text[:20]))\n",
    "print(\"decoded:\", decode(encode(text[:20])))\n",
    "vocab_size = len(itos)\n",
    "print(\"vocab size:\", vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6Pnf9KfP0EqY",
    "outputId": "ff581168-335a-4f0f-efeb-0d4bd5b225ca"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([39526018])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = torch.tensor(encode(text), dtype=torch.int64)\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lZ2fY1pR0EqY",
    "outputId": "5eb599e0-fb79-4cdb-c4b9-52a781d67151"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([43, 73, 62, 71,  1, 82, 72, 78, 75,  1, 70, 66, 71, 61, 10,  1, 43, 73, 62, 71,  1, 82, 72, 78, 75,  1, 70, 66, 71, 61, 10,  0,  0, 34, 58, 75,  1, 59, 62, 82, 72, 71, 61,  1, 77, 65, 62,  1, 61, 62, 62, 73,  1, 59, 69, 78, 62,  1, 33, 58, 75, 77, 65,  8,  1, 82, 72, 78,  1, 58, 71, 61,  1, 37,  1, 76, 65, 58, 69, 69,  1, 70, 62, 62, 77, 10, 10, 10,  0,  0, 29, 36,  2,  1, 41, 53,  1, 35, 43, 32])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oIaYesPh0Eqa",
    "outputId": "caa27cbb-5ae3-4406-8194-646264d4ba99"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([39130757]) torch.Size([395261])\n"
     ]
    }
   ],
   "source": [
    "n = int(len(data) * 0.99)\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]\n",
    "print(train_data.shape, val_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4bFhizcI0Eqa",
    "outputId": "c93a4d43-6fb2-4de7-aefc-8fed47e4a861"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([43, 73, 62, 71,  1, 82, 72, 78, 75])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq_len = 8\n",
    "train_data[:seq_len+1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "skFCPvQC0Eqc",
    "outputId": "08990c5b-88af-4a51-ce37-3c59989d6d74"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs:\n",
      "torch.Size([2, 64])\n",
      "tensor([[64,  1, 72, 78, 75,  1, 66, 71, 77, 62, 75, 71, 76, 65, 66, 73, 76, 10,  0,  0, 48, 65, 62,  1, 71, 66, 64, 65, 77,  1, 37, 66, 61, 58,  1, 64, 72, 77,  1, 77, 65, 62,  1, 75, 62, 76, 78, 69, 77, 76,  1, 72, 63,  1, 77, 65, 62,  1, 62, 81, 58, 70, 66, 71],\n",
      "        [62, 70,  2,  0,  0, 37, 77,  1, 69, 72, 72, 68, 76,  1, 69, 66, 68, 62,  1, 80, 62,  1, 77, 72, 72, 68,  1, 60, 58, 75, 62,  1, 72, 63,  1, 77, 65, 62, 70, 10,  0,  0, 48, 65, 72, 76, 62,  1, 64, 78, 82, 76,  1, 80, 62, 75, 62,  1, 66, 71, 60, 75, 62, 61]], device='cuda:0')\n",
      "targets:\n",
      "torch.Size([2, 64])\n",
      "tensor([[ 1, 72, 78, 75,  1, 66, 71, 77, 62, 75, 71, 76, 65, 66, 73, 76, 10,  0,  0, 48, 65, 62,  1, 71, 66, 64, 65, 77,  1, 37, 66, 61, 58,  1, 64, 72, 77,  1, 77, 65, 62,  1, 75, 62, 76, 78, 69, 77, 76,  1, 72, 63,  1, 77, 65, 62,  1, 62, 81, 58, 70, 66, 71, 58],\n",
      "        [70,  2,  0,  0, 37, 77,  1, 69, 72, 72, 68, 76,  1, 69, 66, 68, 62,  1, 80, 62,  1, 77, 72, 72, 68,  1, 60, 58, 75, 62,  1, 72, 63,  1, 77, 65, 62, 70, 10,  0,  0, 48, 65, 72, 76, 62,  1, 64, 78, 82, 76,  1, 80, 62, 75, 62,  1, 66, 71, 60, 75, 62, 61, 66]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "def get_batch(split, seq_len, batch_size=4):\n",
    "    # generate a small batch of data of inputs x and targets y\n",
    "    # targets are just inputs shifted by 1\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - seq_len, (batch_size,))\n",
    "    x = torch.stack([data[i:i+seq_len] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+seq_len+1] for i in ix])\n",
    "    return x.to(device), y.to(device)\n",
    "\n",
    "xb, yb = get_batch('train', 64, 2)\n",
    "print('inputs:')\n",
    "print(xb.shape)\n",
    "print(xb)\n",
    "print('targets:')\n",
    "print(yb.shape)\n",
    "print(yb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "327680000"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Make all steps, sequence lengths, and batch size the same\n",
    "total_steps = 5000\n",
    "seq_len = 256\n",
    "batch_size = 256 # these are small models so we can use large batch sizes to fully utilize the GPU\n",
    "# should cover around 2x the dataset\n",
    "total_steps * seq_len * batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test forward pass\n",
    "torch_compile_options = {\n",
    "    'epilogue_fusion': True, \n",
    "    'max_autotune': False, \n",
    "    'shape_padding': True, \n",
    "    'trace.enabled': False, \n",
    "    'triton.cudagraphs': False, \n",
    "    'debug': False, \n",
    "    'dce': True, \n",
    "    'memory_planning': True, \n",
    "    'coordinate_descent_tuning': False, \n",
    "    'trace.graph_diagram': False, \n",
    "    'compile_threads': 32, \n",
    "    'group_fusion': True, \n",
    "    'disable_progress': False, \n",
    "    'verbose_progress': False, \n",
    "    'triton.multi_kernel': 0, \n",
    "    'triton.use_block_ptr': False, \n",
    "    'triton.enable_persistent_tma_matmul': False, \n",
    "    'triton.autotune_at_compile_time': False, \n",
    "    'triton.cooperative_reductions': False, \n",
    "    'cuda.compile_opt_level': '-O2', \n",
    "    'cuda.enable_cuda_lto': True, \n",
    "    'combo_kernels': True, \n",
    "    'benchmark_combo_kernel': True, \n",
    "    'combo_kernel_foreach_dynamic_shapes': True\n",
    "}\n",
    "\n",
    "@torch.compile(fullgraph=False, options=torch_compile_options)\n",
    "def compile_optimizer_lr(opt, scheduler):\n",
    "    opt.step()\n",
    "    scheduler.step()\n",
    "\n",
    "\n",
    "def save_checkpoint(model, optimizer, scheduler, step, losses, val_losses, seq_len, batch_size, total_steps, save_dir='checkpoints'):\n",
    "    \"\"\"Save a complete checkpoint including model, optimizer, scheduler states and training metrics\"\"\"\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    \n",
    "    checkpoint = {\n",
    "        'step': step,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'scheduler_state_dict': scheduler.state_dict(),\n",
    "        'train_losses': losses,\n",
    "        'val_losses': val_losses,\n",
    "        'config': {\n",
    "            'seq_len': seq_len,\n",
    "            'batch_size': batch_size,\n",
    "            'total_steps': total_steps,\n",
    "            'vocab_size': model.token_embedding_table.num_embeddings,\n",
    "            'embed_size': model.blocks[0].sa_heads.head_size * model.head_num,\n",
    "            'head_num': model.head_num,\n",
    "            'layer_num': model.layer_num\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    checkpoint_path = os.path.join(save_dir, f'checkpoint_step_{step}.pt')\n",
    "    torch.save(checkpoint, checkpoint_path)\n",
    "    wandb.save(checkpoint_path)\n",
    "    print(f\"Checkpoint saved at step {step}: {checkpoint_path}\")\n",
    "    \n",
    "    return checkpoint_path\n",
    "\n",
    "def load_checkpoint(model, optimizer, scheduler, checkpoint_path):\n",
    "    \"\"\"Load a checkpoint and restore model, optimizer, scheduler states\"\"\"\n",
    "    checkpoint = torch.load(checkpoint_path, map_location=device)\n",
    "    \n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n",
    "    \n",
    "    print(f\"Checkpoint loaded from step {checkpoint['step']}\")\n",
    "    return checkpoint['step'], checkpoint['train_losses'], checkpoint['val_losses']\n",
    "\n",
    "def train(model, optimizer, scheduler, seq_len, batch_size, total_steps, val_steps=10, val_interval=50, checkpoint_interval=500, save_dir='checkpoints'):\n",
    "    losses = []\n",
    "    val_losses = []\n",
    "    # Create save directory\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    \n",
    "    # live plot\n",
    "    fig, ax = plt.subplots()\n",
    "    dh = display.display(fig, display_id=True)\n",
    "    \n",
    "    for step in (bar := tqdm(range(total_steps))):\n",
    "        # sample a batch of data\n",
    "        xb, yb = get_batch('train', seq_len=seq_len, batch_size=batch_size)\n",
    "\n",
    "        # evaluate the loss\n",
    "        logits, loss = model(xb, yb)\n",
    "        # backprop\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "        with torch._dynamo.compiled_autograd._enable(torch.compile()):\n",
    "            loss.backward()\n",
    "        compile_optimizer_lr(optimizer, scheduler)\n",
    "\n",
    "        bar.set_description(f\"loss: {loss.item():.2f}, val loss: {val_losses[-1] if val_losses else 0:.2f}, lr: {scheduler.get_last_lr()[0]:.2e}\")\n",
    "        losses.append(loss.item())\n",
    "        \n",
    "        # Log to wandb\n",
    "        if use_wandb:\n",
    "            wandb.log({\n",
    "                'train_loss': loss.item(),\n",
    "                'learning_rate': scheduler.get_last_lr()[0],\n",
    "                'step': step\n",
    "            })\n",
    "        \n",
    "        if step % val_interval == 0:\n",
    "            # Calculate validation loss\n",
    "            with torch.no_grad():\n",
    "                val_loss = 0\n",
    "                for _ in range(val_steps):\n",
    "                    xb, yb = get_batch('val', seq_len=seq_len, batch_size=batch_size)\n",
    "                    _, loss = model(xb, yb)\n",
    "                    val_loss += loss.item()\n",
    "                val_loss /= val_steps\n",
    "                val_losses.append(val_loss)\n",
    "                \n",
    "            # Log validation loss to wandb\n",
    "            if use_wandb:\n",
    "                wandb.log({\n",
    "                    'val_loss': val_loss,\n",
    "                    'step': step\n",
    "                })\n",
    "            \n",
    "            ax.clear()\n",
    "            ax.plot(losses, color='blue', label='train loss', alpha=0.7)\n",
    "            ax.plot(range(0, len(losses), val_interval), val_losses, color='red', label='val loss', alpha=0.7)\n",
    "            ax.set_ylim(1, 4)\n",
    "            ax.legend()\n",
    "            dh.update(fig)\n",
    "            \n",
    "        # Save checkpoint\n",
    "        if step % checkpoint_interval == 0 and step > 0:\n",
    "            save_checkpoint(model, optimizer, scheduler, step, losses, val_losses, seq_len, batch_size, total_steps, save_dir)\n",
    "            \n",
    "    print('final loss:', loss.item(), 'final val loss:', val_loss)\n",
    "    \n",
    "    # Save final checkpoint\n",
    "    save_checkpoint(model, optimizer, scheduler, total_steps, losses, val_losses, seq_len, batch_size, total_steps, save_dir)\n",
    "    \n",
    "    return losses, val_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Measure post training perplexity on validation set\n",
    "# Create function that receives a model, context length, and PPL sequence length, and returns the perplexity\n",
    "# The PPL sequence length is the number of characters the function uses to calculate the perplexity\n",
    "# We take the logits and calculate the cross entropy loss from scratch, then exponentiate it to get the perplexity\n",
    "# not only that, but we want the models to do this in actual inference\n",
    "def perplexity(model, seq_len, ppl_seq_len, batch_size=128, val_steps=1000):\n",
    "    with torch.no_grad():\n",
    "        val_loss = 0\n",
    "        for _ in tqdm(range(val_steps)):\n",
    "            xb, yb = get_batch('val', seq_len=seq_len, batch_size=batch_size)\n",
    "            with torch.autocast(device_type=\"cuda\", dtype=torch.bfloat16):\n",
    "                logits, _ = model(xb, yb)\n",
    "            logits = logits.reshape(batch_size, seq_len, vocab_size)\n",
    "            logits = logits[:, :ppl_seq_len]\n",
    "            yb = yb[:, :ppl_seq_len]\n",
    "            # flatten logits and targets\n",
    "            logits = logits.reshape(batch_size*ppl_seq_len, vocab_size)\n",
    "            yb = yb.reshape(batch_size*ppl_seq_len)\n",
    "            # calculate cross entropy loss from scratch\n",
    "            loss = F.cross_entropy(logits, yb)\n",
    "            val_loss += loss.item()\n",
    "        val_loss /= val_steps\n",
    "        ppl = torch.exp(torch.tensor(val_loss))\n",
    "        return ppl.item(), val_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classic Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "k6cA2WbrayyL"
   },
   "outputs": [],
   "source": [
    "class TransformerConfig:\n",
    "    def __init__(self, vocab_size, seq_len, embed_size, head_num, layer_num):\n",
    "        self.vocab_size = vocab_size\n",
    "        self.seq_len = seq_len\n",
    "        self.embed_size = embed_size\n",
    "        self.head_num = head_num\n",
    "        self.layer_num = layer_num\n",
    "\n",
    "def precompute_freqs_cis(dim: int, end: int, theta: float = 10000.0):\n",
    "    freqs = 1.0 / (theta ** (torch.arange(0, dim, 2)[: (dim // 2)].float() / dim))\n",
    "    t = torch.arange(end, device=freqs.device, dtype=torch.float32)\n",
    "    freqs = torch.outer(t, freqs)\n",
    "    freqs_cis = torch.polar(torch.ones_like(freqs), freqs)  # complex64\n",
    "    return freqs_cis.to(device)\n",
    "\n",
    "def softpick(x, dim: int = -1, eps: float = 1e-8):\n",
    "    # softpick function: relu(exp(x)-1) / sum(abs(exp(x)-1))\n",
    "    # numerically stable version\n",
    "    x_m = torch.max(x, dim=dim, keepdim=True).values\n",
    "    x_m_e_m = torch.exp(-x_m)\n",
    "    x_e_1 = torch.exp(x - x_m) - x_m_e_m\n",
    "    r_x_e_1 = F.relu(x_e_1)\n",
    "    a_x_e_1 = torch.where(x.isfinite(), torch.abs(x_e_1), 0)\n",
    "    return r_x_e_1 / (torch.sum(a_x_e_1, dim=dim, keepdim=True) + eps)\n",
    "\n",
    "def apply_rotary_emb(\n",
    "    xq: torch.Tensor,\n",
    "    xk: torch.Tensor,\n",
    "    freqs_cis: torch.Tensor,\n",
    "):\n",
    "    xq_ = torch.view_as_complex(xq.float().reshape(*xq.shape[:-1], -1, 2))\n",
    "    xk_ = torch.view_as_complex(xk.float().reshape(*xk.shape[:-1], -1, 2))\n",
    "    # freqs_cis = reshape_for_broadcast(freqs_cis, xq_)\n",
    "    q_shape = [d if i == xq_.ndim - 2 or i == xq_.ndim - 1 else 1 for i, d in enumerate(xq_.shape)]\n",
    "    k_shape = [d if i == xq_.ndim - 2 or i == xk_.ndim - 1 else 1 for i, d in enumerate(xk_.shape)]\n",
    "    T_q = xq_.shape[-2] \n",
    "    q_freqs_cis = freqs_cis[-T_q:].view(*q_shape)\n",
    "    k_freqs_cis = freqs_cis.view(*k_shape)\n",
    "    xq_out = torch.view_as_real(xq_ * q_freqs_cis).flatten(xq.dim() - 1)\n",
    "    xk_out = torch.view_as_real(xk_ * k_freqs_cis).flatten(xq.dim() - 1)\n",
    "    return xq_out.type_as(xq), xk_out.type_as(xk)\n",
    "\n",
    "class RMSNorm(torch.nn.Module):\n",
    "    def __init__(self, dim, eps=1e-6):\n",
    "        super().__init__()\n",
    "        self.eps = eps\n",
    "        self.weight = nn.Parameter(torch.ones(dim))\n",
    "\n",
    "    def _norm(self, x):\n",
    "        return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)\n",
    "\n",
    "    def forward(self, x):\n",
    "        output = self._norm(x.float()).type_as(x)\n",
    "        return output * self.weight\n",
    "    \n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.lin_1 = nn.Linear(config.embed_size, config.embed_size*4)\n",
    "        self.lin_2 = nn.Linear(config.embed_size*4, config.embed_size)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.lin_1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.lin_2(x)\n",
    "        return x\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\" multiple heads of self-attention with AliBi in parallel \"\"\"\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.seq_len = config.seq_len\n",
    "        self.head_num = config.head_num\n",
    "        self.head_size = config.embed_size // config.head_num\n",
    "        self.key = nn.Linear(config.embed_size, config.embed_size, bias=False)\n",
    "        self.query = nn.Linear(config.embed_size, config.embed_size, bias=False)\n",
    "        self.value = nn.Linear(config.embed_size, config.embed_size, bias=False)\n",
    "        self.o = nn.Linear(config.embed_size, config.embed_size)\n",
    "        # block_mask for FlexAttention\n",
    "        def causal(b, h, q_idx, kv_idx):\n",
    "            causal_mask = q_idx >= kv_idx\n",
    "            return causal_mask\n",
    "        self.causal_mask = create_block_mask(causal, B=None, H=None, Q_LEN=config.seq_len, KV_LEN=config.seq_len)\n",
    "        self.freqs_cis = precompute_freqs_cis(config.embed_size//config.head_num, config.seq_len)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(config.seq_len, config.seq_len)))\n",
    "\n",
    "    def forward(self, x, kv_cache=None):\n",
    "        B, T, C = x.shape\n",
    "        _, _, T_past, _ = kv_cache[0].shape if kv_cache is not None and kv_cache[0] is not None else (0, 0, 0, 0)\n",
    "        q = self.query(x) # (B,T,C)\n",
    "        k = self.key(x)   # (B,T,C)\n",
    "        v = self.value(x) # (B,T,C)\n",
    "\n",
    "        # Split into heads\n",
    "        q = q.view(B, T, self.head_num, self.head_size).transpose(1, 2) # (B, H, T, C/H)\n",
    "        k = k.view(B, T, self.head_num, self.head_size).transpose(1, 2) # (B, H, T, C/H)\n",
    "        v = v.view(B, T, self.head_num, self.head_size).transpose(1, 2) # (B, H, T, C/H)\n",
    "\n",
    "        if kv_cache is not None:\n",
    "            k_past, v_past = kv_cache\n",
    "            if k_past is not None:\n",
    "                k = torch.cat((k_past, k), dim=2)\n",
    "                v = torch.cat((v_past, v), dim=2)\n",
    "            if k.shape[-2] > self.seq_len:\n",
    "                k = k[:, :, -self.seq_len:]\n",
    "                v = v[:, :, -self.seq_len:]\n",
    "            kv_cache = (k, v)\n",
    "        T_k = k.shape[-2]\n",
    "        q, k = apply_rotary_emb(q, k, self.freqs_cis[:T_k])\n",
    "\n",
    "        # compute attention scores (\"affinities\")\n",
    "        wei = q @ k.transpose(-2,-1) # (B, H, 1, C/H) @ (B, H, C/H, T) -> (B, H, 1, T)\n",
    "        wei = wei * self.head_size ** -0.5 # scaled attention\n",
    "        wei = wei.masked_fill(self.tril[T_k-T:T_k, T_k-T:T_k] == 0, float('-inf')) # (B, T, T)\n",
    "        wei = softpick(wei, dim=-1) # (B, H, T, T)\n",
    "        # apply attention to values\n",
    "        out = wei @ v # (B, H, 1, T) @ (B, H, T, C/H) -> (B, H, 1, C/H)\n",
    "\n",
    "        out = out.transpose(1, 2).contiguous().view(B, T, C) # (B, H, T, C/H) -> (B, T, H, C/H) -> (B, T, C)\n",
    "        out = self.o(out)\n",
    "        return out, kv_cache\n",
    "\n",
    "class Block(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.sa_heads = MultiHeadAttention(config)\n",
    "        self.ff_layer = FeedForward(config)\n",
    "        self.sa_norm = RMSNorm(config.embed_size)\n",
    "        self.ff_norm = RMSNorm(config.embed_size)\n",
    "    \n",
    "    def forward(self, x, kv_cache=None):\n",
    "        a, kv_cache = self.sa_heads(self.sa_norm(x), kv_cache)\n",
    "        h = x + a\n",
    "        o = h + self.ff_layer(self.ff_norm(h))\n",
    "        return o, kv_cache\n",
    "    \n",
    "class TransformerLM(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.layer_num = config.layer_num\n",
    "        self.head_num = config.head_num\n",
    "        self.seq_len = config.seq_len\n",
    "        # embed raw tokens to a lower dimensional embedding with embed_size\n",
    "        self.token_embedding_table = nn.Embedding(config.vocab_size, config.embed_size)\n",
    "        # Language Modelling (?) Head is a standard linear layer to go from \n",
    "        # embeddings back to logits of vocab_size\n",
    "        self.lm_head = nn.Linear(config.embed_size, config.vocab_size)\n",
    "        # transformer blocks\n",
    "        self.blocks = nn.ModuleList([Block(config) for _ in range(config.layer_num)])\n",
    "\n",
    "    def forward(self, idx, targets=None, kv_cache=None):\n",
    "        B, T = idx.shape\n",
    "        _, _, T_past, _ = kv_cache[0][0].shape if kv_cache is not None and kv_cache[0][0] is not None else (0, 0, 0, 0)\n",
    "        # idx and targets are both (B,T) tensor of integers\n",
    "        tok_embd = self.token_embedding_table(idx) # (B,T,C)\n",
    "        x = tok_embd\n",
    "        # go through blocks\n",
    "        for i, block in enumerate(self.blocks):\n",
    "            x, cache = block(x, None if kv_cache is None else kv_cache[i])\n",
    "            if kv_cache is not None:\n",
    "                kv_cache[i] = cache\n",
    "        # get logits with linear layer\n",
    "        logits = self.lm_head(x) # (B,T,V)\n",
    "        \n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, V = logits.shape\n",
    "            logits = logits.view(B*T, V)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "    \n",
    "    def generate(self, idx, max_new_tokens, temperature=1, use_cache=True):\n",
    "        if use_cache:\n",
    "            # initialize key-value cache\n",
    "            kv_cache = [(None, None) for _ in range(self.layer_num)]\n",
    "            # idx is (B, T) array of indices in the current context\n",
    "            # crop idx to the last seq_len tokens\n",
    "            idx_context = idx[:, -self.seq_len:]\n",
    "            for _ in range(max_new_tokens):\n",
    "                # get the predictions\n",
    "                logits, loss = self(idx_context, kv_cache=kv_cache)\n",
    "                # focus only on the last time step\n",
    "                logits = logits[:, -1, :] # becomes (B, C)\n",
    "                # apply temperature\n",
    "                logits = logits / temperature if temperature > 0 else logits\n",
    "                # apply softpick to get probabilities\n",
    "                probs = softpick(logits, dim=-1) # (B, C)\n",
    "                # sample from the distribution\n",
    "                idx_next = torch.multinomial(probs, num_samples=1) if temperature > 0 else torch.argmax(probs, dim=-1, keepdim=True) # (B, 1)\n",
    "                # append sampled index to the running sequence\n",
    "                idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
    "                # since we have kv cache, only need to pass new token\n",
    "                idx_context = idx_next\n",
    "            return idx\n",
    "        else:\n",
    "            # idx is (B, T) array of indices in the current context\n",
    "            for _ in range(max_new_tokens):\n",
    "                #crop idx to the last seq_len tokens\n",
    "                idx_context = idx[:, -self.seq_len:]\n",
    "                # get the predictions\n",
    "                logits, loss = self(idx_context)\n",
    "                # focus only on the last time step\n",
    "                logits = logits[:, -1, :] # becomes (B, C)\n",
    "                # apply temperature\n",
    "                logits = logits / temperature if temperature > 0 else logits\n",
    "                # apply softpick to get probabilities\n",
    "                probs = softpick(logits, dim=-1) # (B, C)\n",
    "                # sample from the distribution\n",
    "                idx_next = torch.multinomial(probs, num_samples=1) if temperature > 0 else torch.argmax(probs, dim=-1, keepdim=True) # (B, 1)\n",
    "                # append sampled index to the running sequence\n",
    "                idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
    "            return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/vast/users/qirong.ho/miniforge3/envs/flame/lib/python3.11/site-packages/torch/_inductor/lowering.py:1890: UserWarning: Torchinductor does not support code generation for complex operators. Performance may be worse than eager.\n",
      "  warnings.warn(\n",
      "/vast/users/qirong.ho/miniforge3/envs/flame/lib/python3.11/site-packages/torch/_inductor/compile_fx.py:282: UserWarning: TensorFloat32 tensor cores for float32 matrix multiplication available but not enabled. Consider setting `torch.set_float32_matmul_precision('high')` for better performance.\n",
      "  warnings.warn(\n",
      "Inductor Compilation: 0it [00:00, ?it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "4775511"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "config = TransformerConfig(\n",
    "    vocab_size=vocab_size,\n",
    "    seq_len=seq_len,\n",
    "    embed_size=256,\n",
    "    head_num=4,\n",
    "    layer_num=6\n",
    ")\n",
    "m = TransformerLM(config)\n",
    "m = torch.compile(m, options=torch_compile_options, fullgraph=True)\n",
    "m.to(device)\n",
    "xb, yb = get_batch('train', 5, 1)\n",
    "logits, loss = m(xb, yb)\n",
    "total_params = sum(p.numel() for p in m.parameters() if p.requires_grad)\n",
    "total_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inductor Compilation: 100%|██████████| 15/15 [00:00<00:00, 29.08it/s]\n",
      "W1215 11:24:30.383000 1952958 site-packages/torch/_inductor/codegen/triton_combo_kernel.py:110] [0/1] ComboKernels: 3 large pointwise nodes are separated\n",
      "W1215 11:24:31.215000 1952958 site-packages/torch/_inductor/codegen/triton_combo_kernel.py:110] [0/1] ComboKernels: 2 large pointwise nodes are separated\n",
      "W1215 11:24:31.356000 1952958 site-packages/torch/_inductor/codegen/triton_combo_kernel.py:110] [0/1] ComboKernels: 1 large pointwise nodes are separated\n",
      "W1215 11:24:32.823000 1952958 site-packages/torch/_inductor/codegen/triton_combo_kernel.py:110] [0/1] ComboKernels: 1 large pointwise nodes are separated\n",
      "W1215 11:24:37.439000 1952958 site-packages/torch/_inductor/codegen/triton_combo_kernel.py:110] [0/1] ComboKernels: 1 large pointwise nodes are separated\n",
      "W1215 11:24:37.881000 1952958 site-packages/torch/_inductor/codegen/triton_combo_kernel.py:110] [0/1] ComboKernels: 1 large pointwise nodes are separated\n",
      "W1215 11:24:38.042000 1952958 site-packages/torch/_inductor/codegen/triton_combo_kernel.py:110] [0/1] ComboKernels: 1 large pointwise nodes are separated\n",
      "W1215 11:24:38.204000 1952958 site-packages/torch/_inductor/codegen/triton_combo_kernel.py:110] [0/1] ComboKernels: 1 large pointwise nodes are separated\n",
      "W1215 11:24:38.761000 1952958 site-packages/torch/_inductor/codegen/triton_combo_kernel.py:110] [0/1] ComboKernels: 1 large pointwise nodes are separated\n",
      "W1215 11:24:42.767000 1952958 site-packages/torch/_inductor/codegen/triton_combo_kernel.py:110] [0/1] ComboKernels: 3 large pointwise nodes are separated\n",
      "W1215 11:24:43.332000 1952958 site-packages/torch/_inductor/codegen/triton_combo_kernel.py:110] [0/1] ComboKernels: 2 large pointwise nodes are separated\n",
      "W1215 11:24:43.599000 1952958 site-packages/torch/_inductor/codegen/triton_combo_kernel.py:110] [0/1] ComboKernels: 1 large pointwise nodes are separated\n",
      "Inductor Compilation: 100%|██████████| 27/27 [00:00<00:00, 157.01it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.22.2"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/vast/users/qirong.ho/torch_rocm/flame/resources/transformers_playground/wandb/run-20251215_112503-gwcjofjf</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/erlandpg/transformers-playground/runs/gwcjofjf' target=\"_blank\">softpick-lm-animesubs-256seq-256embed-4head-6layer.AMD</a></strong> to <a href='https://wandb.ai/erlandpg/transformers-playground' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/erlandpg/transformers-playground' target=\"_blank\">https://wandb.ai/erlandpg/transformers-playground</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/erlandpg/transformers-playground/runs/gwcjofjf' target=\"_blank\">https://wandb.ai/erlandpg/transformers-playground/runs/gwcjofjf</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 400x300 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "accd25fc43844b2ebf0ed27b87e6a11e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1215 11:25:06.869000 1952958 site-packages/torch/fx/experimental/symbolic_shapes.py:6823] [!0/1/0] _maybe_guard_rel() was called on non-relation expression Eq(s0, s48) | Eq(s48, 1)\n",
      "W1215 11:25:07.178000 1952958 site-packages/torch/fx/experimental/symbolic_shapes.py:6823] [!0/1/0] _maybe_guard_rel() was called on non-relation expression Eq(s0, s4) | Eq(s4, 1)\n",
      "W1215 11:25:07.410000 1952958 site-packages/torch/fx/experimental/symbolic_shapes.py:6823] [!0/1/0] _maybe_guard_rel() was called on non-relation expression Eq(s0, s32) | Eq(s32, 1)\n",
      "W1215 11:25:08.081000 1952958 site-packages/torch/fx/experimental/symbolic_shapes.py:6823] [!0/1/0] _maybe_guard_rel() was called on non-relation expression Eq(s0, s17) | Eq(s17, 1)\n",
      "W1215 11:25:08.312000 1952958 site-packages/torch/fx/experimental/symbolic_shapes.py:6823] [!0/1/0] _maybe_guard_rel() was called on non-relation expression Eq(s0, s62) | Eq(s62, 1)\n",
      "W1215 11:25:08.552000 1952958 site-packages/torch/fx/experimental/symbolic_shapes.py:6823] [!0/1/0] _maybe_guard_rel() was called on non-relation expression Eq(s0, s47) | Eq(s47, 1)\n",
      "W1215 11:25:17.515000 1952958 site-packages/torch/_logging/_internal.py:1154] [3/0] Profiler function <class 'torch.autograd.profiler.record_function'> will be ignored\n",
      "Inductor Compilation: 100%|██████████| 15/15 [00:00<00:00, 26.78it/s]\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "def save_notebook_to_wandb(notebook_path, run_name=None):\n",
    "    \"\"\"Save notebook without outputs to wandb\"\"\"\n",
    "    with open(notebook_path, 'r') as f:\n",
    "        nb = json.load(f)\n",
    "\n",
    "    for cell in nb['cells']:\n",
    "        if 'outputs' in cell:\n",
    "            cell['outputs'] = []\n",
    "        if 'execution_count' in cell:\n",
    "            cell['execution_count'] = None\n",
    "\n",
    "    clean_path = notebook_path.replace('.ipynb', '_clean.ipynb')\n",
    "    with open(clean_path, 'w') as f:\n",
    "        json.dump(nb, f, indent=1)\n",
    "\n",
    "    artifact = wandb.Artifact('training-notebook', type='code')\n",
    "    artifact.add_file(clean_path)\n",
    "    wandb.log_artifact(artifact)\n",
    "\n",
    "    return clean_path\n",
    "\n",
    "def get_wandb_config(model, optimizer, scheduler, seq_len, batch_size, total_steps):\n",
    "    \"\"\"Automatically extract wandb config from model and training parameters\"\"\"\n",
    "    config = {\n",
    "        \"architecture\": \"Transformer\",\n",
    "        \"dataset\": \"anime-subs\",\n",
    "        \"seq_len\": seq_len,\n",
    "        \"batch_size\": batch_size,\n",
    "        \"total_steps\": total_steps,\n",
    "        \"optimizer\": optimizer.__class__.__name__,\n",
    "        \"scheduler\": scheduler.__class__.__name__,\n",
    "        \"initial_lr\": optimizer.param_groups[0]['lr'],\n",
    "    }\n",
    "    \n",
    "    # Add model configuration\n",
    "    config.update({\n",
    "        \"vocab_size\": model.token_embedding_table.num_embeddings,\n",
    "        \"embed_size\": model.blocks[0].sa_heads.head_size * model.head_num,\n",
    "        \"head_num\": model.head_num,\n",
    "        \"layer_num\": model.layer_num,\n",
    "        \"total_params\": sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    })\n",
    "    \n",
    "    # Add scheduler-specific config\n",
    "    if hasattr(scheduler, 'T_max'):\n",
    "        config['scheduler_T_max'] = scheduler.T_max\n",
    "    if hasattr(scheduler, 'eta_min'):\n",
    "        config['scheduler_eta_min'] = scheduler.eta_min\n",
    "    \n",
    "    return config\n",
    "\n",
    "# Initialize wandb with automated config\n",
    "model = TransformerLM(config)\n",
    "model = torch.compile(model, options=torch_compile_options, fullgraph=True)\n",
    "model.to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=2e-3, fused=True)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=total_steps, eta_min=1e-6)\n",
    "\n",
    "# warmup\n",
    "for _ in range(10):\n",
    "    model(*get_batch('train', seq_len=seq_len, batch_size=batch_size))\n",
    "\n",
    "wandb_config = get_wandb_config(model, optimizer, scheduler, seq_len, batch_size, total_steps)\n",
    "wandb.init(\n",
    "    project=\"transformers-playground\",\n",
    "    config=wandb_config,\n",
    "    name=\"softpick-lm-animesubs-256seq-256embed-4head-6layer.AMD\"\n",
    ")\n",
    "\n",
    "losses, val_losses = train(model, optimizer, scheduler, seq_len, batch_size, total_steps)\n",
    "\n",
    "if use_wandb:\n",
    "    save_notebook_to_wandb('./transformer_playground.ipynb')\n",
    "    wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save\n",
    "torch.save(model.state_dict(), 'models/TransformerLM.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OptimizedModule(\n",
       "  (_orig_mod): TransformerLM(\n",
       "    (token_embedding_table): Embedding(87, 256)\n",
       "    (lm_head): Linear(in_features=256, out_features=87, bias=True)\n",
       "    (blocks): ModuleList(\n",
       "      (0-5): 6 x Block(\n",
       "        (sa_heads): MultiHeadAttention(\n",
       "          (key): Linear(in_features=256, out_features=256, bias=False)\n",
       "          (query): Linear(in_features=256, out_features=256, bias=False)\n",
       "          (value): Linear(in_features=256, out_features=256, bias=False)\n",
       "          (o): Linear(in_features=256, out_features=256, bias=True)\n",
       "        )\n",
       "        (ff_layer): FeedForward(\n",
       "          (lin_1): Linear(in_features=256, out_features=1024, bias=True)\n",
       "          (lin_2): Linear(in_features=1024, out_features=256, bias=True)\n",
       "          (relu): ReLU()\n",
       "        )\n",
       "        (sa_norm): RMSNorm()\n",
       "        (ff_norm): RMSNorm()\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load model\n",
    "model = TransformerLM(config)\n",
    "model = torch.compile(model, options=torch_compile_options, fullgraph=True)\n",
    "model.load_state_dict(torch.load('models/TransformerLM.pt'))\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b2696da7fac426783fac8eb8a8bd6fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inductor Compilation: 100%|██████████| 21/21 [00:00<00:00, 27.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "perplexity: 3.2762105464935303 loss: 1.1866875\n"
     ]
    }
   ],
   "source": [
    "# calculate perplexity\n",
    "ppl, loss = perplexity(model, seq_len, seq_len)\n",
    "print(\"perplexity:\", ppl, \"loss:\", loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "00r0pbm3b5eX",
    "outputId": "bdd3b34e-32a0-4724-b528-c162c0fd0c3e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[53, 72, 78,  1, 80, 66, 69, 69,  1, 71, 62, 79, 62, 75]])\n",
      "You will never do this myself.\n",
      "\n",
      "Makes sense, the world is a little while ago.\n",
      "\n",
      "I was surprised that every day was at peace,\n",
      "\n",
      "but I still have the same way to my girlfriend.\n",
      "\n",
      "The one I want to see you again today.\n",
      "\n",
      "I was worried about the sports match,\n",
      "\n",
      "but I was able to talk to you about your clothes.\n",
      "\n",
      "I was thinking I want to talk to you about it too.\n",
      "\n",
      "That way I was the one who told me to help you.\n",
      "\n",
      "I was so surprised as a trainer won't be able to realize the enemy for the cost of the stars.\n",
      "\n",
      "I was able to help you know how to handle the cartency to the contract.\n",
      "\n",
      "I won't let you off the rest of the train so you can call me a movie.\n",
      "\n",
      "But we didn't think that we could come out to the end.\n",
      "\n",
      "I wanted to tell you that.\n",
      "\n",
      "And you're so cool.\n",
      "\n",
      "You want to know what the world is all the best, right?\n",
      "\n",
      "It's the same as my concern.\n",
      "\n",
      "I think that if I can get them out of the second chapter that I was a bit really mature.\n",
      "\n",
      "The second year of the world is dead.\n",
      "\n",
      "It was a great successful for a man who can get a\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "idx = encode(\"You will never\")\n",
    "print(torch.tensor([idx]))\n",
    "print(decode(model.generate(idx=torch.tensor([idx], dtype=torch.long).to(device), max_new_tokens=1000, temperature=0.5, use_cache=True)[0].tolist()))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "flame",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
