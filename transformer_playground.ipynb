{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "sdtDsu1Y0EqL"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /home/coder/.netrc\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33merlandpg\u001b[0m to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wandb initialized successfully\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/coder/Python_project/transformers_playground/.venv/lib/python3.12/site-packages/torch/__init__.py:1617: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n",
      "  _C._set_float32_matmul_precision(precision)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import math\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "from torch.nn.attention.flex_attention import flex_attention, create_block_mask\n",
    "import triton\n",
    "import triton.language as tl\n",
    "from tqdm.notebook import tqdm\n",
    "from dotenv import load_dotenv\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython import display\n",
    "from contextlib import nullcontext\n",
    "import wandb\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "use_wandb = True # set to False to disable wandb logging\n",
    "use_compile = False\n",
    "try:\n",
    "    import wandb\n",
    "    wandb_api_key = os.getenv('WANDB_API_KEY')\n",
    "    if wandb_api_key:  \n",
    "        wandb.login(key=wandb_api_key)\n",
    "        use_wandb = use_wandb and True\n",
    "        print(\"wandb initialized successfully\")\n",
    "    else:\n",
    "        print(\"WANDB_API_KEY not found - wandb logging disabled\")\n",
    "except ImportError:\n",
    "    print(\"wandb not installed - wandb logging disabled\")\n",
    "    wandb = None\n",
    "\n",
    "torch.manual_seed(69)\n",
    "torch.set_printoptions(profile=\"short\", sci_mode=False, linewidth=100000)\n",
    "torch.set_float32_matmul_precision('high')\n",
    "# this script is configured to run on a RTX 3060 12GB GPU. you'll want to adjust the model sizes and batch sizes for other devices\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# device = torch.device('cpu')\n",
    "plt.rcParams['figure.figsize'] = [8, 6]\n",
    "plt.rcParams['figure.dpi'] = 50\n",
    "plt.rcParams['axes.grid'] = True\n",
    "plt.rcParams['xtick.minor.visible'] = True\n",
    "plt.rcParams['ytick.minor.visible'] = True\n",
    "\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ANr5dn7W0EqR",
    "outputId": "a00cbe8d-e1c6-454b-b552-4ffd17ce17e5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of dataset in characters:  39526018\n"
     ]
    }
   ],
   "source": [
    "# we use this 40mb file of concatenated anime subtitles as our dataset\n",
    "# just the right size for toy experiments like this I think\n",
    "with open('animesubs.txt', 'r', encoding='latin') as f:\n",
    "    text = f.read()\n",
    "print(\"length of dataset in characters: \", len(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pANiObIZ0EqU",
    "outputId": "98f70469-1c89-4341-89e8-c142a14331b1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Open your mind. Open your mind.\n",
      "\n",
      "Far beyond the deep blue Earth, you and I shall meet...\n",
      "\n",
      "AH! MY GODDESS\n",
      "\n",
      "A snow-white feather comes fluttering down, swaying gently in the air.\n",
      "\n",
      "Without holding back, I want to envelope you, my one and only love.\n",
      "\n",
      "I know I have the power to protect the one I love, right here in my hands.\n",
      "\n",
      "Open your mind. Just as I've always dreamed.\n",
      "\n",
      "Let the wind carry off your hopes, faraway.\n",
      "\n",
      "I have wings nobody can see. Look, you have them, too.\n",
      "\n",
      "They'll take us to where we ca\n"
     ]
    }
   ],
   "source": [
    "print(text[:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "WvM5h6_i3KsM"
   },
   "outputs": [],
   "source": [
    "# remove japanese characters\n",
    "text = ''.join(filter(lambda character:ord(character) < 0x3000, text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "U7SOcWJM0EqW",
    "outputId": "a82a4da8-a8ed-47bf-cfb5-2e4c59dee2cf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unique characters: 86 \n",
      " !'()*+,-./0123456789:;<=>?@ABCDEFGHIJKLMNOPQRSTUVWXYZ[]_abcdefghijklmnopqrstuvwxyz|Â”\n"
     ]
    }
   ],
   "source": [
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "print(\"unique characters:\", vocab_size, ''.join(chars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yobmmaeK0EqX",
    "outputId": "26669f38-4b26-4b53-f642-ee7543e7c578"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'\\n': 0, ' ': 1, '!': 2, \"'\": 3, '(': 4, ')': 5, '*': 6, '+': 7, ',': 8, '-': 9, '.': 10, '/': 11, '0': 12, '1': 13, '2': 14, '3': 15, '4': 16, '5': 17, '6': 18, '7': 19, '8': 20, '9': 21, ':': 22, ';': 23, '<': 24, '=': 25, '>': 26, '?': 27, '@': 28, 'A': 29, 'B': 30, 'C': 31, 'D': 32, 'E': 33, 'F': 34, 'G': 35, 'H': 36, 'I': 37, 'J': 38, 'K': 39, 'L': 40, 'M': 41, 'N': 42, 'O': 43, 'P': 44, 'Q': 45, 'R': 46, 'S': 47, 'T': 48, 'U': 49, 'V': 50, 'W': 51, 'X': 52, 'Y': 53, 'Z': 54, '[': 55, ']': 56, '_': 57, 'a': 58, 'b': 59, 'c': 60, 'd': 61, 'e': 62, 'f': 63, 'g': 64, 'h': 65, 'i': 66, 'j': 67, 'k': 68, 'l': 69, 'm': 70, 'n': 71, 'o': 72, 'p': 73, 'q': 74, 'r': 75, 's': 76, 't': 77, 'u': 78, 'v': 79, 'w': 80, 'x': 81, 'y': 82, 'z': 83, '|': 84, '\\x94': 85, '': 86}\n",
      "{0: '\\n', 1: ' ', 2: '!', 3: \"'\", 4: '(', 5: ')', 6: '*', 7: '+', 8: ',', 9: '-', 10: '.', 11: '/', 12: '0', 13: '1', 14: '2', 15: '3', 16: '4', 17: '5', 18: '6', 19: '7', 20: '8', 21: '9', 22: ':', 23: ';', 24: '<', 25: '=', 26: '>', 27: '?', 28: '@', 29: 'A', 30: 'B', 31: 'C', 32: 'D', 33: 'E', 34: 'F', 35: 'G', 36: 'H', 37: 'I', 38: 'J', 39: 'K', 40: 'L', 41: 'M', 42: 'N', 43: 'O', 44: 'P', 45: 'Q', 46: 'R', 47: 'S', 48: 'T', 49: 'U', 50: 'V', 51: 'W', 52: 'X', 53: 'Y', 54: 'Z', 55: '[', 56: ']', 57: '_', 58: 'a', 59: 'b', 60: 'c', 61: 'd', 62: 'e', 63: 'f', 64: 'g', 65: 'h', 66: 'i', 67: 'j', 68: 'k', 69: 'l', 70: 'm', 71: 'n', 72: 'o', 73: 'p', 74: 'q', 75: 'r', 76: 's', 77: 't', 78: 'u', 79: 'v', 80: 'w', 81: 'x', 82: 'y', 83: 'z', 84: '|', 85: '\\x94', 86: ''}\n",
      "encoded: [43, 73, 62, 71, 1, 82, 72, 78, 75, 1, 70, 66, 71, 61, 10, 1, 43, 73, 62, 71]\n",
      "decoded: Open your mind. Open\n",
      "vocab size: 87\n"
     ]
    }
   ],
   "source": [
    "# yes, all language models will be character level, which isn't ideal but it's good for simplicity\n",
    "# very simple tokenizer\n",
    "stoi = {ch:i for i,ch in enumerate(chars)}\n",
    "itos = {i:ch for i,ch in enumerate(chars)}\n",
    "# add special token for padding\n",
    "stoi[''] = len(stoi)\n",
    "itos[len(itos)] = ''\n",
    "print(stoi)\n",
    "print(itos)\n",
    "encode = lambda s: [stoi[ch] for ch in s]\n",
    "decode = lambda l: ''.join([itos[i] for i in l])\n",
    "print(\"encoded:\", encode(text[:20]))\n",
    "print(\"decoded:\", decode(encode(text[:20])))\n",
    "vocab_size = len(itos)\n",
    "print(\"vocab size:\", vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6Pnf9KfP0EqY",
    "outputId": "ff581168-335a-4f0f-efeb-0d4bd5b225ca"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([39526018])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = torch.tensor(encode(text), dtype=torch.int64)\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lZ2fY1pR0EqY",
    "outputId": "5eb599e0-fb79-4cdb-c4b9-52a781d67151"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([43, 73, 62, 71,  1, 82, 72, 78, 75,  1, 70, 66, 71, 61, 10,  1, 43, 73, 62, 71,  1, 82, 72, 78, 75,  1, 70, 66, 71, 61, 10,  0,  0, 34, 58, 75,  1, 59, 62, 82, 72, 71, 61,  1, 77, 65, 62,  1, 61, 62, 62, 73,  1, 59, 69, 78, 62,  1, 33, 58, 75, 77, 65,  8,  1, 82, 72, 78,  1, 58, 71, 61,  1, 37,  1, 76, 65, 58, 69, 69,  1, 70, 62, 62, 77, 10, 10, 10,  0,  0, 29, 36,  2,  1, 41, 53,  1, 35, 43, 32])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oIaYesPh0Eqa",
    "outputId": "caa27cbb-5ae3-4406-8194-646264d4ba99"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([39130757]) torch.Size([395261])\n"
     ]
    }
   ],
   "source": [
    "n = int(len(data) * 0.99)\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]\n",
    "print(train_data.shape, val_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4bFhizcI0Eqa",
    "outputId": "c93a4d43-6fb2-4de7-aefc-8fed47e4a861"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([43, 73, 62, 71,  1, 82, 72, 78, 75])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq_len = 8\n",
    "train_data[:seq_len+1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "skFCPvQC0Eqc",
    "outputId": "08990c5b-88af-4a51-ce37-3c59989d6d74"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs:\n",
      "torch.Size([2, 64])\n",
      "tensor([[64,  1, 72, 78, 75,  1, 66, 71, 77, 62, 75, 71, 76, 65, 66, 73, 76, 10,  0,  0, 48, 65, 62,  1, 71, 66, 64, 65, 77,  1, 37, 66, 61, 58,  1, 64, 72, 77,  1, 77, 65, 62,  1, 75, 62, 76, 78, 69, 77, 76,  1, 72, 63,  1, 77, 65, 62,  1, 62, 81, 58, 70, 66, 71],\n",
      "        [62, 70,  2,  0,  0, 37, 77,  1, 69, 72, 72, 68, 76,  1, 69, 66, 68, 62,  1, 80, 62,  1, 77, 72, 72, 68,  1, 60, 58, 75, 62,  1, 72, 63,  1, 77, 65, 62, 70, 10,  0,  0, 48, 65, 72, 76, 62,  1, 64, 78, 82, 76,  1, 80, 62, 75, 62,  1, 66, 71, 60, 75, 62, 61]], device='cuda:0')\n",
      "targets:\n",
      "torch.Size([2, 64])\n",
      "tensor([[ 1, 72, 78, 75,  1, 66, 71, 77, 62, 75, 71, 76, 65, 66, 73, 76, 10,  0,  0, 48, 65, 62,  1, 71, 66, 64, 65, 77,  1, 37, 66, 61, 58,  1, 64, 72, 77,  1, 77, 65, 62,  1, 75, 62, 76, 78, 69, 77, 76,  1, 72, 63,  1, 77, 65, 62,  1, 62, 81, 58, 70, 66, 71, 58],\n",
      "        [70,  2,  0,  0, 37, 77,  1, 69, 72, 72, 68, 76,  1, 69, 66, 68, 62,  1, 80, 62,  1, 77, 72, 72, 68,  1, 60, 58, 75, 62,  1, 72, 63,  1, 77, 65, 62, 70, 10,  0,  0, 48, 65, 72, 76, 62,  1, 64, 78, 82, 76,  1, 80, 62, 75, 62,  1, 66, 71, 60, 75, 62, 61, 66]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "def get_batch(split, seq_len, batch_size=4):\n",
    "    # generate a small batch of data of inputs x and targets y\n",
    "    # targets are just inputs shifted by 1\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - seq_len, (batch_size,))\n",
    "    x = torch.stack([data[i:i+seq_len] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+seq_len+1] for i in ix])\n",
    "    return x.to(device), y.to(device)\n",
    "\n",
    "xb, yb = get_batch('train', 64, 2)\n",
    "print('inputs:')\n",
    "print(xb.shape)\n",
    "print(xb)\n",
    "print('targets:')\n",
    "print(yb.shape)\n",
    "print(yb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "327680000"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Make all steps, sequence lengths, and batch size the same\n",
    "total_steps = 5000\n",
    "seq_len = 256\n",
    "batch_size = 256 # these are small models so we can use large batch sizes to fully utilize the GPU\n",
    "# should cover around 2x the dataset\n",
    "total_steps * seq_len * batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Test forward pass\ntorch_compile_options = {\n    'epilogue_fusion': True, \n    'max_autotune': False, \n    'shape_padding': True, \n    'trace.enabled': False, \n    'triton.cudagraphs': False, \n    'debug': False, \n    'dce': True, \n    'memory_planning': True, \n    'coordinate_descent_tuning': False, \n    'trace.graph_diagram': False, \n    'compile_threads': 32, \n    'group_fusion': True, \n    'disable_progress': False, \n    'verbose_progress': False, \n    'triton.multi_kernel': 0, \n    'triton.use_block_ptr': False, \n    'triton.enable_persistent_tma_matmul': False, \n    'triton.autotune_at_compile_time': False, \n    'triton.cooperative_reductions': False, \n    'cuda.compile_opt_level': '-O2', \n    'cuda.enable_cuda_lto': True, \n    'combo_kernels': True, \n    'benchmark_combo_kernel': True, \n    'combo_kernel_foreach_dynamic_shapes': True\n}\n\ntorch_compile_optimizer_lr = torch.compile(fullgraph=False, options=torch_compile_options) if use_compile else (lambda f: f)\n\n@torch_compile_optimizer_lr\ndef compile_optimizer_lr(opt, scheduler):\n    opt.step()\n    scheduler.step()\n\n\ndef save_checkpoint(model, optimizer, scheduler, step, losses, val_losses, seq_len, batch_size, total_steps, save_dir='checkpoints'):\n    \"\"\"Save a complete checkpoint including model, optimizer, scheduler states and training metrics\"\"\"\n    os.makedirs(save_dir, exist_ok=True)\n    \n    checkpoint = {\n        'step': step,\n        'model_state_dict': model.state_dict(),\n        'optimizer_state_dict': optimizer.state_dict(),\n        'scheduler_state_dict': scheduler.state_dict(),\n        'train_losses': losses,\n        'val_losses': val_losses,\n        'config': {\n            'seq_len': seq_len,\n            'batch_size': batch_size,\n            'total_steps': total_steps,\n            'vocab_size': model.token_embedding_table.num_embeddings,\n            'embed_size': model.blocks[0].sa_heads.head_size * model.head_num,\n            'head_num': model.head_num,\n            'layer_num': model.layer_num\n        }\n    }\n    \n    checkpoint_path = os.path.join(save_dir, f'checkpoint_step_{step}.pt')\n    torch.save(checkpoint, checkpoint_path)\n    # Checkpoints saved locally only (not uploaded to W&B to save storage)\n    print(f\"Checkpoint saved at step {step}: {checkpoint_path}\")\n    \n    return checkpoint_path\n\ndef load_checkpoint(model, optimizer, scheduler, checkpoint_path):\n    \"\"\"Load a checkpoint and restore model, optimizer, scheduler states\"\"\"\n    checkpoint = torch.load(checkpoint_path, map_location=device)\n    \n    model.load_state_dict(checkpoint['model_state_dict'])\n    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n    scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n    \n    print(f\"Checkpoint loaded from step {checkpoint['step']}\")\n    return checkpoint['step'], checkpoint['train_losses'], checkpoint['val_losses']\n\nthreshold = 0.5\n\ndef train(model, optimizer, scheduler, seq_len, batch_size, total_steps, val_steps=10, val_interval=50, checkpoint_interval=500, save_dir='checkpoints'):\n    losses = []\n    val_losses = []\n    # Create save directory\n    os.makedirs(save_dir, exist_ok=True)\n    \n    # live plot\n    fig, ax = plt.subplots()\n    dh = display.display(fig, display_id=True)\n    \n    for step in (bar := tqdm(range(total_steps))):\n        # sample a batch of data\n        xb, yb = get_batch('train', seq_len=seq_len, batch_size=batch_size)\n\n        # evaluate the loss\n        p = torch.rand(1).item()\n        # print(f\"{p=:.4f}, threshold={threshold}\")\n        if p < threshold:\n            # print(\"Using standard softmax at step\", step)\n            logits, loss = model(xb, yb, use_silu_softpick=False)\n        else:\n            # print(\"Using SiLU softpick at step\", step)\n            logits, loss = model(xb, yb, use_silu_softpick=True)\n        # backprop\n        optimizer.zero_grad(set_to_none=True)\n\n        compile_autograd = torch._dynamo.compiled_autograd._enable(torch.compile()) if use_compile else nullcontext()\n        with compile_autograd:\n            loss.backward()\n        compile_optimizer_lr(optimizer, scheduler)\n\n        bar.set_description(f\"loss: {loss.item():.2f}, val loss: {val_losses[-1] if val_losses else 0:.2f}, lr: {scheduler.get_last_lr()[0]:.2e}\")\n        losses.append(loss.item())\n        \n        # Log to wandb\n        if use_wandb:\n            wandb.log({\n                'train_loss': loss.item(),\n                'learning_rate': scheduler.get_last_lr()[0],\n                'step': step\n            })\n        \n        if step % val_interval == 0:\n            # Calculate validation loss\n            with torch.no_grad():\n                val_loss = 0\n                for _ in range(val_steps):\n                    xb, yb = get_batch('val', seq_len=seq_len, batch_size=batch_size)\n                    _, loss = model(xb, yb)\n                    val_loss += loss.item()\n                val_loss /= val_steps\n                val_losses.append(val_loss)\n                \n            # Log validation loss to wandb\n            if use_wandb:\n                wandb.log({\n                    'val_loss': val_loss,\n                    'step': step\n                })\n            \n            ax.clear()\n            ax.plot(losses, color='blue', label='train loss', alpha=0.7)\n            ax.plot(range(0, len(losses), val_interval), val_losses, color='red', label='val loss', alpha=0.7)\n            ax.set_ylim(1, 4)\n            ax.legend()\n            dh.update(fig)\n            \n        # Save checkpoint\n        if step % checkpoint_interval == 0 and step > 0:\n            save_checkpoint(model, optimizer, scheduler, step, losses, val_losses, seq_len, batch_size, total_steps, save_dir)\n            \n    print('final loss:', loss.item(), 'final val loss:', val_loss)\n    \n    # Save final checkpoint\n    save_checkpoint(model, optimizer, scheduler, total_steps, losses, val_losses, seq_len, batch_size, total_steps, save_dir)\n    \n    return losses, val_losses"
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Measure post training perplexity on validation set\n",
    "# Create function that receives a model, context length, and PPL sequence length, and returns the perplexity\n",
    "# The PPL sequence length is the number of characters the function uses to calculate the perplexity\n",
    "# We take the logits and calculate the cross entropy loss from scratch, then exponentiate it to get the perplexity\n",
    "# not only that, but we want the models to do this in actual inference\n",
    "def perplexity(model, seq_len, ppl_seq_len, batch_size=128, val_steps=1000):\n",
    "    with torch.no_grad():\n",
    "        val_loss = 0\n",
    "        for _ in tqdm(range(val_steps)):\n",
    "            xb, yb = get_batch('val', seq_len=seq_len, batch_size=batch_size)\n",
    "            with torch.autocast(device_type=\"cuda\", dtype=torch.bfloat16):\n",
    "                logits, _ = model(xb, yb)\n",
    "            logits = logits.reshape(batch_size, seq_len, vocab_size)\n",
    "            logits = logits[:, :ppl_seq_len]\n",
    "            yb = yb[:, :ppl_seq_len]\n",
    "            # flatten logits and targets\n",
    "            logits = logits.reshape(batch_size*ppl_seq_len, vocab_size)\n",
    "            yb = yb.reshape(batch_size*ppl_seq_len)\n",
    "            # calculate cross entropy loss from scratch\n",
    "            loss = F.cross_entropy(logits, yb)\n",
    "            val_loss += loss.item()\n",
    "        val_loss /= val_steps\n",
    "        ppl = torch.exp(torch.tensor(val_loss))\n",
    "        return ppl.item(), val_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classic Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "k6cA2WbrayyL"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append(\".\")\n",
    "\n",
    "from flash_softpick_attn import parallel_softpick_attn\n",
    "from naive_softpick_attn import naive_softpick_attn, naive_silu_softpick_attn\n",
    "\n",
    "threshold = 0.5\n",
    "\n",
    "class TransformerConfig:\n",
    "    def __init__(self, vocab_size, seq_len, embed_size, head_num, layer_num):\n",
    "        self.vocab_size = vocab_size\n",
    "        self.seq_len = seq_len\n",
    "        self.embed_size = embed_size\n",
    "        self.head_num = head_num\n",
    "        self.layer_num = layer_num\n",
    "\n",
    "def precompute_freqs_cis(dim: int, end: int, theta: float = 10000.0):\n",
    "    freqs = 1.0 / (theta ** (torch.arange(0, dim, 2)[: (dim // 2)].float() / dim))\n",
    "    t = torch.arange(end, device=freqs.device, dtype=torch.float32)\n",
    "    freqs = torch.outer(t, freqs)\n",
    "    freqs_cis = torch.polar(torch.ones_like(freqs), freqs)  # complex64\n",
    "    return freqs_cis.to(device)\n",
    "\n",
    "def apply_rotary_emb(\n",
    "    xq: torch.Tensor,\n",
    "    xk: torch.Tensor,\n",
    "    freqs_cis: torch.Tensor,\n",
    "):\n",
    "    xq_ = torch.view_as_complex(xq.float().reshape(*xq.shape[:-1], -1, 2))\n",
    "    xk_ = torch.view_as_complex(xk.float().reshape(*xk.shape[:-1], -1, 2))\n",
    "    # freqs_cis = reshape_for_broadcast(freqs_cis, xq_)\n",
    "    q_shape = [d if i == xq_.ndim - 2 or i == xq_.ndim - 1 else 1 for i, d in enumerate(xq_.shape)]\n",
    "    k_shape = [d if i == xq_.ndim - 2 or i == xk_.ndim - 1 else 1 for i, d in enumerate(xk_.shape)]\n",
    "    T_q = xq_.shape[-2] \n",
    "    q_freqs_cis = freqs_cis[-T_q:].view(*q_shape)\n",
    "    k_freqs_cis = freqs_cis.view(*k_shape)\n",
    "    xq_out = torch.view_as_real(xq_ * q_freqs_cis).flatten(xq.dim() - 1)\n",
    "    xk_out = torch.view_as_real(xk_ * k_freqs_cis).flatten(xq.dim() - 1)\n",
    "    return xq_out.type_as(xq), xk_out.type_as(xk)\n",
    "\n",
    "class RMSNorm(torch.nn.Module):\n",
    "    def __init__(self, dim, eps=1e-6):\n",
    "        super().__init__()\n",
    "        self.eps = eps\n",
    "        self.weight = nn.Parameter(torch.ones(dim))\n",
    "\n",
    "    def _norm(self, x):\n",
    "        return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)\n",
    "\n",
    "    def forward(self, x):\n",
    "        output = self._norm(x.float()).type_as(x)\n",
    "        return output * self.weight\n",
    "    \n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.lin_1 = nn.Linear(config.embed_size, config.embed_size*4)\n",
    "        self.lin_2 = nn.Linear(config.embed_size*4, config.embed_size)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.lin_1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.lin_2(x)\n",
    "        return x\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\" multiple heads of self-attention with AliBi in parallel \"\"\"\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.seq_len = config.seq_len\n",
    "        self.head_num = config.head_num\n",
    "        self.head_size = config.embed_size // config.head_num\n",
    "        self.key = nn.Linear(config.embed_size, config.embed_size, bias=False)\n",
    "        self.query = nn.Linear(config.embed_size, config.embed_size, bias=False)\n",
    "        self.value = nn.Linear(config.embed_size, config.embed_size, bias=False)\n",
    "        self.o = nn.Linear(config.embed_size, config.embed_size)\n",
    "        # block_mask for FlexAttention\n",
    "        def causal(b, h, q_idx, kv_idx):\n",
    "            causal_mask = q_idx >= kv_idx\n",
    "            return causal_mask\n",
    "        self.causal_mask = create_block_mask(causal, B=None, H=None, Q_LEN=config.seq_len, KV_LEN=config.seq_len)\n",
    "        self.freqs_cis = precompute_freqs_cis(config.embed_size//config.head_num, config.seq_len)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(config.seq_len, config.seq_len)))\n",
    "\n",
    "    def forward(self, x, kv_cache=None, use_silu_softpick=False):\n",
    "        B, T, C = x.shape\n",
    "        _, _, T_past, _ = kv_cache[0].shape if kv_cache is not None and kv_cache[0] is not None else (0, 0, 0, 0)\n",
    "        q = self.query(x) # (B,T,C)\n",
    "        k = self.key(x)   # (B,T,C)\n",
    "        v = self.value(x) # (B,T,C)\n",
    "\n",
    "        # Split into heads\n",
    "        q = q.view(B, T, self.head_num, self.head_size).transpose(1, 2) # (B, H, T, C/H)\n",
    "        k = k.view(B, T, self.head_num, self.head_size).transpose(1, 2) # (B, H, T, C/H)\n",
    "        v = v.view(B, T, self.head_num, self.head_size).transpose(1, 2) # (B, H, T, C/H)\n",
    "\n",
    "        if kv_cache is not None:\n",
    "            k_past, v_past = kv_cache\n",
    "            if k_past is not None:\n",
    "                k = torch.cat((k_past, k), dim=2)\n",
    "                v = torch.cat((v_past, v), dim=2)\n",
    "            if k.shape[-2] > self.seq_len:\n",
    "                k = k[:, :, -self.seq_len:]\n",
    "                v = v[:, :, -self.seq_len:]\n",
    "            kv_cache = (k, v)\n",
    "        T_k = k.shape[-2]\n",
    "        q, k = apply_rotary_emb(q, k, self.freqs_cis[:T_k])\n",
    "\n",
    "        if T == self.seq_len:\n",
    "            if use_silu_softpick:\n",
    "                # print(\"Using SiLU softpick attention\")\n",
    "                out = naive_softpick_attn(q, k, v, head_first=True)[0]\n",
    "            else:\n",
    "                # naive sofptick\n",
    "                # out = naive_softpick_attn(q, k, v, head_first=True)[0]\n",
    "                # print(\"Using softmax attention\")\n",
    "                out = torch.nn.functional.scaled_dot_product_attention(q, k, v, is_causal=True)\n",
    "        else:\n",
    "            # compute attention scores (\"affinities\")\n",
    "            wei = q @ k.transpose(-2,-1) # (B, H, 1, C/H) @ (B, H, C/H, T) -> (B, H, 1, T)\n",
    "            wei = wei * self.head_size ** -0.5 # scaled attention\n",
    "            wei = wei.masked_fill(self.tril[T_k-T:T_k, T_k-T:T_k] == 0, float('-inf')) # (B, T, T)\n",
    "            wei = F.softmax(wei, dim=-1) # (B, H, T, T)\n",
    "            # apply attention to values\n",
    "            out = wei @ v # (B, H, 1, T) @ (B, H, T, C/H) -> (B, H, 1, C/H)\n",
    "\n",
    "        out = out.transpose(1, 2).contiguous().view(B, T, C) # (B, H, T, C/H) -> (B, T, H, C/H) -> (B, T, C)\n",
    "        out = self.o(out)\n",
    "        return out, kv_cache\n",
    "\n",
    "class Block(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.sa_heads = MultiHeadAttention(config)\n",
    "        self.ff_layer = FeedForward(config)\n",
    "        self.sa_norm = RMSNorm(config.embed_size)\n",
    "        self.ff_norm = RMSNorm(config.embed_size)\n",
    "    \n",
    "    def forward(self, x, kv_cache=None, use_silu_softpick=False):\n",
    "        a, kv_cache = self.sa_heads(self.sa_norm(x), kv_cache, use_silu_softpick=use_silu_softpick)\n",
    "        h = x + a\n",
    "        o = h + self.ff_layer(self.ff_norm(h))\n",
    "        return o, kv_cache\n",
    "    \n",
    "class TransformerLM(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.layer_num = config.layer_num\n",
    "        self.head_num = config.head_num\n",
    "        self.seq_len = config.seq_len\n",
    "        # embed raw tokens to a lower dimensional embedding with embed_size\n",
    "        self.token_embedding_table = nn.Embedding(config.vocab_size, config.embed_size)\n",
    "        # Language Modelling (?) Head is a standard linear layer to go from \n",
    "        # embeddings back to logits of vocab_size\n",
    "        self.lm_head = nn.Linear(config.embed_size, config.vocab_size)\n",
    "        # transformer blocks\n",
    "        self.blocks = nn.ModuleList([Block(config) for _ in range(config.layer_num)])\n",
    "\n",
    "    def forward(self, idx, targets=None, kv_cache=None, use_silu_softpick=False):\n",
    "        B, T = idx.shape\n",
    "        _, _, T_past, _ = kv_cache[0][0].shape if kv_cache is not None and kv_cache[0][0] is not None else (0, 0, 0, 0)\n",
    "        # idx and targets are both (B,T) tensor of integers\n",
    "        tok_embd = self.token_embedding_table(idx) # (B,T,C)\n",
    "        x = tok_embd\n",
    "        # go through blocks\n",
    "        for i, block in enumerate(self.blocks):\n",
    "            x, cache = block(x, None if kv_cache is None else kv_cache[i], use_silu_softpick=use_silu_softpick)\n",
    "            if kv_cache is not None:\n",
    "                kv_cache[i] = cache\n",
    "        # get logits with linear layer\n",
    "        logits = self.lm_head(x) # (B,T,V)\n",
    "        \n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, V = logits.shape\n",
    "            logits = logits.view(B*T, V)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "    \n",
    "    def generate(self, idx, max_new_tokens, temperature=1, use_cache=True):\n",
    "        if use_cache:\n",
    "            # initialize key-value cache\n",
    "            kv_cache = [(None, None) for _ in range(self.layer_num)]\n",
    "            # idx is (B, T) array of indices in the current context\n",
    "            # crop idx to the last seq_len tokens\n",
    "            idx_context = idx[:, -self.seq_len:]\n",
    "            for _ in range(max_new_tokens):\n",
    "                # get the predictions\n",
    "                logits, loss = self(idx_context, kv_cache=kv_cache)\n",
    "                # focus only on the last time step\n",
    "                logits = logits[:, -1, :] # becomes (B, C)\n",
    "                # apply temperature\n",
    "                logits = logits / temperature if temperature > 0 else logits\n",
    "                # apply softmax to get probabilities\n",
    "                probs = F.softmax(logits, dim=-1) # (B, C)\n",
    "                # sample from the distribution\n",
    "                idx_next = torch.multinomial(probs, num_samples=1) if temperature > 0 else torch.argmax(probs, dim=-1, keepdim=True) # (B, 1)\n",
    "                # append sampled index to the running sequence\n",
    "                idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
    "                # since we have kv cache, only need to pass new token\n",
    "                idx_context = idx_next\n",
    "            return idx\n",
    "        else:\n",
    "            # idx is (B, T) array of indices in the current context\n",
    "            for _ in range(max_new_tokens):\n",
    "                #crop idx to the last seq_len tokens\n",
    "                idx_context = idx[:, -self.seq_len:]\n",
    "                # get the predictions\n",
    "                logits, loss = self(idx_context)\n",
    "                # focus only on the last time step\n",
    "                logits = logits[:, -1, :] # becomes (B, C)\n",
    "                # apply temperature\n",
    "                logits = logits / temperature if temperature > 0 else logits\n",
    "                # apply softmax to get probabilities\n",
    "                probs = F.softmax(logits, dim=-1) # (B, C)\n",
    "                # sample from the distribution\n",
    "                idx_next = torch.multinomial(probs, num_samples=1) if temperature > 0 else torch.argmax(probs, dim=-1, keepdim=True) # (B, 1)\n",
    "                # append sampled index to the running sequence\n",
    "                idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
    "            return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4775511"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "config = TransformerConfig(\n",
    "    vocab_size=vocab_size,\n",
    "    seq_len=seq_len,\n",
    "    embed_size=256,\n",
    "    head_num=4,\n",
    "    layer_num=6\n",
    ")\n",
    "m = TransformerLM(config)\n",
    "if use_compile:\n",
    "    m = torch.compile(m, options=torch_compile_options, fullgraph=True)\n",
    "m.to(device)\n",
    "xb, yb = get_batch('train', 5, 1)\n",
    "logits, loss = m(xb, yb)\n",
    "total_params = sum(p.numel() for p in m.parameters() if p.requires_grad)\n",
    "total_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.22.2"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/coder/Python_project/transformers_playground/wandb/run-20251104_182455-bwg9fcgk</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/erlandpg/transformers-playground/runs/bwg9fcgk' target=\"_blank\">stochastic-softpick-softmax-lm-animesubs-256seq-256embed-4head-6layer</a></strong> to <a href='https://wandb.ai/erlandpg/transformers-playground' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/erlandpg/transformers-playground' target=\"_blank\">https://wandb.ai/erlandpg/transformers-playground</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/erlandpg/transformers-playground/runs/bwg9fcgk' target=\"_blank\">https://wandb.ai/erlandpg/transformers-playground/runs/bwg9fcgk</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAVAAAAD+CAYAAABsiV3zAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjMsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvZiW1igAAAAlwSFlzAAAHsQAAB7EBBsVhhgAAJuJJREFUeJzt3Xl0FFW+B/BvdzohK1kISzAIgjgCUXkOeaCixihGFhGfGZBtwAExiuIYRwQdnsgBHXxHR2CQMIgiUXwyTETUhw044IKjojBCAwMKhJCErUP2Pel6f/zodDpJZ2nS9K3O93NOn06qq6rvzfLrX9361S2DpmkaiIiozYzebgARkV4xgBIRuYkBlIjITQygRERuchlAS0tLMWTIEHzyySd1y3bu3Ilp06Zh8uTJyM3NvSwNJCJSlcsAunTpUowfP95pWVpaGt5++23Mnz8fa9eu9XjjiIhUZmpq4fbt2zFw4EBUVFQ4Ldc0DUajEb1790Z2dnaj7cxmM8xmM7744gsMGjSoTQ2x2Qz4xz9CYDQacZ/pE5QOHQpbp05t2oceVFZWopMP9qs+9tE3sI/OSkpKkJGR4bSsyQC6a9culJaW4tChQwgKCsKoUaNgNBphNBphs9mQlZWF2NjYRtslJSUhKSkJqampeO2119rUkaoq4O67ixAW1hkrQ/OA5cuBrl3btA89sFgsiIuL83YzPIp99A3so7PU1NRGy5oMoEuWLAEArFu3DtHR0Zg2bRrS09Mxa9YszJw5E9XV1Vi6dOklNLsFBgPA+n4iUlyTAdRu+vTpAIAxY8YAABITE5GYmOjxRhER6UGzAdRrmIESKa+wsBCFhYUwGAzeborb/Pz8cOrUqSZfMxgMiIqKQnBwsMvtGUCJyC2FhYXo1auXrgNoeXk5goKCmnyttrYWOTk5uPLKK11ur24hPQMokdIMBoOug2dL/Pz8WuxfuwfQPXv2uLWdUzt9+JdCRM1bt26d0wU8AGCz2Rqtl5aWhmPHjjW7r+Tk5HZtW0M8hCcit2kaUFvr/vZ+fo3zpa+//hplZWUAgE2bNqFPnz647rrrUF5ejn379qG4uBgrV67EmTNnUF5ejoULF6K4uBgmkwnXXnstHnrooUbvs3r1auzfvx9FRUV4/fXXsW7dOpw8eRLBwcFYtGgRpk2bhtjYWNxyyy0YN25cq9vf7gE0Pj4e77///qXthAGUSBdqa4H773d/+w8/BEwNotDw4cMRHR2NMWPGYNOmTXj44YdxxRVX4N1334W/vz9ycnKwb98+p23Gjx+PoUOHYuLEiU0GULPZjIyMDHzxxRd4//33kZmZifj4eCQkJKCyshKlpaUYOXIkbrvttja1X5kMlEftRPrj5ydB8FK2b8hodB5ZDA8PBwBs3LgRW7ZswYsvvliXodqFhIQAkKslm2MwGKBpGpYtW4Y9e/bgkUcewQcffID09HRs27YNjz/+ONLS0lrdfmUCqBNmoES6YDA0ziAv1Q033IAlS5agpqbGaXlMTAxeeeUVfP/997j99tvbtM+77roLc+bMQX5+Pv785z/jlVdegdVqRVRUFAoLC/HKK6/Az8+vzZegM4ASkVJuuOEGbNy4EQCcxiNXr14NAJg7dy4AICEhAQCcLsX83//9X6d9bdq0CQDw2GOPOS2fN28eAEcZ04oVK9xqq7plTEREilMzgDIDJSIdUKYO1AkDKBHpgJoZKMAASkQuNSyQ93TBvCvtHkDj4+Pd2o5XIhHpkKYBNTXuP5pIlFJSUpCXlwebzYYHH3wQubm5eP7555GSkoLNmzc325zVq1dj9uzZmDp1KvLy8vDqq69izpw5WLBgAaqqqjBx4kQ888wzLe6ntXgWnojc54FK+vHjx2Pjxo3o378/EhMTYTKZUFlZie7du+O9995r9kohVwXzI0eOvKSCeVcYQInIfR6opE9ISMBf//pX7N+/Hy+99BLeeustjB07FkOHDsV9993Xqt02LJh/6KGHsGHDBrcL5l1RJoDyqJ1IhzxQSW+/71pubi4iIyNx8803Iy0tDbt370ZAQECz23qqYN4VZQKoE2agRB1a/VsG3XTTTbjpppucXrcXyDf83lXBvJ27BfOusIyJiMhNypQxNTqEZwAlIsUpU8bkhAOiRMozGAyovZTJQBVXUlICUwvjuxwDJSK3REVFIScnR9e39SgpKUFoaGiTr5lMJnTv3r3Z7dUMoAADKJHigoODm73hmh5YLBb06tXL7e2VGQN1ouNPNCLqONQNoMxAiUhxDKBERG5Spg6UR+1EpDfMQImI3KRuHSgDKBEpTs0MFGAAJSLlqRlAOSBKRDqgbgBlBkpEinMZQA8fPoyUlBQkJydj1apVdcsXLlyICRMmICUlBbm5uZ5pFQMoEemAywA6YMAApKWlYePGjdi9e3fdcpPJhICAAPj7+yMiIuJytJGISEnNHsJv2bIFo0ePxqhRo+qWPffcc0hPT8eIESPw5ptveqZVzECJSAeanUxk7NixGDt2LEaPHo1JkyYBkOn2AaBbt26wWCxO65vNZpjNZhw8eLDRa61RVdUDxcVFsObloeDnn1HRxP1S9M5qtbr1s9ET9tE3sI8tcxlAd+3ahYyMDFRWVmLUqFGYOnUq0tPT8dJLL+HUqVOwWq1Yvny50zZJSUlISkpCamoq4uLi2tyYgIAihIV1RnR0NKKvvhpwYx+qs1gsbv1s9IR99A3sY8tcBtCEhAQkJCTUfT979mwAcgjvcTyEJyIdYBkTEZGb1AygAAMoESlPzQDKK5GISAeUmc7OCQ/hiUgH1M1AGUCJSHFqTmdHRKQDzECJiNzEAEpE5CY1AygRkQ6oGUCZgRKRDjCAEhG5Sc06UIABlIiUp24GSkSkODXrQHkIT0Q6oG4GygBKRIpTM4ASEemAmgGUGSgR6QADKBGRm9QMoEREOqBmHSgzUCLSATUzUAZQItIBNetAAQZQIlKeuhkoEZHi1A2gzECJSHEMoEREblIzgBIR6QDLmIiI3KRmBsoASkQ6oG4ZExGR4piBEhG5iQGUiMhNagZQgAGUiJSnZgDllUhEpAMuA+jhw4eRkpKC5ORkrFq1qm65xWLB5MmTMXnyZFgsFs+0iofwRKQDLgPogAEDkJaWho0bN2L37t11y5ctW4aVK1fijTfewIoVKzzTKgZQItIBU3MvbtmyBatWrcLUqVPrlhUWFiIiIgIAUFxc7LS+2WyG2WzGwYMH3cpOq6p6oLi4CKdPn0ZlYCBKPJXhepHVavVc5q4I9tE3sI8tazaAjh07FmPHjsXo0aMxadIkAEB4eDgKCwthMBgQFhbmtH5SUhKSkpKQmpqKuLi4NjcmIKAIYWGdEdOzJ9C7N+DGPlRnsVjc+tnoCfvoG9jHlrkMoLt27UJGRgYqKysxatQoTJ06Fenp6XjyySfxxBNPAADmzp3r9hs3i4fwRKQDLgNoQkICEhIS6r6fPXs2ACAuLg7r16/3eMMYQIlIdSxjIiJyk7oBlBkoESlOzQAKMIASkfLUnQ+UiEhxamagPIQnIh1Qcz5QBlAi0gE1M1AiIh1QM4AyAyUiHWAAJSJyk5oBFGAAJSLlqRlAWcZERDqgbh0oM1AiUpySGWhNLQMoEalPyTrQmpp2aAgRkYcpmYHyEJ6I9ECpAPrkk2flCwZQItIBpQJoQIAETRtjJxHpgFIB1FG9xAyUiNSnVBlTXQDlITwR6YBSGahdSQkYQIlIeYqVMUnQ/HAzr0QiIvUpmYFWVfMQnojUp1QAtY+B2jQGUCJSn1IB1I6xk4j0QKkAarzYGhvLmIhIB5QMoKwDJSI9UKwOlEGTiPRDqQw0LKwWAKDBAI3XcxKR4pSqAw0OlqCpwYCSYgZQIlKbUhlofcxAiUh1agZQg4HnkIhIeUoGUI6BEpEemFy9sHnzZnz66acoKirCjBkzcPfddwMApk+fDpPJBJPJhGXLlqFTp07t3igGUCLSA5cZ6Lhx47BmzRqkpaXhgw8+qFseFBQEg8GAiIgI+Pv7t3uD+vSRZx7CE5HqWjyEX7x4MWbPnl33/cqVK7FmzRr07NkTn3zySbs3aMgQQDMYYKtlBCUitbk8hNc0DfPmzcPIkSNx44031i03XrxcqFu3bigpKXHaxmw2w2w24+DBg7BYLG1ujNVqxRVX/Bsnystx5vRpnHFjH6qzWq1u/Wz0hH30Dexjy1wG0BUrVmDHjh0oLCzEL7/8gt27dyM9PR1PP/00ysvLkZ+fjzfffNNpm6SkJCQlJSE1NRVxcXFtbozFYkGPHtdiT/BBdA7piivd2IfqLBaLWz8bPWEffQP72DKXAXTOnDmYM2dO3fcpKSkAgFdffdXtN2uN6GigxD8CeceO4kqPvhMR0aVRsoypzw0RiDQWeLsZRETNcpmBetN3RyPR61g++ni7IUREzVAyAy31j0Bodb63m0FE1CylprOzK/MPh7+tEqioaIcWERF5hpIZqM3ghzJTZ6CgwNtNISJySanp7Oor8Y8E8nkYT0TqUjIDBWQclAGUiFSmZABdtgwwRkXyEJ6IlKZkAO3cGcirjWAGSkRKUzKAhoYCF7RI2PILvN0UIiKXlAygnToBpQGRqDnHDJSI1KVkHajBANSERaLWygBKROpSMgMFgAu2CJTnMoASkbqUrQMt8ovEv78r4NT0RKQsZTPQclMYjJoNKCvzdlOIiJqkbACFwcBieiJSmrIB9LbbgIqgCAZQIlKWsgH0lluAikBejURE6lI2gGZlAXm2SJRmMwMlIjUpWQcKAF26yIxM544ygBKRmpTNQIcNk5vLnTnMAEpEalK2DjQsDCj1j8SJvQXtsj8iovam5E3l7EJjI3B9EDNQIlKTsofwAHD97ZEIKCvwdjOIiJqkdAD17xYJv+ICXs5JREpSOoAGRQWhymYCioq83RQiokaULWMCAJtmwKniCF6NRERKUjoD7dxZzsTbLhR4uylERI0oW8YEAEOGSDF95RlmoESkHqUzUIMBqAztgprT573dFCKiRpQOoABQ2jkGtTlnvN0MIqJGlA+gWVU9cHrfaW83g4ioEeUDaH5gDHJ+YAZKROpxGUA3b96Mhx9+GBMmTMC2bdvqlu/cuRPTpk3D5MmTkZub6/EG5nfqgbDqPKCqyuPvRUTUFi4D6Lhx47BmzRqkpaXhgw8+qFuelpaGt99+G/Pnz8fatWs93sDkSQEI6BEFnD3r8fciImqLFicTWbx4MWbPnl33vaZpMBqN6N27N7Kzs53WNZvNMJvNOHjwICwWS5sbY7VaG21nMgUi1xaOzl9/jfLrrmvzPlXTVB99DfvoG9jHlrkMoJqmYd68eRg5ciRuvPHGuuVGoxE2mw1ZWVmIjY112iYpKQlJSUlITU1FXFxcmxtjsVgabRceDmyt7YvRwcGAG/tUTVN99DXso29gH1vmMoCuWLECO3bsQGFhIX755Rfs3r0b6enpmDVrFmbOnInq6mosXbrU7Tdure7d5USSLfeM+me8iKhDcRlA58yZgzlz5tR9n5KSAgBITExEYmKi51t2kckkJ5Kqsw6h02V7VyKilimf1BmNwIVOMcj9kbWgRKQW5QMoIIfwZ/afA2w2bzeFiKiOLgJohSkUQVGBgNXq7aYQEdVRej5Qu+RkIODKGOA0D+OJSB26yEBDQoCi4B4MoESkFKXnA7Xbvx/45gQzUCJSiy4y0NtvlzPx2mlOKkJE6tBFAL3jDqkFtR5gBkpE6tBFADUaAa1HDPytp3EqS+PETESkBF0EUAAIvyoKtlob/jCrCBs3ers1REQ6CqCZJw244N8dURW5KCvzdmuIiHRSBwoAJSXA5zkD0Kd4P4y6CftE5Mt0E4oCAoCfI+JxTcEeGAzebg0RkU7qQAFg+nTgeOfB6F52Ap0qCj3yHkREbaGbDPTee4Fqv0BkhQ1E11N7vd0cIiL9BFC7n8PjEfCTZ8ZZiYjaQn8BNCIefvv3ArW13m4KEXVwugug+YEx6NI3Ajh82NtNIaIOTjdlTAAwdao8/6NoCPDDDx57HyKi1tBVBjpunDwfjYiH7XuOgxKRd+mmjAmQWlAAyAodhMw9Vk5vR0RepasMFACeeQawGU3YXj4c+PxzbzeHiDow3QXQYcPk+V9dR0gA5Y3miMhLdBdA7Yfx2SG/Ajp1An76ybsNIqIOS3cBFACefRaAwYDaxBHAjh3ebg4RdVC6DKChofK8/MAdwPffA8XF3m0QEXVIuqoDtbv6ann+x7+igOuuA7780uPvSUTUkK4zUACoThgBbNsGaJr3GkREHZKu6kCbkrw0HjAYgA0bLuv7EhHpMgMFUHdfJJvRBDz/PPDZZ8A333i3UUTUoeg2gAYFOb7+fH9XYN48YPlyIDPTa20ioo5FtwG0vtdfBzBokExbv3ix3ECJiMjDXAbQ48ePY8aMGUhOTnZavnDhQkyYMAEpKSnIzc31eAObU3/Y88IFAElJQFycRFSeVCIiD3MZQPv27Yu1a9c2Wm4ymRAQEAB/f39ERER4sm0tCgtzfD1tGqDBADz6KHD2LPDhh95rGBF1CG0+hH/uueeQnp6OESNG4M033/REm9rkvfccX+/YAbm8c9484G9/A/bv91q7iMj3mdq6gfHiTdm7desGi8Xi9JrZbIbZbMbBgwcbvdYaVqvVre2Ki68EACxZAhw7dgHDh5cg6N570eXZZ1E6bBgKRo2CVv+skxe520c9YR99A/vYCpoLVqtVe+SRR7S+fftqL730kjZlyhRN0zRtyZIlWkpKipacnKzl5uY2ue1TTz3larfNOnDggFvb2WyaNmaM41Hn/HlN+9OfNG3qVE375hu39t3e3O2jnrCPvoF9dNZUXHOZgXbp0gVpaWmNlj/33HPuR2sPMRiAlBTA3tzt24ERIwBER8vMI/v2yYmln34Cfvc7x5RORESXwCfKmABg1CjH18uXA999V2+q0P/4D1l45ozMyOzl6gEi8g0+E0ANBsfVSYCUg86aBdTUXFwQHg688AJw663A008DX33llXYSke/wmQAKyNVJ8+c7vj97Frj//norGAxAcjKwYAGwdi3wxhvA8eNAdfVlbysR6Z8up7Nrzk03NV7WqEkDBwLLlgEVFXLq/je/AebMAT75RJYREbWCT2WggCSZs2c7L1u0CPjnPxusGB4OpKZKJvree8BvfyuR9ne/A9avv3hpExGRa7qfzq4p99wDfPyx87KXXpIh0CaFhABDhgAvvigrXrgAPPKI48z98eNy4qmqytNNJyIdaXMhvZ68845c4mm3dy9w773y9QsvSMxspE8f4Pe/l4z0k0+At98GysuBsjIpf0pNlYlLiKjD8+kAGhUFbNkCjB3b+LUXXwTuuAOYMgXo1s3Fxr/9rTzsvvxSxkzvvhtISJBZn8rKgJgYIDZWxg+IqMPw6QAKSExzFUR37pQY+N//3cqd3XabnIBauVLqpEJD5dT/qVNSdDpokJz2HziwXftARGry+QAKSBBdt06mC21ozx45rJ82TSqcWhQd3XgwVdOkSH/PHhlDHTQImDhRJjYpL5d1YmN5BRSRj+kQARQAunQBXnsN+OIL4KOPGr/+zjtyd+Rf/QoYPBgIDm7Dzg0GOYwfOxa46y7g738HnnsOMBplR7W1QF4e0LMnIrt2Ba64AoiMbK+uEZGXtHsA9XYdaHP695fHf/0XsGoV8O23zq9nZMhzUJAMgTYxFUDLgoOBqVPlUV95OZCZCUN6uly4P2YMcNVVwIEDwKFDgL+/ZKmxsbIPo1Ey2P79gV69OL5KpKAOk4HWFxUl96ErLgYmTWr8enk5kJMjh/ajRgEzZgAnT0osc1tQEDBgAC5MmoSeEREynf6//y0z6D/8sIyhnjolb5ybK9+XlcnYQ20tcO21Elx79pQa1pISoKgICAyUhl11lQRhIrps2j2AxsfH4/3332/v3XpEWJhMXP+XvwCff970Ov/3f/IA5DLRm2+W80dRURJc+/Rx441jY4G5cxsvHzy48TJNA06fBo4ckeC6f79E/tBQoHNnCaQffQScPy+NufpqoF8/ed1oBPz8JJMNCpJ61/Bwec1gkH2XlwMmE8dnidzQITPQ+kwmKfv8/e/l+0OHZAa8prz8svP3W7dKwb69vt6eALbr0bbBIFlnz57Nr1dcDBw7BvzyiwTZsjLJXG02uTy1vBwoLZWs1WaTgFpWJtsGBUmFwd13SwB21QF7VhwSwiEFIjCANjJwoARFTWu69Kkhe2F+UJDjhPsf/yjbDxvmuXY2EhYmGWxTWWx9miZBsLxcAmFgoFQQbNsm17xWV8u+wsLk06W2Vqa0KiwE8vNlH507y7BBz56ybU6O7O/qq4H+/RFSViYnzQAZv73qKgZc8kkMoC7Yp8erqgLefRf47LPm17cHT0AO8QG5GnTYMIlDpaVy8t1q9cNbb8kl915hMEjgDAlxLIuJkTquKVMkSBYXy8Nmk2EAoxGIiJBxi6AgCZhHj0rwvPlm6VinTpIBHz2K4EOHZB2bTZb5+QH/+Z/yXjk5Mk2WpknKHhAg++3aVQLy4MHAlVe2PuBWVkpwj4zkGDBddgygzQgKksfs2cBjjwF/+APw618DrR3iXb1aHnZr1wILF16BsDAvBtDm+PlJnWt0dPPr2asFGurTB7jzTpy3WNA9Lk6WaRrw88/ADz/IJ8l11wE9esh7VVfL8MKFCzKGe+yY3AzQ3x/o3dtxoszfX4J89+6SDVut8jh/XtYJCZGsuls3CeY9esgjMlKCuH3o4fx5yYx79ZJbYLOUjC4RA2grGQzAq6/K1xMnyvPx4zKl6NGjrdvHjBmOr++9V/bz/vtSo/rKK/L/X1MjCZq9lEr3DAbgmmvk0RqaJuO4p0/LUEHnzpJlnjkjD39/YMAACfJdu8oPz2SSQJyTI48zZ4CsLBkLtp9ICwqSbeLiZKB71ixg6FAZs4mKkvc5exbIzHRkz4BsN2iQBP6QEJlc5qefEH3ypGzfq5dk60ePyh+Ev7/sLyrKuY32rD88vPUn7DSNQx+K61B1oO3F/jfdr58E1eJiuZPyH/8IfP890Nq7Pdsz2bw85+Bq9z//IyfM6yd7OTlypGtvQ36+HF3X1PjIEazB4CjYrW/AgOa3CwyUX0i/fi2/x8iR8oPbsQM4fFgy4KIi+QTr00eGJUwX/zWKiqRWd8MGGaeJiwMGD0ZFUJD84j/7TH5J11wj8yPU1Mi+8/IkIFss8nVpqWTB1dVyGDN8uLzfsWOSoRcXO/pfUACcOyf7iY2VAN63r2TdJ0/Ke0yY4PpnUlEh7TcxP/I0/oTbQViYXB4PAPfdJ+OeoaGS+GzdCrz1lrwWE+P4P2mNZ55x/Vq3bsCNNzqPzX74If9nWi0yUibSbo0xYyQbrK2t+wGXWCwSTNvqwgXgm2+ATz+VQNmvn5x8i4yU9wAkG+7eXT4ZMzMlY96/X5bddJME4iVLJCu+/XbJto8fl0/X8+fldaNR1o+JkazXfsKwpkYCbG2tI0sOC5Msv7JShkQuXADy8tD9+HHZrqJC9hMXJ4/YWOcx9Joa2c6eMdtssqymRn5eISFyccixYzIx79698qEwblzLw0X1lZY6xs3dVVEh7QoNdX8f9XToOlBP6d7d8fX998usT/ZSzAMHsnD+fBx+/FEmd3LXuXONT2zZb18SHCx/Z5MmOdrSr5/8j507J8H+448luQoL41FiqxgM7fPpFBUlAXnMmNatbw9aDSUmynjxhx9K1nzjjVI20rWrvEdFhVyQcfq0fGqXlsqyoCAJ1gaDBEp7XXGnThJgQ0Jk+759UXTVVegaFyevZWVJNr1tm2TWgYES4O0nHAMCnP+Q/P3lYQ+uNpsMZQwbBjz4oIyJP/qozClpMEjgz893nLQ0meQPOThYAnt2tryPwSD9veYa6at9HZPJsa2fn2OMPSfHcYHKmTPyoTVxomMc7hIxX7kMIiIcXxsM8refmOjIMGtqZNhv8WI5oXyp7OWdq1a5XsdeflXfwoUyPGA0ytjuwoXyN11QIH2w2STgdup06W2kSxQS0vTsOPVfb2oopA3K62fZ/fsDd94pX9tsEnzz8yVbjoxsPivUNAnegYGOIDt0qASxr76SoN61q+OkXm2tBL+yMnnYL3Pu3l2C6c8/y5hzXp4E9vo1zzabo/TOz09OKvbuLcMyPXvKoVs7jnUxgCrAZJIrNd99V74vLJSjrqoquaX9kiVyzuOrryTRsK/X3hYudP7eVR3sDTdIm6dPl4sL/vQn+R/IzZWhOjv7/8FvfiN3SeFJbx9hNLauWsPOYJA/kIaiomTMqy1MptbVO18mDKAKCg+X54AA+aDeskW+tx/1TZggz99+K6/FxsoH+Pr1jn288YZcXeWJu5D89JM8//ijPNefcxoAiouvRFiY87ING2SI4cgR+VCIj5cPjawsOafyww/S78BAObEt+5GMt6qKV5qSmhhAdWzYMOernRqeE/nLX2Scs0cPCU4VFXLk88ILcoRzOX32mfOY7c6drdvu4YeBNWtklsAdO2SmQH9/4IMPJIEZMcJxxJaXJ4/sbJmnwBV75QLHfulSsYzJh8XEyMMuMBC4/no571Bf/XLDqirggQdkXtRbb3XUpc+dK0dP+fmSFX/33eXpw5o18rxjhzw/8IDz6ytWNL3dzp0y2RXgfG+shx6S21y9+KJk7YsWyc9k9GgZzrMfla5fL0G4LSeJqeNhBkpOmVhAQOM7mgJyCN4UTQPMZjkUt2/3wAP56NGjM2pqHNuFhMiJ4MvFHjwB5xsLvv22PNe/qYB9KoCG/vY3OeGbmSm3fSkqkiGT0FAgJiYMR444hliGD5cKnUcecd2mykrHCTh7xZIns2CrVU58M9P2HJYx0SUxGOQ20oDUp1dVAeXlxXUnb+tXi1RVyYkme0VQ586Ok6zr1skl8F9+CZw4ATz1FPDnP1/27jSSmSnPixY5L9+9OxJhYdJuQOp9AbmRK+D6A+PWW6Wfn30mRwSrVsmYckmJnJsJCJDKnspKGZpwdWeE8+flZ9jUibmvvwZuuUWy7fnzJchXVDiXblL7YAZK7cZ+8sdiafr1gIDG86faTzalpspz/ftSJSY6vt6xQ2pY+/eXLO766yW4lZfLCaiNGyVj/OYb4PXXHRmet7jKtr/6yvn71sz4ZZeYKB86mzc7lv3971JRlJ4uMxIGBgJLlzqmXqw/BeOWLRKo65/gy86WI4jsbODpp5uvLz97VqqAmNE6MICSLtx1V+Nl9mA8YIDUZvv7O2psa2vlNT8/x/r2yaXsjh1zXEATFCSTxdQ3ZIhkb/arzLztH/9ovKz+mHD9CzPmz2+8bkvBetkyqSo6cQL461+BIUMiceiQjINfdZUMT4wfLwG8pkaOLrp0kekEMjKAxx+Xn/s//ymTb9WvrZ87VzLi7t0dRx92J07I0ckf/iAlekFBsm1LUwFcuOCYL+LUKccdxktKWp4+t70wgJJPaFgbXT9w2tUPnkDjy+brj/0+9picWBoyRIYoiotle/thsMWShX794pCdLcHl0CHJkH/8UdYbORLYvVuCS3q6fADccIMEiWuuaf0ENJfTt9863yds584w/PCD8zobNzq+fu8959eaGkeur6mbMDQ0ZUrTy59+2nGbsLNn5Xc3b568ds89ja/KGzlS6v7Pn5dLqc+flw+IvXtbeffdVmIAJWrCG284f9+wrhWQTMl+oc/118tz/UzZPg58662OZfYgXVAg2VqXLo7XNE3mCundW4Ymzp0DnnhC6n/vukvqen/9a0f9bUdinwmtKU3N1bt1q2Nc2u7JJ+V57165+3h7cBlAjx8/jiVLlqCwsBCbNm2qW26xWPDyxYGV+fPnI86dCRWIOrj6l/fa2S/zBuTkUZ8+zllxU9UR9R05IuVngJyBP3BAqgNqayWj7ttXrmQbOFDKs8rK5Kq32lrgX/+SGtr6wejOO4vw/fedERAgh9i/+pW8h94dONB++3IZQPv27Yu1a9ciuUG+u2zZMqxcuRIGgwFz587F6vozBhOR19iDJyAB8o475Gt/f8eMYPWFhspJIUCuCgNk8nA7i6UACxY4T5xtv7KtoEDKuqKjJejGxMjEUIDUCoeHSxZvNku97c03y6O6Wk6kmc3ygfD663JDx/XrpeSttFRenzdPLhEGpAJi/Xo5bD98WCoKzp1ztGnwYPkAaK2mxofdZdC05s9XJicnO2Wg48ePx8aLAyGTJk3ChnoFgmazGWazGd999x2GDh2KM2fOAAB69OjRqsZkZmaiTytvc9mWfXtqXXfWZx8vTzvYx0tfX299bGs7gLb9P2ZmZiIjI8N5odaCBx54wOn7mTNnagUFBVphYaE2a9asljZvk6eeeqpd96ci9tE3sI++4VL76PIQPi8vD88//zz27duHl19+GYcOHUJ6ejqefPJJPPHEEwCAua05rdYGSUlJ7bo/FbGPvoF99A2X2scWD+GJiKhpxpZXISKipihRB1paWorHHnsMAQEBSEhIwOTJk73dJLc1LP/asGEDdu7cicrKSqy6OEV8w742XCdE8YuWN2/ejE8//RRFRUWYMWMGDhw4gBMnTqC6uhppaWk4ffo0nnnmGfj5+eGhhx7CHXfcgVdffdVpHYPi1wMePnwYy5Ytg9VqxZ133onw8HCf+z2Wlpbi9ttvx8KFC3HkyBGf+x3u2rULCxYswKBBg/Dggw/ixx9/bP8+tstI7CVav369tmXLFk3TNG38+PFebk37sJ98S05O1jRN0z7++GNt/fr1Tfa14Tp6ceHCBW369OnapEmTNE3TtBUrVmhffvmltmjRIm3//v1abW2tNnHiRK2ysrLROnpRW1urTZ482Sd/jwsWLNCWLl2qffTRRz75O9y1a5d2zz33aNOmTdOOHDnikT4qcQifnZ2NXhdnovBr6ho8HbN/gvXu3RvZ2dlN9rXhOnqxePFizJw5E127dgXQuI/Gi9dO5uXlNVpHD7Zs2YLRo0dj1KhRPvd73L59OwYOHIhu3bqhsLDQJ3+Ht956K7Zu3YqlS5fi0Ucf9UgflQigsbGxdY212Wxebo1nZGVlITY2ttm+2tdRnaZpePbZZzFy5EjEx8fDarUCaNxHe/+6dOnSaB09GDt2LLZu3Yr36l307Su/x127duHbb7/Fhg0bsGHDBpy7WJnuS79De2CMjIxEeHi4R/5OlTgLX1paiscffxyBgYEYPny4rsdA7eVf27dvx8yZM9G7d2989dVXKC8vx8qL0/o07OuGDRuc1lF97Gz58uV45513EB8fj8GDB6OsrAwnT56sG/s7ffo05s2bB5PJhClTpiAxMRGvvfaa0zp6GD/LyMhAZWUlrr/+ekRGRvrc7xEA1q1bh+joaBw9etTnfocZGRkwm80oKCjAo48+ir1797Z7H5UIoEREeqTEITwRkR4xgBIRuYkBlIjITQygRERuYgAlInLT/wOBRbhVCqWXlwAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 400x300 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e1520255f514eb08a9c5d348c08db73",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint saved at step 500: checkpoints/checkpoint_step_500.pt\n",
      "Checkpoint saved at step 1000: checkpoints/checkpoint_step_1000.pt\n",
      "Checkpoint saved at step 1500: checkpoints/checkpoint_step_1500.pt\n",
      "Checkpoint saved at step 2000: checkpoints/checkpoint_step_2000.pt\n",
      "Checkpoint saved at step 2500: checkpoints/checkpoint_step_2500.pt\n",
      "Checkpoint saved at step 3000: checkpoints/checkpoint_step_3000.pt\n",
      "Checkpoint saved at step 3500: checkpoints/checkpoint_step_3500.pt\n",
      "Checkpoint saved at step 4000: checkpoints/checkpoint_step_4000.pt\n",
      "Checkpoint saved at step 4500: checkpoints/checkpoint_step_4500.pt\n",
      "final loss: 1.0219905376434326 final val loss: 1.1781994342803954\n",
      "Checkpoint saved at step 5000: checkpoints/checkpoint_step_5000.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m The nbformat package was not found. It is required to save notebook history.\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>learning_rate</td><td>â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‡â–‡â–‡â–‡â–‡â–‡â–‡â–†â–…â–…â–…â–…â–…â–„â–„â–„â–ƒâ–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–</td></tr><tr><td>step</td><td>â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆ</td></tr><tr><td>train_loss</td><td>â–ˆâ–‡â–†â–…â–„â–„â–„â–„â–ƒâ–„â–ƒâ–ƒâ–ƒâ–‚â–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–</td></tr><tr><td>val_loss</td><td>â–ˆâ–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>learning_rate</td><td>0.0</td></tr><tr><td>step</td><td>4999</td></tr><tr><td>train_loss</td><td>1.02199</td></tr><tr><td>val_loss</td><td>1.1782</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">stochastic-softpick-softmax-lm-animesubs-256seq-256embed-4head-6layer</strong> at: <a href='https://wandb.ai/erlandpg/transformers-playground/runs/bwg9fcgk' target=\"_blank\">https://wandb.ai/erlandpg/transformers-playground/runs/bwg9fcgk</a><br> View project at: <a href='https://wandb.ai/erlandpg/transformers-playground' target=\"_blank\">https://wandb.ai/erlandpg/transformers-playground</a><br>Synced 5 W&B file(s), 0 media file(s), 2 artifact file(s) and 10 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20251104_182455-bwg9fcgk/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 400x300 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "def save_notebook_to_wandb(notebook_path, run_name=None):\n",
    "    \"\"\"Save notebook without outputs to wandb\"\"\"\n",
    "    with open(notebook_path, 'r') as f:\n",
    "        nb = json.load(f)\n",
    "\n",
    "    for cell in nb['cells']:\n",
    "        if 'outputs' in cell:\n",
    "            cell['outputs'] = []\n",
    "        if 'execution_count' in cell:\n",
    "            cell['execution_count'] = None\n",
    "\n",
    "    clean_path = notebook_path.replace('.ipynb', '_clean.ipynb')\n",
    "    with open(clean_path, 'w') as f:\n",
    "        json.dump(nb, f, indent=1)\n",
    "\n",
    "    artifact = wandb.Artifact('training-notebook', type='code')\n",
    "    artifact.add_file(clean_path)\n",
    "    wandb.log_artifact(artifact)\n",
    "\n",
    "    return clean_path\n",
    "\n",
    "def get_wandb_config(model, optimizer, scheduler, seq_len, batch_size, total_steps):\n",
    "    \"\"\"Automatically extract wandb config from model and training parameters\"\"\"\n",
    "    config = {\n",
    "        \"architecture\": \"Transformer\",\n",
    "        \"dataset\": \"anime-subs\",\n",
    "        \"seq_len\": seq_len,\n",
    "        \"batch_size\": batch_size,\n",
    "        \"total_steps\": total_steps,\n",
    "        \"optimizer\": optimizer.__class__.__name__,\n",
    "        \"scheduler\": scheduler.__class__.__name__,\n",
    "        \"initial_lr\": optimizer.param_groups[0]['lr'],\n",
    "    }\n",
    "    \n",
    "    # Add model configuration\n",
    "    config.update({\n",
    "        \"vocab_size\": model.token_embedding_table.num_embeddings,\n",
    "        \"embed_size\": model.blocks[0].sa_heads.head_size * model.head_num,\n",
    "        \"head_num\": model.head_num,\n",
    "        \"layer_num\": model.layer_num,\n",
    "        \"total_params\": sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    })\n",
    "    \n",
    "    # Add scheduler-specific config\n",
    "    if hasattr(scheduler, 'T_max'):\n",
    "        config['scheduler_T_max'] = scheduler.T_max\n",
    "    if hasattr(scheduler, 'eta_min'):\n",
    "        config['scheduler_eta_min'] = scheduler.eta_min\n",
    "    \n",
    "    return config\n",
    "\n",
    "# Initialize wandb with automated config\n",
    "model = TransformerLM(config)\n",
    "if use_compile:\n",
    "    model = torch.compile(model, options=torch_compile_options, fullgraph=True)\n",
    "model.to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=2e-3, fused=True)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=total_steps, eta_min=1e-6)\n",
    "\n",
    "# warmup\n",
    "for _ in range(10):\n",
    "    model(*get_batch('train', seq_len=seq_len, batch_size=batch_size))\n",
    "\n",
    "wandb_config = get_wandb_config(model, optimizer, scheduler, seq_len, batch_size, total_steps)\n",
    "name = \"stochastic-softpick-softmax-lm-animesubs-256seq-256embed-4head-6layer\"\n",
    "if use_compile:\n",
    "    name += \"-torchcompile\"\n",
    "\n",
    "if use_wandb:\n",
    "    wandb.init(\n",
    "        project=\"transformers-playground\",\n",
    "        config=wandb_config,\n",
    "        name=name\n",
    "    )\n",
    "\n",
    "losses, val_losses = train(model, optimizer, scheduler, seq_len, batch_size, total_steps)\n",
    "\n",
    "if use_wandb:\n",
    "    save_notebook_to_wandb('./transformer_playground.ipynb')\n",
    "    wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save\n",
    "torch.save(model.state_dict(), 'models/TransformerLM.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for OptimizedModule:\n\tMissing key(s) in state_dict: \"_orig_mod.token_embedding_table.weight\", \"_orig_mod.lm_head.weight\", \"_orig_mod.lm_head.bias\", \"_orig_mod.blocks.0.sa_heads.tril\", \"_orig_mod.blocks.0.sa_heads.key.weight\", \"_orig_mod.blocks.0.sa_heads.query.weight\", \"_orig_mod.blocks.0.sa_heads.value.weight\", \"_orig_mod.blocks.0.sa_heads.o.weight\", \"_orig_mod.blocks.0.sa_heads.o.bias\", \"_orig_mod.blocks.0.ff_layer.lin_1.weight\", \"_orig_mod.blocks.0.ff_layer.lin_1.bias\", \"_orig_mod.blocks.0.ff_layer.lin_2.weight\", \"_orig_mod.blocks.0.ff_layer.lin_2.bias\", \"_orig_mod.blocks.0.sa_norm.weight\", \"_orig_mod.blocks.0.ff_norm.weight\", \"_orig_mod.blocks.1.sa_heads.tril\", \"_orig_mod.blocks.1.sa_heads.key.weight\", \"_orig_mod.blocks.1.sa_heads.query.weight\", \"_orig_mod.blocks.1.sa_heads.value.weight\", \"_orig_mod.blocks.1.sa_heads.o.weight\", \"_orig_mod.blocks.1.sa_heads.o.bias\", \"_orig_mod.blocks.1.ff_layer.lin_1.weight\", \"_orig_mod.blocks.1.ff_layer.lin_1.bias\", \"_orig_mod.blocks.1.ff_layer.lin_2.weight\", \"_orig_mod.blocks.1.ff_layer.lin_2.bias\", \"_orig_mod.blocks.1.sa_norm.weight\", \"_orig_mod.blocks.1.ff_norm.weight\", \"_orig_mod.blocks.2.sa_heads.tril\", \"_orig_mod.blocks.2.sa_heads.key.weight\", \"_orig_mod.blocks.2.sa_heads.query.weight\", \"_orig_mod.blocks.2.sa_heads.value.weight\", \"_orig_mod.blocks.2.sa_heads.o.weight\", \"_orig_mod.blocks.2.sa_heads.o.bias\", \"_orig_mod.blocks.2.ff_layer.lin_1.weight\", \"_orig_mod.blocks.2.ff_layer.lin_1.bias\", \"_orig_mod.blocks.2.ff_layer.lin_2.weight\", \"_orig_mod.blocks.2.ff_layer.lin_2.bias\", \"_orig_mod.blocks.2.sa_norm.weight\", \"_orig_mod.blocks.2.ff_norm.weight\", \"_orig_mod.blocks.3.sa_heads.tril\", \"_orig_mod.blocks.3.sa_heads.key.weight\", \"_orig_mod.blocks.3.sa_heads.query.weight\", \"_orig_mod.blocks.3.sa_heads.value.weight\", \"_orig_mod.blocks.3.sa_heads.o.weight\", \"_orig_mod.blocks.3.sa_heads.o.bias\", \"_orig_mod.blocks.3.ff_layer.lin_1.weight\", \"_orig_mod.blocks.3.ff_layer.lin_1.bias\", \"_orig_mod.blocks.3.ff_layer.lin_2.weight\", \"_orig_mod.blocks.3.ff_layer.lin_2.bias\", \"_orig_mod.blocks.3.sa_norm.weight\", \"_orig_mod.blocks.3.ff_norm.weight\", \"_orig_mod.blocks.4.sa_heads.tril\", \"_orig_mod.blocks.4.sa_heads.key.weight\", \"_orig_mod.blocks.4.sa_heads.query.weight\", \"_orig_mod.blocks.4.sa_heads.value.weight\", \"_orig_mod.blocks.4.sa_heads.o.weight\", \"_orig_mod.blocks.4.sa_heads.o.bias\", \"_orig_mod.blocks.4.ff_layer.lin_1.weight\", \"_orig_mod.blocks.4.ff_layer.lin_1.bias\", \"_orig_mod.blocks.4.ff_layer.lin_2.weight\", \"_orig_mod.blocks.4.ff_layer.lin_2.bias\", \"_orig_mod.blocks.4.sa_norm.weight\", \"_orig_mod.blocks.4.ff_norm.weight\", \"_orig_mod.blocks.5.sa_heads.tril\", \"_orig_mod.blocks.5.sa_heads.key.weight\", \"_orig_mod.blocks.5.sa_heads.query.weight\", \"_orig_mod.blocks.5.sa_heads.value.weight\", \"_orig_mod.blocks.5.sa_heads.o.weight\", \"_orig_mod.blocks.5.sa_heads.o.bias\", \"_orig_mod.blocks.5.ff_layer.lin_1.weight\", \"_orig_mod.blocks.5.ff_layer.lin_1.bias\", \"_orig_mod.blocks.5.ff_layer.lin_2.weight\", \"_orig_mod.blocks.5.ff_layer.lin_2.bias\", \"_orig_mod.blocks.5.sa_norm.weight\", \"_orig_mod.blocks.5.ff_norm.weight\". \n\tUnexpected key(s) in state_dict: \"token_embedding_table.weight\", \"lm_head.weight\", \"lm_head.bias\", \"blocks.0.sa_heads.tril\", \"blocks.0.sa_heads.key.weight\", \"blocks.0.sa_heads.query.weight\", \"blocks.0.sa_heads.value.weight\", \"blocks.0.sa_heads.o.weight\", \"blocks.0.sa_heads.o.bias\", \"blocks.0.ff_layer.lin_1.weight\", \"blocks.0.ff_layer.lin_1.bias\", \"blocks.0.ff_layer.lin_2.weight\", \"blocks.0.ff_layer.lin_2.bias\", \"blocks.0.sa_norm.weight\", \"blocks.0.ff_norm.weight\", \"blocks.1.sa_heads.tril\", \"blocks.1.sa_heads.key.weight\", \"blocks.1.sa_heads.query.weight\", \"blocks.1.sa_heads.value.weight\", \"blocks.1.sa_heads.o.weight\", \"blocks.1.sa_heads.o.bias\", \"blocks.1.ff_layer.lin_1.weight\", \"blocks.1.ff_layer.lin_1.bias\", \"blocks.1.ff_layer.lin_2.weight\", \"blocks.1.ff_layer.lin_2.bias\", \"blocks.1.sa_norm.weight\", \"blocks.1.ff_norm.weight\", \"blocks.2.sa_heads.tril\", \"blocks.2.sa_heads.key.weight\", \"blocks.2.sa_heads.query.weight\", \"blocks.2.sa_heads.value.weight\", \"blocks.2.sa_heads.o.weight\", \"blocks.2.sa_heads.o.bias\", \"blocks.2.ff_layer.lin_1.weight\", \"blocks.2.ff_layer.lin_1.bias\", \"blocks.2.ff_layer.lin_2.weight\", \"blocks.2.ff_layer.lin_2.bias\", \"blocks.2.sa_norm.weight\", \"blocks.2.ff_norm.weight\", \"blocks.3.sa_heads.tril\", \"blocks.3.sa_heads.key.weight\", \"blocks.3.sa_heads.query.weight\", \"blocks.3.sa_heads.value.weight\", \"blocks.3.sa_heads.o.weight\", \"blocks.3.sa_heads.o.bias\", \"blocks.3.ff_layer.lin_1.weight\", \"blocks.3.ff_layer.lin_1.bias\", \"blocks.3.ff_layer.lin_2.weight\", \"blocks.3.ff_layer.lin_2.bias\", \"blocks.3.sa_norm.weight\", \"blocks.3.ff_norm.weight\", \"blocks.4.sa_heads.tril\", \"blocks.4.sa_heads.key.weight\", \"blocks.4.sa_heads.query.weight\", \"blocks.4.sa_heads.value.weight\", \"blocks.4.sa_heads.o.weight\", \"blocks.4.sa_heads.o.bias\", \"blocks.4.ff_layer.lin_1.weight\", \"blocks.4.ff_layer.lin_1.bias\", \"blocks.4.ff_layer.lin_2.weight\", \"blocks.4.ff_layer.lin_2.bias\", \"blocks.4.sa_norm.weight\", \"blocks.4.ff_norm.weight\", \"blocks.5.sa_heads.tril\", \"blocks.5.sa_heads.key.weight\", \"blocks.5.sa_heads.query.weight\", \"blocks.5.sa_heads.value.weight\", \"blocks.5.sa_heads.o.weight\", \"blocks.5.sa_heads.o.bias\", \"blocks.5.ff_layer.lin_1.weight\", \"blocks.5.ff_layer.lin_1.bias\", \"blocks.5.ff_layer.lin_2.weight\", \"blocks.5.ff_layer.lin_2.bias\", \"blocks.5.sa_norm.weight\", \"blocks.5.ff_norm.weight\". ",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[19]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      2\u001b[39m model = TransformerLM(config)\n\u001b[32m      3\u001b[39m model = torch.compile(model, options=torch_compile_options, fullgraph=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload_state_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mmodels/TransformerLM.pt\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      5\u001b[39m model.to(device)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Python_project/transformers_playground/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:2629\u001b[39m, in \u001b[36mModule.load_state_dict\u001b[39m\u001b[34m(self, state_dict, strict, assign)\u001b[39m\n\u001b[32m   2621\u001b[39m         error_msgs.insert(\n\u001b[32m   2622\u001b[39m             \u001b[32m0\u001b[39m,\n\u001b[32m   2623\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mMissing key(s) in state_dict: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[33m. \u001b[39m\u001b[33m\"\u001b[39m.format(\n\u001b[32m   2624\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33m, \u001b[39m\u001b[33m\"\u001b[39m.join(\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\u001b[33m'\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m missing_keys)\n\u001b[32m   2625\u001b[39m             ),\n\u001b[32m   2626\u001b[39m         )\n\u001b[32m   2628\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(error_msgs) > \u001b[32m0\u001b[39m:\n\u001b[32m-> \u001b[39m\u001b[32m2629\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m   2630\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mError(s) in loading state_dict for \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[33m:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[33m\"\u001b[39m.format(\n\u001b[32m   2631\u001b[39m             \u001b[38;5;28mself\u001b[39m.\u001b[34m__class__\u001b[39m.\u001b[34m__name__\u001b[39m, \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[33m\"\u001b[39m.join(error_msgs)\n\u001b[32m   2632\u001b[39m         )\n\u001b[32m   2633\u001b[39m     )\n\u001b[32m   2634\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m _IncompatibleKeys(missing_keys, unexpected_keys)\n",
      "\u001b[31mRuntimeError\u001b[39m: Error(s) in loading state_dict for OptimizedModule:\n\tMissing key(s) in state_dict: \"_orig_mod.token_embedding_table.weight\", \"_orig_mod.lm_head.weight\", \"_orig_mod.lm_head.bias\", \"_orig_mod.blocks.0.sa_heads.tril\", \"_orig_mod.blocks.0.sa_heads.key.weight\", \"_orig_mod.blocks.0.sa_heads.query.weight\", \"_orig_mod.blocks.0.sa_heads.value.weight\", \"_orig_mod.blocks.0.sa_heads.o.weight\", \"_orig_mod.blocks.0.sa_heads.o.bias\", \"_orig_mod.blocks.0.ff_layer.lin_1.weight\", \"_orig_mod.blocks.0.ff_layer.lin_1.bias\", \"_orig_mod.blocks.0.ff_layer.lin_2.weight\", \"_orig_mod.blocks.0.ff_layer.lin_2.bias\", \"_orig_mod.blocks.0.sa_norm.weight\", \"_orig_mod.blocks.0.ff_norm.weight\", \"_orig_mod.blocks.1.sa_heads.tril\", \"_orig_mod.blocks.1.sa_heads.key.weight\", \"_orig_mod.blocks.1.sa_heads.query.weight\", \"_orig_mod.blocks.1.sa_heads.value.weight\", \"_orig_mod.blocks.1.sa_heads.o.weight\", \"_orig_mod.blocks.1.sa_heads.o.bias\", \"_orig_mod.blocks.1.ff_layer.lin_1.weight\", \"_orig_mod.blocks.1.ff_layer.lin_1.bias\", \"_orig_mod.blocks.1.ff_layer.lin_2.weight\", \"_orig_mod.blocks.1.ff_layer.lin_2.bias\", \"_orig_mod.blocks.1.sa_norm.weight\", \"_orig_mod.blocks.1.ff_norm.weight\", \"_orig_mod.blocks.2.sa_heads.tril\", \"_orig_mod.blocks.2.sa_heads.key.weight\", \"_orig_mod.blocks.2.sa_heads.query.weight\", \"_orig_mod.blocks.2.sa_heads.value.weight\", \"_orig_mod.blocks.2.sa_heads.o.weight\", \"_orig_mod.blocks.2.sa_heads.o.bias\", \"_orig_mod.blocks.2.ff_layer.lin_1.weight\", \"_orig_mod.blocks.2.ff_layer.lin_1.bias\", \"_orig_mod.blocks.2.ff_layer.lin_2.weight\", \"_orig_mod.blocks.2.ff_layer.lin_2.bias\", \"_orig_mod.blocks.2.sa_norm.weight\", \"_orig_mod.blocks.2.ff_norm.weight\", \"_orig_mod.blocks.3.sa_heads.tril\", \"_orig_mod.blocks.3.sa_heads.key.weight\", \"_orig_mod.blocks.3.sa_heads.query.weight\", \"_orig_mod.blocks.3.sa_heads.value.weight\", \"_orig_mod.blocks.3.sa_heads.o.weight\", \"_orig_mod.blocks.3.sa_heads.o.bias\", \"_orig_mod.blocks.3.ff_layer.lin_1.weight\", \"_orig_mod.blocks.3.ff_layer.lin_1.bias\", \"_orig_mod.blocks.3.ff_layer.lin_2.weight\", \"_orig_mod.blocks.3.ff_layer.lin_2.bias\", \"_orig_mod.blocks.3.sa_norm.weight\", \"_orig_mod.blocks.3.ff_norm.weight\", \"_orig_mod.blocks.4.sa_heads.tril\", \"_orig_mod.blocks.4.sa_heads.key.weight\", \"_orig_mod.blocks.4.sa_heads.query.weight\", \"_orig_mod.blocks.4.sa_heads.value.weight\", \"_orig_mod.blocks.4.sa_heads.o.weight\", \"_orig_mod.blocks.4.sa_heads.o.bias\", \"_orig_mod.blocks.4.ff_layer.lin_1.weight\", \"_orig_mod.blocks.4.ff_layer.lin_1.bias\", \"_orig_mod.blocks.4.ff_layer.lin_2.weight\", \"_orig_mod.blocks.4.ff_layer.lin_2.bias\", \"_orig_mod.blocks.4.sa_norm.weight\", \"_orig_mod.blocks.4.ff_norm.weight\", \"_orig_mod.blocks.5.sa_heads.tril\", \"_orig_mod.blocks.5.sa_heads.key.weight\", \"_orig_mod.blocks.5.sa_heads.query.weight\", \"_orig_mod.blocks.5.sa_heads.value.weight\", \"_orig_mod.blocks.5.sa_heads.o.weight\", \"_orig_mod.blocks.5.sa_heads.o.bias\", \"_orig_mod.blocks.5.ff_layer.lin_1.weight\", \"_orig_mod.blocks.5.ff_layer.lin_1.bias\", \"_orig_mod.blocks.5.ff_layer.lin_2.weight\", \"_orig_mod.blocks.5.ff_layer.lin_2.bias\", \"_orig_mod.blocks.5.sa_norm.weight\", \"_orig_mod.blocks.5.ff_norm.weight\". \n\tUnexpected key(s) in state_dict: \"token_embedding_table.weight\", \"lm_head.weight\", \"lm_head.bias\", \"blocks.0.sa_heads.tril\", \"blocks.0.sa_heads.key.weight\", \"blocks.0.sa_heads.query.weight\", \"blocks.0.sa_heads.value.weight\", \"blocks.0.sa_heads.o.weight\", \"blocks.0.sa_heads.o.bias\", \"blocks.0.ff_layer.lin_1.weight\", \"blocks.0.ff_layer.lin_1.bias\", \"blocks.0.ff_layer.lin_2.weight\", \"blocks.0.ff_layer.lin_2.bias\", \"blocks.0.sa_norm.weight\", \"blocks.0.ff_norm.weight\", \"blocks.1.sa_heads.tril\", \"blocks.1.sa_heads.key.weight\", \"blocks.1.sa_heads.query.weight\", \"blocks.1.sa_heads.value.weight\", \"blocks.1.sa_heads.o.weight\", \"blocks.1.sa_heads.o.bias\", \"blocks.1.ff_layer.lin_1.weight\", \"blocks.1.ff_layer.lin_1.bias\", \"blocks.1.ff_layer.lin_2.weight\", \"blocks.1.ff_layer.lin_2.bias\", \"blocks.1.sa_norm.weight\", \"blocks.1.ff_norm.weight\", \"blocks.2.sa_heads.tril\", \"blocks.2.sa_heads.key.weight\", \"blocks.2.sa_heads.query.weight\", \"blocks.2.sa_heads.value.weight\", \"blocks.2.sa_heads.o.weight\", \"blocks.2.sa_heads.o.bias\", \"blocks.2.ff_layer.lin_1.weight\", \"blocks.2.ff_layer.lin_1.bias\", \"blocks.2.ff_layer.lin_2.weight\", \"blocks.2.ff_layer.lin_2.bias\", \"blocks.2.sa_norm.weight\", \"blocks.2.ff_norm.weight\", \"blocks.3.sa_heads.tril\", \"blocks.3.sa_heads.key.weight\", \"blocks.3.sa_heads.query.weight\", \"blocks.3.sa_heads.value.weight\", \"blocks.3.sa_heads.o.weight\", \"blocks.3.sa_heads.o.bias\", \"blocks.3.ff_layer.lin_1.weight\", \"blocks.3.ff_layer.lin_1.bias\", \"blocks.3.ff_layer.lin_2.weight\", \"blocks.3.ff_layer.lin_2.bias\", \"blocks.3.sa_norm.weight\", \"blocks.3.ff_norm.weight\", \"blocks.4.sa_heads.tril\", \"blocks.4.sa_heads.key.weight\", \"blocks.4.sa_heads.query.weight\", \"blocks.4.sa_heads.value.weight\", \"blocks.4.sa_heads.o.weight\", \"blocks.4.sa_heads.o.bias\", \"blocks.4.ff_layer.lin_1.weight\", \"blocks.4.ff_layer.lin_1.bias\", \"blocks.4.ff_layer.lin_2.weight\", \"blocks.4.ff_layer.lin_2.bias\", \"blocks.4.sa_norm.weight\", \"blocks.4.ff_norm.weight\", \"blocks.5.sa_heads.tril\", \"blocks.5.sa_heads.key.weight\", \"blocks.5.sa_heads.query.weight\", \"blocks.5.sa_heads.value.weight\", \"blocks.5.sa_heads.o.weight\", \"blocks.5.sa_heads.o.bias\", \"blocks.5.ff_layer.lin_1.weight\", \"blocks.5.ff_layer.lin_1.bias\", \"blocks.5.ff_layer.lin_2.weight\", \"blocks.5.ff_layer.lin_2.bias\", \"blocks.5.sa_norm.weight\", \"blocks.5.ff_norm.weight\". "
     ]
    }
   ],
   "source": [
    "# Load model\n",
    "model = TransformerLM(config)\n",
    "model = torch.compile(model, options=torch_compile_options, fullgraph=True)\n",
    "model.load_state_dict(torch.load('models/TransformerLM.pt'))\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a2d3725208974a81aaab15dcbea1d72e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inductor Compilation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 15/15 [00:00<00:00, 2132.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "perplexity: 3.2360689640045166 loss: 1.174359375\n"
     ]
    }
   ],
   "source": [
    "# calculate perplexity\n",
    "ppl, loss = perplexity(model, seq_len, seq_len)\n",
    "print(\"perplexity:\", ppl, \"loss:\", loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "00r0pbm3b5eX",
    "outputId": "bdd3b34e-32a0-4724-b528-c162c0fd0c3e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[53, 72, 78,  1, 80, 66, 69, 69,  1, 71, 62, 79, 62, 75]])\n",
      "You will never die without the hotels and bear contact.\n",
      "\n",
      "You won't be able to worry about this.\n",
      "\n",
      "I want to become a monster in the same height as I was talking about.\n",
      "\n",
      "He was really pretty cool.\n",
      "\n",
      "What is this place you get better than that?\n",
      "\n",
      "This is my customer.\n",
      "\n",
      "I can still stand by your side.\n",
      "\n",
      "Thank you, Haru.\n",
      "\n",
      "I was able to see you today.\n",
      "\n",
      "You're not a person who can see the path of the time you picked out the terrorist of the local state.\n",
      "\n",
      "I can use the corner and the others with her too long.\n",
      "\n",
      "It was my fault that I can do it.\n",
      "\n",
      "Too bad that I was wrong.\n",
      "\n",
      "I wanted to see it.\n",
      "\n",
      "I was the one who told you that I was searching for you.\n",
      "\n",
      "I see. I was the only one who could destroy the test.\n",
      "\n",
      "I was wondering why I was watching over the power,\n",
      "\n",
      "but I can't see a thing of helping me out of the prince.\n",
      "\n",
      "I wanted to know the people that store in the white sword.\n",
      "\n",
      "That's why I was a little assigned to be close to the front line.\n",
      "\n",
      "I heard that your power is also a manga that they are for a man who are going\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "idx = encode(\"You will never\")\n",
    "print(torch.tensor([idx]))\n",
    "print(decode(model.generate(idx=torch.tensor([idx], dtype=torch.long).to(device), max_new_tokens=1000, temperature=0.5, use_cache=True)[0].tolist()))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 0
}