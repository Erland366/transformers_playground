{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "sdtDsu1Y0EqL"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/vast/users/qirong.ho/miniforge3/envs/flame/lib/python3.11/site-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'repr' attribute with value False was provided to the `Field()` function, which has no effect in the context it was used. 'repr' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n",
      "  warnings.warn(\n",
      "/vast/users/qirong.ho/miniforge3/envs/flame/lib/python3.11/site-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'frozen' attribute with value True was provided to the `Field()` function, which has no effect in the context it was used. 'frozen' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n",
      "  warnings.warn(\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /vast/users/qirong.ho/.netrc\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33merlandpg\u001b[0m to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wandb initialized successfully\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=5)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import math\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "from torch.nn.attention.flex_attention import flex_attention, create_block_mask\n",
    "import triton\n",
    "import triton.language as tl\n",
    "from tqdm.notebook import tqdm\n",
    "from dotenv import load_dotenv\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython import display\n",
    "import wandb\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "use_wandb = True # set to False to disable wandb logging\n",
    "try:\n",
    "    import wandb\n",
    "    wandb_api_key = os.getenv('WANDB_API_KEY')\n",
    "    if wandb_api_key:  \n",
    "        wandb.login(key=wandb_api_key)\n",
    "        use_wandb = use_wandb and True\n",
    "        print(\"wandb initialized successfully\")\n",
    "    else:\n",
    "        print(\"WANDB_API_KEY not found - wandb logging disabled\")\n",
    "except ImportError:\n",
    "    print(\"wandb not installed - wandb logging disabled\")\n",
    "    wandb = None\n",
    "\n",
    "torch.manual_seed(69)\n",
    "torch.set_printoptions(profile=\"short\", sci_mode=False, linewidth=100000)\n",
    "torch.set_float32_matmul_precision('high')\n",
    "# this script is configured to run on a RTX 3060 12GB GPU. you'll want to adjust the model sizes and batch sizes for other devices\n",
    "device = torch.device('cuda:5' if torch.cuda.is_available() else 'cpu')\n",
    "# device = torch.device('cpu')\n",
    "plt.rcParams['figure.figsize'] = [8, 6]\n",
    "plt.rcParams['figure.dpi'] = 50\n",
    "plt.rcParams['axes.grid'] = True\n",
    "plt.rcParams['xtick.minor.visible'] = True\n",
    "plt.rcParams['ytick.minor.visible'] = True\n",
    "\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ANr5dn7W0EqR",
    "outputId": "a00cbe8d-e1c6-454b-b552-4ffd17ce17e5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of dataset in characters:  39526018\n"
     ]
    }
   ],
   "source": [
    "# we use this 40mb file of concatenated anime subtitles as our dataset\n",
    "# just the right size for toy experiments like this I think\n",
    "with open('animesubs.txt', 'r', encoding='latin') as f:\n",
    "    text = f.read()\n",
    "print(\"length of dataset in characters: \", len(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pANiObIZ0EqU",
    "outputId": "98f70469-1c89-4341-89e8-c142a14331b1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Open your mind. Open your mind.\n",
      "\n",
      "Far beyond the deep blue Earth, you and I shall meet...\n",
      "\n",
      "AH! MY GODDESS\n",
      "\n",
      "A snow-white feather comes fluttering down, swaying gently in the air.\n",
      "\n",
      "Without holding back, I want to envelope you, my one and only love.\n",
      "\n",
      "I know I have the power to protect the one I love, right here in my hands.\n",
      "\n",
      "Open your mind. Just as I've always dreamed.\n",
      "\n",
      "Let the wind carry off your hopes, faraway.\n",
      "\n",
      "I have wings nobody can see. Look, you have them, too.\n",
      "\n",
      "They'll take us to where we ca\n"
     ]
    }
   ],
   "source": [
    "print(text[:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "WvM5h6_i3KsM"
   },
   "outputs": [],
   "source": [
    "# remove japanese characters\n",
    "text = ''.join(filter(lambda character:ord(character) < 0x3000, text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "U7SOcWJM0EqW",
    "outputId": "a82a4da8-a8ed-47bf-cfb5-2e4c59dee2cf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unique characters: 86 \n",
      " !'()*+,-./0123456789:;<=>?@ABCDEFGHIJKLMNOPQRSTUVWXYZ[]_abcdefghijklmnopqrstuvwxyz|\n"
     ]
    }
   ],
   "source": [
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "print(\"unique characters:\", vocab_size, ''.join(chars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yobmmaeK0EqX",
    "outputId": "26669f38-4b26-4b53-f642-ee7543e7c578"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'\\n': 0, ' ': 1, '!': 2, \"'\": 3, '(': 4, ')': 5, '*': 6, '+': 7, ',': 8, '-': 9, '.': 10, '/': 11, '0': 12, '1': 13, '2': 14, '3': 15, '4': 16, '5': 17, '6': 18, '7': 19, '8': 20, '9': 21, ':': 22, ';': 23, '<': 24, '=': 25, '>': 26, '?': 27, '@': 28, 'A': 29, 'B': 30, 'C': 31, 'D': 32, 'E': 33, 'F': 34, 'G': 35, 'H': 36, 'I': 37, 'J': 38, 'K': 39, 'L': 40, 'M': 41, 'N': 42, 'O': 43, 'P': 44, 'Q': 45, 'R': 46, 'S': 47, 'T': 48, 'U': 49, 'V': 50, 'W': 51, 'X': 52, 'Y': 53, 'Z': 54, '[': 55, ']': 56, '_': 57, 'a': 58, 'b': 59, 'c': 60, 'd': 61, 'e': 62, 'f': 63, 'g': 64, 'h': 65, 'i': 66, 'j': 67, 'k': 68, 'l': 69, 'm': 70, 'n': 71, 'o': 72, 'p': 73, 'q': 74, 'r': 75, 's': 76, 't': 77, 'u': 78, 'v': 79, 'w': 80, 'x': 81, 'y': 82, 'z': 83, '|': 84, '\\x94': 85, '': 86}\n",
      "{0: '\\n', 1: ' ', 2: '!', 3: \"'\", 4: '(', 5: ')', 6: '*', 7: '+', 8: ',', 9: '-', 10: '.', 11: '/', 12: '0', 13: '1', 14: '2', 15: '3', 16: '4', 17: '5', 18: '6', 19: '7', 20: '8', 21: '9', 22: ':', 23: ';', 24: '<', 25: '=', 26: '>', 27: '?', 28: '@', 29: 'A', 30: 'B', 31: 'C', 32: 'D', 33: 'E', 34: 'F', 35: 'G', 36: 'H', 37: 'I', 38: 'J', 39: 'K', 40: 'L', 41: 'M', 42: 'N', 43: 'O', 44: 'P', 45: 'Q', 46: 'R', 47: 'S', 48: 'T', 49: 'U', 50: 'V', 51: 'W', 52: 'X', 53: 'Y', 54: 'Z', 55: '[', 56: ']', 57: '_', 58: 'a', 59: 'b', 60: 'c', 61: 'd', 62: 'e', 63: 'f', 64: 'g', 65: 'h', 66: 'i', 67: 'j', 68: 'k', 69: 'l', 70: 'm', 71: 'n', 72: 'o', 73: 'p', 74: 'q', 75: 'r', 76: 's', 77: 't', 78: 'u', 79: 'v', 80: 'w', 81: 'x', 82: 'y', 83: 'z', 84: '|', 85: '\\x94', 86: ''}\n",
      "encoded: [43, 73, 62, 71, 1, 82, 72, 78, 75, 1, 70, 66, 71, 61, 10, 1, 43, 73, 62, 71]\n",
      "decoded: Open your mind. Open\n",
      "vocab size: 87\n"
     ]
    }
   ],
   "source": [
    "# yes, all language models will be character level, which isn't ideal but it's good for simplicity\n",
    "# very simple tokenizer\n",
    "stoi = {ch:i for i,ch in enumerate(chars)}\n",
    "itos = {i:ch for i,ch in enumerate(chars)}\n",
    "# add special token for padding\n",
    "stoi[''] = len(stoi)\n",
    "itos[len(itos)] = ''\n",
    "print(stoi)\n",
    "print(itos)\n",
    "encode = lambda s: [stoi[ch] for ch in s]\n",
    "decode = lambda l: ''.join([itos[i] for i in l])\n",
    "print(\"encoded:\", encode(text[:20]))\n",
    "print(\"decoded:\", decode(encode(text[:20])))\n",
    "vocab_size = len(itos)\n",
    "print(\"vocab size:\", vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6Pnf9KfP0EqY",
    "outputId": "ff581168-335a-4f0f-efeb-0d4bd5b225ca"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([39526018])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = torch.tensor(encode(text), dtype=torch.int64)\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lZ2fY1pR0EqY",
    "outputId": "5eb599e0-fb79-4cdb-c4b9-52a781d67151"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([43, 73, 62, 71,  1, 82, 72, 78, 75,  1, 70, 66, 71, 61, 10,  1, 43, 73, 62, 71,  1, 82, 72, 78, 75,  1, 70, 66, 71, 61, 10,  0,  0, 34, 58, 75,  1, 59, 62, 82, 72, 71, 61,  1, 77, 65, 62,  1, 61, 62, 62, 73,  1, 59, 69, 78, 62,  1, 33, 58, 75, 77, 65,  8,  1, 82, 72, 78,  1, 58, 71, 61,  1, 37,  1, 76, 65, 58, 69, 69,  1, 70, 62, 62, 77, 10, 10, 10,  0,  0, 29, 36,  2,  1, 41, 53,  1, 35, 43, 32])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oIaYesPh0Eqa",
    "outputId": "caa27cbb-5ae3-4406-8194-646264d4ba99"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([39130757]) torch.Size([395261])\n"
     ]
    }
   ],
   "source": [
    "n = int(len(data) * 0.99)\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]\n",
    "print(train_data.shape, val_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4bFhizcI0Eqa",
    "outputId": "c93a4d43-6fb2-4de7-aefc-8fed47e4a861"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([43, 73, 62, 71,  1, 82, 72, 78, 75])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq_len = 8\n",
    "train_data[:seq_len+1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "skFCPvQC0Eqc",
    "outputId": "08990c5b-88af-4a51-ce37-3c59989d6d74"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs:\n",
      "torch.Size([2, 64])\n",
      "tensor([[64,  1, 72, 78, 75,  1, 66, 71, 77, 62, 75, 71, 76, 65, 66, 73, 76, 10,  0,  0, 48, 65, 62,  1, 71, 66, 64, 65, 77,  1, 37, 66, 61, 58,  1, 64, 72, 77,  1, 77, 65, 62,  1, 75, 62, 76, 78, 69, 77, 76,  1, 72, 63,  1, 77, 65, 62,  1, 62, 81, 58, 70, 66, 71],\n",
      "        [62, 70,  2,  0,  0, 37, 77,  1, 69, 72, 72, 68, 76,  1, 69, 66, 68, 62,  1, 80, 62,  1, 77, 72, 72, 68,  1, 60, 58, 75, 62,  1, 72, 63,  1, 77, 65, 62, 70, 10,  0,  0, 48, 65, 72, 76, 62,  1, 64, 78, 82, 76,  1, 80, 62, 75, 62,  1, 66, 71, 60, 75, 62, 61]], device='cuda:5')\n",
      "targets:\n",
      "torch.Size([2, 64])\n",
      "tensor([[ 1, 72, 78, 75,  1, 66, 71, 77, 62, 75, 71, 76, 65, 66, 73, 76, 10,  0,  0, 48, 65, 62,  1, 71, 66, 64, 65, 77,  1, 37, 66, 61, 58,  1, 64, 72, 77,  1, 77, 65, 62,  1, 75, 62, 76, 78, 69, 77, 76,  1, 72, 63,  1, 77, 65, 62,  1, 62, 81, 58, 70, 66, 71, 58],\n",
      "        [70,  2,  0,  0, 37, 77,  1, 69, 72, 72, 68, 76,  1, 69, 66, 68, 62,  1, 80, 62,  1, 77, 72, 72, 68,  1, 60, 58, 75, 62,  1, 72, 63,  1, 77, 65, 62, 70, 10,  0,  0, 48, 65, 72, 76, 62,  1, 64, 78, 82, 76,  1, 80, 62, 75, 62,  1, 66, 71, 60, 75, 62, 61, 66]], device='cuda:5')\n"
     ]
    }
   ],
   "source": [
    "def get_batch(split, seq_len, batch_size=4):\n",
    "    # generate a small batch of data of inputs x and targets y\n",
    "    # targets are just inputs shifted by 1\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - seq_len, (batch_size,))\n",
    "    x = torch.stack([data[i:i+seq_len] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+seq_len+1] for i in ix])\n",
    "    return x.to(device), y.to(device)\n",
    "\n",
    "xb, yb = get_batch('train', 64, 2)\n",
    "print('inputs:')\n",
    "print(xb.shape)\n",
    "print(xb)\n",
    "print('targets:')\n",
    "print(yb.shape)\n",
    "print(yb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "327680000"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Make all steps, sequence lengths, and batch size the same\n",
    "total_steps = 5000\n",
    "seq_len = 256\n",
    "batch_size = 256 # these are small models so we can use large batch sizes to fully utilize the GPU\n",
    "# should cover around 2x the dataset\n",
    "total_steps * seq_len * batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test forward pass\n",
    "torch_compile_options = {\n",
    "    'epilogue_fusion': True, \n",
    "    'max_autotune': False, \n",
    "    'shape_padding': True, \n",
    "    'trace.enabled': False, \n",
    "    'triton.cudagraphs': False, \n",
    "    'debug': False, \n",
    "    'dce': True, \n",
    "    'memory_planning': True, \n",
    "    'coordinate_descent_tuning': False, \n",
    "    'trace.graph_diagram': False, \n",
    "    'compile_threads': 32, \n",
    "    'group_fusion': True, \n",
    "    'disable_progress': False, \n",
    "    'verbose_progress': False, \n",
    "    'triton.multi_kernel': 0, \n",
    "    'triton.use_block_ptr': False, \n",
    "    'triton.enable_persistent_tma_matmul': False, \n",
    "    'triton.autotune_at_compile_time': False, \n",
    "    'triton.cooperative_reductions': False, \n",
    "    'cuda.compile_opt_level': '-O2', \n",
    "    'cuda.enable_cuda_lto': True, \n",
    "    'combo_kernels': True, \n",
    "    'benchmark_combo_kernel': True, \n",
    "    'combo_kernel_foreach_dynamic_shapes': True\n",
    "}\n",
    "\n",
    "@torch.compile(fullgraph=False, options=torch_compile_options)\n",
    "def compile_optimizer_lr(opt, scheduler):\n",
    "    opt.step()\n",
    "    scheduler.step()\n",
    "\n",
    "\n",
    "def save_checkpoint(model, optimizer, scheduler, step, losses, val_losses, seq_len, batch_size, total_steps, save_dir='checkpoints'):\n",
    "    \"\"\"Save a complete checkpoint including model, optimizer, scheduler states and training metrics\"\"\"\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    \n",
    "    checkpoint = {\n",
    "        'step': step,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'scheduler_state_dict': scheduler.state_dict(),\n",
    "        'train_losses': losses,\n",
    "        'val_losses': val_losses,\n",
    "        'config': {\n",
    "            'seq_len': seq_len,\n",
    "            'batch_size': batch_size,\n",
    "            'total_steps': total_steps,\n",
    "            'vocab_size': model.token_embedding_table.num_embeddings,\n",
    "            'embed_size': model.blocks[0].sa_heads.head_size * model.head_num,\n",
    "            'head_num': model.head_num,\n",
    "            'layer_num': model.layer_num\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    checkpoint_path = os.path.join(save_dir, f'checkpoint_step_{step}.pt')\n",
    "    torch.save(checkpoint, checkpoint_path)\n",
    "    wandb.save(checkpoint_path)\n",
    "    print(f\"Checkpoint saved at step {step}: {checkpoint_path}\")\n",
    "    \n",
    "    return checkpoint_path\n",
    "\n",
    "def load_checkpoint(model, optimizer, scheduler, checkpoint_path):\n",
    "    \"\"\"Load a checkpoint and restore model, optimizer, scheduler states\"\"\"\n",
    "    checkpoint = torch.load(checkpoint_path, map_location=device)\n",
    "    \n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n",
    "    \n",
    "    print(f\"Checkpoint loaded from step {checkpoint['step']}\")\n",
    "    return checkpoint['step'], checkpoint['train_losses'], checkpoint['val_losses']\n",
    "\n",
    "def train(model, optimizer, scheduler, seq_len, batch_size, total_steps, val_steps=10, val_interval=50, checkpoint_interval=500, save_dir='checkpoints'):\n",
    "    losses = []\n",
    "    val_losses = []\n",
    "    # Create save directory\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    \n",
    "    # live plot\n",
    "    fig, ax = plt.subplots()\n",
    "    dh = display.display(fig, display_id=True)\n",
    "    \n",
    "    for step in (bar := tqdm(range(total_steps))):\n",
    "        # sample a batch of data\n",
    "        xb, yb = get_batch('train', seq_len=seq_len, batch_size=batch_size)\n",
    "\n",
    "        # evaluate the loss\n",
    "        logits, loss = model(xb, yb)\n",
    "        # backprop\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "        with torch._dynamo.compiled_autograd._enable(torch.compile()):\n",
    "            loss.backward()\n",
    "        compile_optimizer_lr(optimizer, scheduler)\n",
    "\n",
    "        bar.set_description(f\"loss: {loss.item():.2f}, val loss: {val_losses[-1] if val_losses else 0:.2f}, lr: {scheduler.get_last_lr()[0]:.2e}\")\n",
    "        losses.append(loss.item())\n",
    "        \n",
    "        # Log to wandb\n",
    "        if use_wandb:\n",
    "            wandb.log({\n",
    "                'train_loss': loss.item(),\n",
    "                'learning_rate': scheduler.get_last_lr()[0],\n",
    "                'step': step\n",
    "            })\n",
    "        \n",
    "        if step % val_interval == 0:\n",
    "            # Calculate validation loss\n",
    "            with torch.no_grad():\n",
    "                val_loss = 0\n",
    "                for _ in range(val_steps):\n",
    "                    xb, yb = get_batch('val', seq_len=seq_len, batch_size=batch_size)\n",
    "                    _, loss = model(xb, yb)\n",
    "                    val_loss += loss.item()\n",
    "                val_loss /= val_steps\n",
    "                val_losses.append(val_loss)\n",
    "                \n",
    "            # Log validation loss to wandb\n",
    "            if use_wandb:\n",
    "                wandb.log({\n",
    "                    'val_loss': val_loss,\n",
    "                    'step': step\n",
    "                })\n",
    "            \n",
    "            ax.clear()\n",
    "            ax.plot(losses, color='blue', label='train loss', alpha=0.7)\n",
    "            ax.plot(range(0, len(losses), val_interval), val_losses, color='red', label='val loss', alpha=0.7)\n",
    "            ax.set_ylim(1, 4)\n",
    "            ax.legend()\n",
    "            dh.update(fig)\n",
    "            \n",
    "        # Save checkpoint\n",
    "        if step % checkpoint_interval == 0 and step > 0:\n",
    "            save_checkpoint(model, optimizer, scheduler, step, losses, val_losses, seq_len, batch_size, total_steps, save_dir)\n",
    "            \n",
    "    print('final loss:', loss.item(), 'final val loss:', val_loss)\n",
    "    \n",
    "    # Save final checkpoint\n",
    "    save_checkpoint(model, optimizer, scheduler, total_steps, losses, val_losses, seq_len, batch_size, total_steps, save_dir)\n",
    "    \n",
    "    return losses, val_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Measure post training perplexity on validation set\n",
    "# Create function that receives a model, context length, and PPL sequence length, and returns the perplexity\n",
    "# The PPL sequence length is the number of characters the function uses to calculate the perplexity\n",
    "# We take the logits and calculate the cross entropy loss from scratch, then exponentiate it to get the perplexity\n",
    "# not only that, but we want the models to do this in actual inference\n",
    "def perplexity(model, seq_len, ppl_seq_len, batch_size=128, val_steps=1000):\n",
    "    with torch.no_grad():\n",
    "        val_loss = 0\n",
    "        for _ in tqdm(range(val_steps)):\n",
    "            xb, yb = get_batch('val', seq_len=seq_len, batch_size=batch_size)\n",
    "            with torch.autocast(device_type=\"cuda\", dtype=torch.bfloat16):\n",
    "                logits, _ = model(xb, yb)\n",
    "            logits = logits.reshape(batch_size, seq_len, vocab_size)\n",
    "            logits = logits[:, :ppl_seq_len]\n",
    "            yb = yb[:, :ppl_seq_len]\n",
    "            # flatten logits and targets\n",
    "            logits = logits.reshape(batch_size*ppl_seq_len, vocab_size)\n",
    "            yb = yb.reshape(batch_size*ppl_seq_len)\n",
    "            # calculate cross entropy loss from scratch\n",
    "            loss = F.cross_entropy(logits, yb)\n",
    "            val_loss += loss.item()\n",
    "        val_loss /= val_steps\n",
    "        ppl = torch.exp(torch.tensor(val_loss))\n",
    "        return ppl.item(), val_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classic Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "k6cA2WbrayyL"
   },
   "outputs": [],
   "source": [
    "class TransformerConfig:\n",
    "    def __init__(self, vocab_size, seq_len, embed_size, head_num, layer_num):\n",
    "        self.vocab_size = vocab_size\n",
    "        self.seq_len = seq_len\n",
    "        self.embed_size = embed_size\n",
    "        self.head_num = head_num\n",
    "        self.layer_num = layer_num\n",
    "\n",
    "def precompute_freqs_cis(dim: int, end: int, theta: float = 10000.0):\n",
    "    freqs = 1.0 / (theta ** (torch.arange(0, dim, 2)[: (dim // 2)].float() / dim))\n",
    "    t = torch.arange(end, device=freqs.device, dtype=torch.float32)\n",
    "    freqs = torch.outer(t, freqs)\n",
    "    freqs_cis = torch.polar(torch.ones_like(freqs), freqs)  # complex64\n",
    "    return freqs_cis.to(device)\n",
    "\n",
    "def apply_rotary_emb(\n",
    "    xq: torch.Tensor,\n",
    "    xk: torch.Tensor,\n",
    "    freqs_cis: torch.Tensor,\n",
    "):\n",
    "    xq_ = torch.view_as_complex(xq.float().reshape(*xq.shape[:-1], -1, 2))\n",
    "    xk_ = torch.view_as_complex(xk.float().reshape(*xk.shape[:-1], -1, 2))\n",
    "    # freqs_cis = reshape_for_broadcast(freqs_cis, xq_)\n",
    "    q_shape = [d if i == xq_.ndim - 2 or i == xq_.ndim - 1 else 1 for i, d in enumerate(xq_.shape)]\n",
    "    k_shape = [d if i == xq_.ndim - 2 or i == xk_.ndim - 1 else 1 for i, d in enumerate(xk_.shape)]\n",
    "    T_q = xq_.shape[-2] \n",
    "    q_freqs_cis = freqs_cis[-T_q:].view(*q_shape)\n",
    "    k_freqs_cis = freqs_cis.view(*k_shape)\n",
    "    xq_out = torch.view_as_real(xq_ * q_freqs_cis).flatten(xq.dim() - 1)\n",
    "    xk_out = torch.view_as_real(xk_ * k_freqs_cis).flatten(xq.dim() - 1)\n",
    "    return xq_out.type_as(xq), xk_out.type_as(xk)\n",
    "\n",
    "class RMSNorm(torch.nn.Module):\n",
    "    def __init__(self, dim, eps=1e-6):\n",
    "        super().__init__()\n",
    "        self.eps = eps\n",
    "        self.weight = nn.Parameter(torch.ones(dim))\n",
    "\n",
    "    def _norm(self, x):\n",
    "        return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)\n",
    "\n",
    "    def forward(self, x):\n",
    "        output = self._norm(x.float()).type_as(x)\n",
    "        return output * self.weight\n",
    "    \n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.lin_1 = nn.Linear(config.embed_size, config.embed_size*4)\n",
    "        self.lin_2 = nn.Linear(config.embed_size*4, config.embed_size)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.lin_1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.lin_2(x)\n",
    "        return x\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\" multiple heads of self-attention with AliBi in parallel \"\"\"\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.seq_len = config.seq_len\n",
    "        self.head_num = config.head_num\n",
    "        self.head_size = config.embed_size // config.head_num\n",
    "        self.key = nn.Linear(config.embed_size, config.embed_size, bias=False)\n",
    "        self.query = nn.Linear(config.embed_size, config.embed_size, bias=False)\n",
    "        self.value = nn.Linear(config.embed_size, config.embed_size, bias=False)\n",
    "        self.o = nn.Linear(config.embed_size, config.embed_size)\n",
    "        # block_mask for FlexAttention\n",
    "        def causal(b, h, q_idx, kv_idx):\n",
    "            causal_mask = q_idx >= kv_idx\n",
    "            return causal_mask\n",
    "        self.causal_mask = create_block_mask(causal, B=None, H=None, Q_LEN=config.seq_len, KV_LEN=config.seq_len)\n",
    "        self.freqs_cis = precompute_freqs_cis(config.embed_size//config.head_num, config.seq_len)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(config.seq_len, config.seq_len)))\n",
    "\n",
    "    def forward(self, x, kv_cache=None):\n",
    "        B, T, C = x.shape\n",
    "        _, _, T_past, _ = kv_cache[0].shape if kv_cache is not None and kv_cache[0] is not None else (0, 0, 0, 0)\n",
    "        q = self.query(x) # (B,T,C)\n",
    "        k = self.key(x)   # (B,T,C)\n",
    "        v = self.value(x) # (B,T,C)\n",
    "\n",
    "        # Split into heads\n",
    "        q = q.view(B, T, self.head_num, self.head_size).transpose(1, 2) # (B, H, T, C/H)\n",
    "        k = k.view(B, T, self.head_num, self.head_size).transpose(1, 2) # (B, H, T, C/H)\n",
    "        v = v.view(B, T, self.head_num, self.head_size).transpose(1, 2) # (B, H, T, C/H)\n",
    "\n",
    "        if kv_cache is not None:\n",
    "            k_past, v_past = kv_cache\n",
    "            if k_past is not None:\n",
    "                k = torch.cat((k_past, k), dim=2)\n",
    "                v = torch.cat((v_past, v), dim=2)\n",
    "            if k.shape[-2] > self.seq_len:\n",
    "                k = k[:, :, -self.seq_len:]\n",
    "                v = v[:, :, -self.seq_len:]\n",
    "            kv_cache = (k, v)\n",
    "        T_k = k.shape[-2]\n",
    "        q, k = apply_rotary_emb(q, k, self.freqs_cis[:T_k])\n",
    "\n",
    "        # compute attention scores (\"affinities\")\n",
    "        wei = q @ k.transpose(-2,-1) # (B, H, 1, C/H) @ (B, H, C/H, T) -> (B, H, 1, T)\n",
    "        wei = wei * self.head_size ** -0.5 # scaled attention\n",
    "        wei = wei.masked_fill(self.tril[T_k-T:T_k, T_k-T:T_k] == 0, float('-inf')) # (B, T, T)\n",
    "        wei = F.softmax(wei, dim=-1) # (B, H, T, T)\n",
    "        # apply attention to values\n",
    "        out = wei @ v # (B, H, 1, T) @ (B, H, T, C/H) -> (B, H, 1, C/H)\n",
    "\n",
    "        out = out.transpose(1, 2).contiguous().view(B, T, C) # (B, H, T, C/H) -> (B, T, H, C/H) -> (B, T, C)\n",
    "        out = self.o(out)\n",
    "        return out, kv_cache\n",
    "\n",
    "class Block(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.sa_heads = MultiHeadAttention(config)\n",
    "        self.ff_layer = FeedForward(config)\n",
    "        self.sa_norm = RMSNorm(config.embed_size)\n",
    "        self.ff_norm = RMSNorm(config.embed_size)\n",
    "    \n",
    "    def forward(self, x, kv_cache=None):\n",
    "        a, kv_cache = self.sa_heads(self.sa_norm(x), kv_cache)\n",
    "        h = x + a\n",
    "        o = h + self.ff_layer(self.ff_norm(h))\n",
    "        return o, kv_cache\n",
    "    \n",
    "class TransformerLM(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.layer_num = config.layer_num\n",
    "        self.head_num = config.head_num\n",
    "        self.seq_len = config.seq_len\n",
    "        # embed raw tokens to a lower dimensional embedding with embed_size\n",
    "        self.token_embedding_table = nn.Embedding(config.vocab_size, config.embed_size)\n",
    "        # Language Modelling (?) Head is a standard linear layer to go from \n",
    "        # embeddings back to logits of vocab_size\n",
    "        self.lm_head = nn.Linear(config.embed_size, config.vocab_size)\n",
    "        # transformer blocks\n",
    "        self.blocks = nn.ModuleList([Block(config) for _ in range(config.layer_num)])\n",
    "\n",
    "    def forward(self, idx, targets=None, kv_cache=None):\n",
    "        B, T = idx.shape\n",
    "        _, _, T_past, _ = kv_cache[0][0].shape if kv_cache is not None and kv_cache[0][0] is not None else (0, 0, 0, 0)\n",
    "        # idx and targets are both (B,T) tensor of integers\n",
    "        tok_embd = self.token_embedding_table(idx) # (B,T,C)\n",
    "        x = tok_embd\n",
    "        # go through blocks\n",
    "        for i, block in enumerate(self.blocks):\n",
    "            x, cache = block(x, None if kv_cache is None else kv_cache[i])\n",
    "            if kv_cache is not None:\n",
    "                kv_cache[i] = cache\n",
    "        # get logits with linear layer\n",
    "        logits = self.lm_head(x) # (B,T,V)\n",
    "        \n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, V = logits.shape\n",
    "            logits = logits.view(B*T, V)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "    \n",
    "    def generate(self, idx, max_new_tokens, temperature=1, use_cache=True):\n",
    "        if use_cache:\n",
    "            # initialize key-value cache\n",
    "            kv_cache = [(None, None) for _ in range(self.layer_num)]\n",
    "            # idx is (B, T) array of indices in the current context\n",
    "            # crop idx to the last seq_len tokens\n",
    "            idx_context = idx[:, -self.seq_len:]\n",
    "            for _ in range(max_new_tokens):\n",
    "                # get the predictions\n",
    "                logits, loss = self(idx_context, kv_cache=kv_cache)\n",
    "                # focus only on the last time step\n",
    "                logits = logits[:, -1, :] # becomes (B, C)\n",
    "                # apply temperature\n",
    "                logits = logits / temperature if temperature > 0 else logits\n",
    "                # apply softmax to get probabilities\n",
    "                probs = F.softmax(logits, dim=-1) # (B, C)\n",
    "                # sample from the distribution\n",
    "                idx_next = torch.multinomial(probs, num_samples=1) if temperature > 0 else torch.argmax(probs, dim=-1, keepdim=True) # (B, 1)\n",
    "                # append sampled index to the running sequence\n",
    "                idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
    "                # since we have kv cache, only need to pass new token\n",
    "                idx_context = idx_next\n",
    "            return idx\n",
    "        else:\n",
    "            # idx is (B, T) array of indices in the current context\n",
    "            for _ in range(max_new_tokens):\n",
    "                #crop idx to the last seq_len tokens\n",
    "                idx_context = idx[:, -self.seq_len:]\n",
    "                # get the predictions\n",
    "                logits, loss = self(idx_context)\n",
    "                # focus only on the last time step\n",
    "                logits = logits[:, -1, :] # becomes (B, C)\n",
    "                # apply temperature\n",
    "                logits = logits / temperature if temperature > 0 else logits\n",
    "                # apply softmax to get probabilities\n",
    "                probs = F.softmax(logits, dim=-1) # (B, C)\n",
    "                # sample from the distribution\n",
    "                idx_next = torch.multinomial(probs, num_samples=1) if temperature > 0 else torch.argmax(probs, dim=-1, keepdim=True) # (B, 1)\n",
    "                # append sampled index to the running sequence\n",
    "                idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
    "            return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/vast/users/qirong.ho/miniforge3/envs/flame/lib/python3.11/site-packages/torch/_inductor/lowering.py:1890: UserWarning: Torchinductor does not support code generation for complex operators. Performance may be worse than eager.\n",
      "  warnings.warn(\n",
      "/vast/users/qirong.ho/miniforge3/envs/flame/lib/python3.11/site-packages/torch/_inductor/compile_fx.py:282: UserWarning: TensorFloat32 tensor cores for float32 matrix multiplication available but not enabled. Consider setting `torch.set_float32_matmul_precision('high')` for better performance.\n",
      "  warnings.warn(\n",
      "/vast/users/qirong.ho/miniforge3/envs/flame/lib/python3.11/site-packages/torch/_inductor/lowering.py:7095: UserWarning: \n",
      "Online softmax is disabled on the fly since Inductor decides to\n",
      "split the reduction. Cut an issue to PyTorch if this is an\n",
      "important use case and you want to speed it up with online\n",
      "softmax.\n",
      "\n",
      "  warnings.warn(\n",
      "Inductor Compilation: 0it [00:00, ?it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "4775511"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "config = TransformerConfig(\n",
    "    vocab_size=vocab_size,\n",
    "    seq_len=seq_len,\n",
    "    embed_size=256,\n",
    "    head_num=4,\n",
    "    layer_num=6\n",
    ")\n",
    "m = TransformerLM(config)\n",
    "m = torch.compile(m, options=torch_compile_options, fullgraph=True)\n",
    "m.to(device)\n",
    "xb, yb = get_batch('train', 5, 1)\n",
    "logits, loss = m(xb, yb)\n",
    "total_params = sum(p.numel() for p in m.parameters() if p.requires_grad)\n",
    "total_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inductor Compilation: 100%|██████████| 14/14 [00:00<00:00, 30.97it/s]\n",
      "W1215 12:33:26.302000 1998175 site-packages/torch/_inductor/codegen/triton_combo_kernel.py:110] [0/1] ComboKernels: 1 large pointwise nodes are separated\n",
      "W1215 12:33:31.071000 1998175 site-packages/torch/_inductor/codegen/triton_combo_kernel.py:110] [0/1] ComboKernels: 1 large pointwise nodes are separated\n",
      "W1215 12:33:31.483000 1998175 site-packages/torch/_inductor/codegen/triton_combo_kernel.py:110] [0/1] ComboKernels: 1 large pointwise nodes are separated\n",
      "W1215 12:33:31.632000 1998175 site-packages/torch/_inductor/codegen/triton_combo_kernel.py:110] [0/1] ComboKernels: 1 large pointwise nodes are separated\n",
      "W1215 12:33:31.781000 1998175 site-packages/torch/_inductor/codegen/triton_combo_kernel.py:110] [0/1] ComboKernels: 1 large pointwise nodes are separated\n",
      "W1215 12:33:31.931000 1998175 site-packages/torch/_inductor/codegen/triton_combo_kernel.py:110] [0/1] ComboKernels: 1 large pointwise nodes are separated\n",
      "W1215 12:33:39.307000 1998175 site-packages/torch/_inductor/codegen/triton_combo_kernel.py:110] [0/1] ComboKernels: 1 large pointwise nodes are separated\n",
      "W1215 12:33:39.450000 1998175 site-packages/torch/_inductor/codegen/triton_combo_kernel.py:110] [0/1] ComboKernels: 1 large pointwise nodes are separated\n",
      "W1215 12:33:39.556000 1998175 site-packages/torch/_inductor/codegen/triton_combo_kernel.py:110] [0/1] ComboKernels: 1 large pointwise nodes are separated\n",
      "W1215 12:33:39.658000 1998175 site-packages/torch/_inductor/codegen/triton_combo_kernel.py:110] [0/1] ComboKernels: 1 large pointwise nodes are separated\n",
      "W1215 12:33:39.761000 1998175 site-packages/torch/_inductor/codegen/triton_combo_kernel.py:110] [0/1] ComboKernels: 1 large pointwise nodes are separated\n",
      "W1215 12:33:39.866000 1998175 site-packages/torch/_inductor/codegen/triton_combo_kernel.py:110] [0/1] ComboKernels: 1 large pointwise nodes are separated\n",
      "Inductor Compilation: 100%|██████████| 23/23 [00:00<00:00, 52.00it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.22.2"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/vast/users/qirong.ho/torch_rocm/flame/resources/transformers_playground_vanilla_softmax/wandb/run-20251215_123359-d3x6pzpc</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/erlandpg/transformers-playground/runs/d3x6pzpc' target=\"_blank\">torch_softmax-lm-animesubs-256seq-256embed-4head-6layer.AMD</a></strong> to <a href='https://wandb.ai/erlandpg/transformers-playground' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/erlandpg/transformers-playground' target=\"_blank\">https://wandb.ai/erlandpg/transformers-playground</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/erlandpg/transformers-playground/runs/d3x6pzpc' target=\"_blank\">https://wandb.ai/erlandpg/transformers-playground/runs/d3x6pzpc</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 400x300 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fcc16ed8dac247f18b6ff162be110557",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1215 12:34:03.404000 1998175 site-packages/torch/fx/experimental/symbolic_shapes.py:6823] [!0/1/0] _maybe_guard_rel() was called on non-relation expression Eq(s30, 1) | Eq(s32, s30)\n",
      "W1215 12:34:04.056000 1998175 site-packages/torch/fx/experimental/symbolic_shapes.py:6823] [!0/1/0] _maybe_guard_rel() was called on non-relation expression Eq(s27, s63) | Eq(s63, 1)\n",
      "W1215 12:34:04.799000 1998175 site-packages/torch/fx/experimental/symbolic_shapes.py:6823] [!0/1/0] _maybe_guard_rel() was called on non-relation expression Eq(s15, s60) | Eq(s60, 1)\n",
      "W1215 12:34:05.175000 1998175 site-packages/torch/fx/experimental/symbolic_shapes.py:6823] [!0/1/0] _maybe_guard_rel() was called on non-relation expression Eq(s15, s93) | Eq(s93, 1)\n",
      "W1215 12:34:05.719000 1998175 site-packages/torch/fx/experimental/symbolic_shapes.py:6823] [!0/1/0] _maybe_guard_rel() was called on non-relation expression Eq(s12, s7) | Eq(s7, 1)\n",
      "W1215 12:34:05.972000 1998175 site-packages/torch/fx/experimental/symbolic_shapes.py:6823] [!0/1/0] _maybe_guard_rel() was called on non-relation expression Eq(s12, s9) | Eq(s9, 1)\n",
      "W1215 12:34:14.363000 1998175 site-packages/torch/_logging/_internal.py:1154] [3/0] Profiler function <class 'torch.autograd.profiler.record_function'> will be ignored\n",
      "Inductor Compilation: 100%|██████████| 15/15 [00:00<00:00, 26.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint saved at step 500: checkpoints/checkpoint_step_500.pt\n",
      "Checkpoint saved at step 1000: checkpoints/checkpoint_step_1000.pt\n",
      "Checkpoint saved at step 1500: checkpoints/checkpoint_step_1500.pt\n",
      "Checkpoint saved at step 2000: checkpoints/checkpoint_step_2000.pt\n",
      "Checkpoint saved at step 2500: checkpoints/checkpoint_step_2500.pt\n",
      "Checkpoint saved at step 3000: checkpoints/checkpoint_step_3000.pt\n",
      "Checkpoint saved at step 3500: checkpoints/checkpoint_step_3500.pt\n",
      "Checkpoint saved at step 4000: checkpoints/checkpoint_step_4000.pt\n",
      "Checkpoint saved at step 4500: checkpoints/checkpoint_step_4500.pt\n",
      "final loss: 1.0334129333496094 final val loss: 1.1813583016395568\n",
      "Checkpoint saved at step 5000: checkpoints/checkpoint_step_5000.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m The nbformat package was not found. It is required to save notebook history.\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>learning_rate</td><td>████▇▇▇▇▇▆▆▆▆▅▅▅▅▅▄▄▄▃▃▃▃▃▂▂▂▂▂▂▂▂▁▁▁▁▁▁</td></tr><tr><td>step</td><td>▁▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▃▃▃▄▄▄▄▄▅▆▆▆▆▇▇▇▇▇▇▇███</td></tr><tr><td>train_loss</td><td>█▄▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>learning_rate</td><td>0.0</td></tr><tr><td>step</td><td>4999</td></tr><tr><td>train_loss</td><td>1.03341</td></tr><tr><td>val_loss</td><td>1.18136</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">torch_softmax-lm-animesubs-256seq-256embed-4head-6layer.AMD</strong> at: <a href='https://wandb.ai/erlandpg/transformers-playground/runs/d3x6pzpc' target=\"_blank\">https://wandb.ai/erlandpg/transformers-playground/runs/d3x6pzpc</a><br> View project at: <a href='https://wandb.ai/erlandpg/transformers-playground' target=\"_blank\">https://wandb.ai/erlandpg/transformers-playground</a><br>Synced 5 W&B file(s), 0 media file(s), 2 artifact file(s) and 10 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20251215_123359-d3x6pzpc/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 400x300 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "def save_notebook_to_wandb(notebook_path, run_name=None):\n",
    "    \"\"\"Save notebook without outputs to wandb\"\"\"\n",
    "    with open(notebook_path, 'r') as f:\n",
    "        nb = json.load(f)\n",
    "\n",
    "    for cell in nb['cells']:\n",
    "        if 'outputs' in cell:\n",
    "            cell['outputs'] = []\n",
    "        if 'execution_count' in cell:\n",
    "            cell['execution_count'] = None\n",
    "\n",
    "    clean_path = notebook_path.replace('.ipynb', '_clean.ipynb')\n",
    "    with open(clean_path, 'w') as f:\n",
    "        json.dump(nb, f, indent=1)\n",
    "\n",
    "    artifact = wandb.Artifact('training-notebook', type='code')\n",
    "    artifact.add_file(clean_path)\n",
    "    wandb.log_artifact(artifact)\n",
    "\n",
    "    return clean_path\n",
    "\n",
    "def get_wandb_config(model, optimizer, scheduler, seq_len, batch_size, total_steps):\n",
    "    \"\"\"Automatically extract wandb config from model and training parameters\"\"\"\n",
    "    config = {\n",
    "        \"architecture\": \"Transformer\",\n",
    "        \"dataset\": \"anime-subs\",\n",
    "        \"seq_len\": seq_len,\n",
    "        \"batch_size\": batch_size,\n",
    "        \"total_steps\": total_steps,\n",
    "        \"optimizer\": optimizer.__class__.__name__,\n",
    "        \"scheduler\": scheduler.__class__.__name__,\n",
    "        \"initial_lr\": optimizer.param_groups[0]['lr'],\n",
    "    }\n",
    "    \n",
    "    # Add model configuration\n",
    "    config.update({\n",
    "        \"vocab_size\": model.token_embedding_table.num_embeddings,\n",
    "        \"embed_size\": model.blocks[0].sa_heads.head_size * model.head_num,\n",
    "        \"head_num\": model.head_num,\n",
    "        \"layer_num\": model.layer_num,\n",
    "        \"total_params\": sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    })\n",
    "    \n",
    "    # Add scheduler-specific config\n",
    "    if hasattr(scheduler, 'T_max'):\n",
    "        config['scheduler_T_max'] = scheduler.T_max\n",
    "    if hasattr(scheduler, 'eta_min'):\n",
    "        config['scheduler_eta_min'] = scheduler.eta_min\n",
    "    \n",
    "    return config\n",
    "\n",
    "# Initialize wandb with automated config\n",
    "model = TransformerLM(config)\n",
    "model = torch.compile(model, options=torch_compile_options, fullgraph=True)\n",
    "model.to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=2e-3, fused=True)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=total_steps, eta_min=1e-6)\n",
    "\n",
    "# warmup\n",
    "for _ in range(10):\n",
    "    model(*get_batch('train', seq_len=seq_len, batch_size=batch_size))\n",
    "\n",
    "wandb_config = get_wandb_config(model, optimizer, scheduler, seq_len, batch_size, total_steps)\n",
    "wandb.init(\n",
    "    project=\"transformers-playground\",\n",
    "    config=wandb_config,\n",
    "    name=\"torch_softmax-lm-animesubs-256seq-256embed-4head-6layer.AMD\"\n",
    ")\n",
    "\n",
    "losses, val_losses = train(model, optimizer, scheduler, seq_len, batch_size, total_steps)\n",
    "\n",
    "if use_wandb:\n",
    "    save_notebook_to_wandb('./transformer_playground.ipynb')\n",
    "    wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Parent directory models does not exist.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[18]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Save\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstate_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mmodels/TransformerLM.pt\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/flame/lib/python3.11/site-packages/torch/serialization.py:966\u001b[39m, in \u001b[36msave\u001b[39m\u001b[34m(obj, f, pickle_module, pickle_protocol, _use_new_zipfile_serialization, _disable_byteorder_record)\u001b[39m\n\u001b[32m    963\u001b[39m     f = os.fspath(f)\n\u001b[32m    965\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m _use_new_zipfile_serialization:\n\u001b[32m--> \u001b[39m\u001b[32m966\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43m_open_zipfile_writer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m opened_zipfile:\n\u001b[32m    967\u001b[39m         _save(\n\u001b[32m    968\u001b[39m             obj,\n\u001b[32m    969\u001b[39m             opened_zipfile,\n\u001b[32m   (...)\u001b[39m\u001b[32m    972\u001b[39m             _disable_byteorder_record,\n\u001b[32m    973\u001b[39m         )\n\u001b[32m    974\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/flame/lib/python3.11/site-packages/torch/serialization.py:828\u001b[39m, in \u001b[36m_open_zipfile_writer\u001b[39m\u001b[34m(name_or_buffer)\u001b[39m\n\u001b[32m    826\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    827\u001b[39m     container = _open_zipfile_writer_buffer\n\u001b[32m--> \u001b[39m\u001b[32m828\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcontainer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname_or_buffer\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/flame/lib/python3.11/site-packages/torch/serialization.py:792\u001b[39m, in \u001b[36m_open_zipfile_writer_file.__init__\u001b[39m\u001b[34m(self, name)\u001b[39m\n\u001b[32m    785\u001b[39m     \u001b[38;5;28msuper\u001b[39m().\u001b[34m__init__\u001b[39m(\n\u001b[32m    786\u001b[39m         torch._C.PyTorchFileWriter(\n\u001b[32m    787\u001b[39m             \u001b[38;5;28mself\u001b[39m.file_stream, get_crc32_options(), _get_storage_alignment()\n\u001b[32m    788\u001b[39m         )\n\u001b[32m    789\u001b[39m     )\n\u001b[32m    790\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    791\u001b[39m     \u001b[38;5;28msuper\u001b[39m().\u001b[34m__init__\u001b[39m(\n\u001b[32m--> \u001b[39m\u001b[32m792\u001b[39m         \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_C\u001b[49m\u001b[43m.\u001b[49m\u001b[43mPyTorchFileWriter\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    793\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mget_crc32_options\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_get_storage_alignment\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    794\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    795\u001b[39m     )\n",
      "\u001b[31mRuntimeError\u001b[39m: Parent directory models does not exist."
     ]
    }
   ],
   "source": [
    "# Save\n",
    "torch.save(model.state_dict(), 'models/TransformerLM.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OptimizedModule(\n",
       "  (_orig_mod): TransformerLM(\n",
       "    (token_embedding_table): Embedding(87, 256)\n",
       "    (lm_head): Linear(in_features=256, out_features=87, bias=True)\n",
       "    (blocks): ModuleList(\n",
       "      (0-5): 6 x Block(\n",
       "        (sa_heads): MultiHeadAttention(\n",
       "          (key): Linear(in_features=256, out_features=256, bias=False)\n",
       "          (query): Linear(in_features=256, out_features=256, bias=False)\n",
       "          (value): Linear(in_features=256, out_features=256, bias=False)\n",
       "          (o): Linear(in_features=256, out_features=256, bias=True)\n",
       "        )\n",
       "        (ff_layer): FeedForward(\n",
       "          (lin_1): Linear(in_features=256, out_features=1024, bias=True)\n",
       "          (lin_2): Linear(in_features=1024, out_features=256, bias=True)\n",
       "          (relu): ReLU()\n",
       "        )\n",
       "        (sa_norm): RMSNorm()\n",
       "        (ff_norm): RMSNorm()\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load model\n",
    "model = TransformerLM(config)\n",
    "model = torch.compile(model, options=torch_compile_options, fullgraph=True)\n",
    "model.load_state_dict(torch.load('models/TransformerLM.pt'))\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b2696da7fac426783fac8eb8a8bd6fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inductor Compilation: 100%|██████████| 21/21 [00:00<00:00, 27.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "perplexity: 3.2762105464935303 loss: 1.1866875\n"
     ]
    }
   ],
   "source": [
    "# calculate perplexity\n",
    "ppl, loss = perplexity(model, seq_len, seq_len)\n",
    "print(\"perplexity:\", ppl, \"loss:\", loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "00r0pbm3b5eX",
    "outputId": "bdd3b34e-32a0-4724-b528-c162c0fd0c3e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[53, 72, 78,  1, 80, 66, 69, 69,  1, 71, 62, 79, 62, 75]])\n",
      "You will never do this myself.\n",
      "\n",
      "Makes sense, the world is a little while ago.\n",
      "\n",
      "I was surprised that every day was at peace,\n",
      "\n",
      "but I still have the same way to my girlfriend.\n",
      "\n",
      "The one I want to see you again today.\n",
      "\n",
      "I was worried about the sports match,\n",
      "\n",
      "but I was able to talk to you about your clothes.\n",
      "\n",
      "I was thinking I want to talk to you about it too.\n",
      "\n",
      "That way I was the one who told me to help you.\n",
      "\n",
      "I was so surprised as a trainer won't be able to realize the enemy for the cost of the stars.\n",
      "\n",
      "I was able to help you know how to handle the cartency to the contract.\n",
      "\n",
      "I won't let you off the rest of the train so you can call me a movie.\n",
      "\n",
      "But we didn't think that we could come out to the end.\n",
      "\n",
      "I wanted to tell you that.\n",
      "\n",
      "And you're so cool.\n",
      "\n",
      "You want to know what the world is all the best, right?\n",
      "\n",
      "It's the same as my concern.\n",
      "\n",
      "I think that if I can get them out of the second chapter that I was a bit really mature.\n",
      "\n",
      "The second year of the world is dead.\n",
      "\n",
      "It was a great successful for a man who can get a\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "idx = encode(\"You will never\")\n",
    "print(torch.tensor([idx]))\n",
    "print(decode(model.generate(idx=torch.tensor([idx], dtype=torch.long).to(device), max_new_tokens=1000, temperature=0.5, use_cache=True)[0].tolist()))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "flame",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
