{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%env CUDA_VISIBLE_DEVICES=1\n",
    "%env ROCM_VISIBLE_DEVICES=1\n",
    "%env HIP_VISIBLE_DEVICES=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "sdtDsu1Y0EqL"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /home/coder/.netrc\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33merlandpg\u001b[0m to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wandb initialized successfully\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/coder/Python_project/transformers_playground/.venv/lib/python3.12/site-packages/torch/__init__.py:1617: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n",
      "  _C._set_float32_matmul_precision(precision)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import math\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "from torch.nn.attention.flex_attention import flex_attention, create_block_mask\n",
    "import triton\n",
    "import triton.language as tl\n",
    "from tqdm.notebook import tqdm\n",
    "from dotenv import load_dotenv\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython import display\n",
    "import wandb\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "use_wandb = True # set to False to disable wandb logging\n",
    "try:\n",
    "    import wandb\n",
    "    wandb_api_key = os.getenv('WANDB_API_KEY')\n",
    "    if wandb_api_key:  \n",
    "        wandb.login(key=wandb_api_key)\n",
    "        use_wandb = use_wandb and True\n",
    "        print(\"wandb initialized successfully\")\n",
    "    else:\n",
    "        print(\"WANDB_API_KEY not found - wandb logging disabled\")\n",
    "except ImportError:\n",
    "    print(\"wandb not installed - wandb logging disabled\")\n",
    "    wandb = None\n",
    "\n",
    "torch.manual_seed(69)\n",
    "torch.set_printoptions(profile=\"short\", sci_mode=False, linewidth=100000)\n",
    "torch.set_float32_matmul_precision('high')\n",
    "# this script is configured to run on a RTX 3060 12GB GPU. you'll want to adjust the model sizes and batch sizes for other devices\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# device = torch.device('cpu')\n",
    "plt.rcParams['figure.figsize'] = [8, 6]\n",
    "plt.rcParams['figure.dpi'] = 50\n",
    "plt.rcParams['axes.grid'] = True\n",
    "plt.rcParams['xtick.minor.visible'] = True\n",
    "plt.rcParams['ytick.minor.visible'] = True\n",
    "\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ANr5dn7W0EqR",
    "outputId": "a00cbe8d-e1c6-454b-b552-4ffd17ce17e5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of dataset in characters:  39526018\n"
     ]
    }
   ],
   "source": [
    "# we use this 40mb file of concatenated anime subtitles as our dataset\n",
    "# just the right size for toy experiments like this I think\n",
    "with open('animesubs.txt', 'r', encoding='latin') as f:\n",
    "    text = f.read()\n",
    "print(\"length of dataset in characters: \", len(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pANiObIZ0EqU",
    "outputId": "98f70469-1c89-4341-89e8-c142a14331b1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Open your mind. Open your mind.\n",
      "\n",
      "Far beyond the deep blue Earth, you and I shall meet...\n",
      "\n",
      "AH! MY GODDESS\n",
      "\n",
      "A snow-white feather comes fluttering down, swaying gently in the air.\n",
      "\n",
      "Without holding back, I want to envelope you, my one and only love.\n",
      "\n",
      "I know I have the power to protect the one I love, right here in my hands.\n",
      "\n",
      "Open your mind. Just as I've always dreamed.\n",
      "\n",
      "Let the wind carry off your hopes, faraway.\n",
      "\n",
      "I have wings nobody can see. Look, you have them, too.\n",
      "\n",
      "They'll take us to where we ca\n"
     ]
    }
   ],
   "source": [
    "print(text[:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "WvM5h6_i3KsM"
   },
   "outputs": [],
   "source": [
    "# remove japanese characters\n",
    "text = ''.join(filter(lambda character:ord(character) < 0x3000, text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "U7SOcWJM0EqW",
    "outputId": "a82a4da8-a8ed-47bf-cfb5-2e4c59dee2cf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unique characters: 86 \n",
      " !'()*+,-./0123456789:;<=>?@ABCDEFGHIJKLMNOPQRSTUVWXYZ[]_abcdefghijklmnopqrstuvwxyz|\n"
     ]
    }
   ],
   "source": [
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "print(\"unique characters:\", vocab_size, ''.join(chars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yobmmaeK0EqX",
    "outputId": "26669f38-4b26-4b53-f642-ee7543e7c578"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'\\n': 0, ' ': 1, '!': 2, \"'\": 3, '(': 4, ')': 5, '*': 6, '+': 7, ',': 8, '-': 9, '.': 10, '/': 11, '0': 12, '1': 13, '2': 14, '3': 15, '4': 16, '5': 17, '6': 18, '7': 19, '8': 20, '9': 21, ':': 22, ';': 23, '<': 24, '=': 25, '>': 26, '?': 27, '@': 28, 'A': 29, 'B': 30, 'C': 31, 'D': 32, 'E': 33, 'F': 34, 'G': 35, 'H': 36, 'I': 37, 'J': 38, 'K': 39, 'L': 40, 'M': 41, 'N': 42, 'O': 43, 'P': 44, 'Q': 45, 'R': 46, 'S': 47, 'T': 48, 'U': 49, 'V': 50, 'W': 51, 'X': 52, 'Y': 53, 'Z': 54, '[': 55, ']': 56, '_': 57, 'a': 58, 'b': 59, 'c': 60, 'd': 61, 'e': 62, 'f': 63, 'g': 64, 'h': 65, 'i': 66, 'j': 67, 'k': 68, 'l': 69, 'm': 70, 'n': 71, 'o': 72, 'p': 73, 'q': 74, 'r': 75, 's': 76, 't': 77, 'u': 78, 'v': 79, 'w': 80, 'x': 81, 'y': 82, 'z': 83, '|': 84, '\\x94': 85, '': 86}\n",
      "{0: '\\n', 1: ' ', 2: '!', 3: \"'\", 4: '(', 5: ')', 6: '*', 7: '+', 8: ',', 9: '-', 10: '.', 11: '/', 12: '0', 13: '1', 14: '2', 15: '3', 16: '4', 17: '5', 18: '6', 19: '7', 20: '8', 21: '9', 22: ':', 23: ';', 24: '<', 25: '=', 26: '>', 27: '?', 28: '@', 29: 'A', 30: 'B', 31: 'C', 32: 'D', 33: 'E', 34: 'F', 35: 'G', 36: 'H', 37: 'I', 38: 'J', 39: 'K', 40: 'L', 41: 'M', 42: 'N', 43: 'O', 44: 'P', 45: 'Q', 46: 'R', 47: 'S', 48: 'T', 49: 'U', 50: 'V', 51: 'W', 52: 'X', 53: 'Y', 54: 'Z', 55: '[', 56: ']', 57: '_', 58: 'a', 59: 'b', 60: 'c', 61: 'd', 62: 'e', 63: 'f', 64: 'g', 65: 'h', 66: 'i', 67: 'j', 68: 'k', 69: 'l', 70: 'm', 71: 'n', 72: 'o', 73: 'p', 74: 'q', 75: 'r', 76: 's', 77: 't', 78: 'u', 79: 'v', 80: 'w', 81: 'x', 82: 'y', 83: 'z', 84: '|', 85: '\\x94', 86: ''}\n",
      "encoded: [43, 73, 62, 71, 1, 82, 72, 78, 75, 1, 70, 66, 71, 61, 10, 1, 43, 73, 62, 71]\n",
      "decoded: Open your mind. Open\n",
      "vocab size: 87\n"
     ]
    }
   ],
   "source": [
    "# yes, all language models will be character level, which isn't ideal but it's good for simplicity\n",
    "# very simple tokenizer\n",
    "stoi = {ch:i for i,ch in enumerate(chars)}\n",
    "itos = {i:ch for i,ch in enumerate(chars)}\n",
    "# add special token for padding\n",
    "stoi[''] = len(stoi)\n",
    "itos[len(itos)] = ''\n",
    "print(stoi)\n",
    "print(itos)\n",
    "encode = lambda s: [stoi[ch] for ch in s]\n",
    "decode = lambda l: ''.join([itos[i] for i in l])\n",
    "print(\"encoded:\", encode(text[:20]))\n",
    "print(\"decoded:\", decode(encode(text[:20])))\n",
    "vocab_size = len(itos)\n",
    "print(\"vocab size:\", vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6Pnf9KfP0EqY",
    "outputId": "ff581168-335a-4f0f-efeb-0d4bd5b225ca"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([39526018])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = torch.tensor(encode(text), dtype=torch.int64)\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lZ2fY1pR0EqY",
    "outputId": "5eb599e0-fb79-4cdb-c4b9-52a781d67151"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([43, 73, 62, 71,  1, 82, 72, 78, 75,  1, 70, 66, 71, 61, 10,  1, 43, 73, 62, 71,  1, 82, 72, 78, 75,  1, 70, 66, 71, 61, 10,  0,  0, 34, 58, 75,  1, 59, 62, 82, 72, 71, 61,  1, 77, 65, 62,  1, 61, 62, 62, 73,  1, 59, 69, 78, 62,  1, 33, 58, 75, 77, 65,  8,  1, 82, 72, 78,  1, 58, 71, 61,  1, 37,  1, 76, 65, 58, 69, 69,  1, 70, 62, 62, 77, 10, 10, 10,  0,  0, 29, 36,  2,  1, 41, 53,  1, 35, 43, 32])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oIaYesPh0Eqa",
    "outputId": "caa27cbb-5ae3-4406-8194-646264d4ba99"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([39130757]) torch.Size([395261])\n"
     ]
    }
   ],
   "source": [
    "n = int(len(data) * 0.99)\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]\n",
    "print(train_data.shape, val_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4bFhizcI0Eqa",
    "outputId": "c93a4d43-6fb2-4de7-aefc-8fed47e4a861"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([43, 73, 62, 71,  1, 82, 72, 78, 75])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq_len = 8\n",
    "train_data[:seq_len+1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "skFCPvQC0Eqc",
    "outputId": "08990c5b-88af-4a51-ce37-3c59989d6d74"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs:\n",
      "torch.Size([2, 64])\n",
      "tensor([[64,  1, 72, 78, 75,  1, 66, 71, 77, 62, 75, 71, 76, 65, 66, 73, 76, 10,  0,  0, 48, 65, 62,  1, 71, 66, 64, 65, 77,  1, 37, 66, 61, 58,  1, 64, 72, 77,  1, 77, 65, 62,  1, 75, 62, 76, 78, 69, 77, 76,  1, 72, 63,  1, 77, 65, 62,  1, 62, 81, 58, 70, 66, 71],\n",
      "        [62, 70,  2,  0,  0, 37, 77,  1, 69, 72, 72, 68, 76,  1, 69, 66, 68, 62,  1, 80, 62,  1, 77, 72, 72, 68,  1, 60, 58, 75, 62,  1, 72, 63,  1, 77, 65, 62, 70, 10,  0,  0, 48, 65, 72, 76, 62,  1, 64, 78, 82, 76,  1, 80, 62, 75, 62,  1, 66, 71, 60, 75, 62, 61]], device='cuda:0')\n",
      "targets:\n",
      "torch.Size([2, 64])\n",
      "tensor([[ 1, 72, 78, 75,  1, 66, 71, 77, 62, 75, 71, 76, 65, 66, 73, 76, 10,  0,  0, 48, 65, 62,  1, 71, 66, 64, 65, 77,  1, 37, 66, 61, 58,  1, 64, 72, 77,  1, 77, 65, 62,  1, 75, 62, 76, 78, 69, 77, 76,  1, 72, 63,  1, 77, 65, 62,  1, 62, 81, 58, 70, 66, 71, 58],\n",
      "        [70,  2,  0,  0, 37, 77,  1, 69, 72, 72, 68, 76,  1, 69, 66, 68, 62,  1, 80, 62,  1, 77, 72, 72, 68,  1, 60, 58, 75, 62,  1, 72, 63,  1, 77, 65, 62, 70, 10,  0,  0, 48, 65, 72, 76, 62,  1, 64, 78, 82, 76,  1, 80, 62, 75, 62,  1, 66, 71, 60, 75, 62, 61, 66]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "def get_batch(split, seq_len, batch_size=4):\n",
    "    # generate a small batch of data of inputs x and targets y\n",
    "    # targets are just inputs shifted by 1\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - seq_len, (batch_size,))\n",
    "    x = torch.stack([data[i:i+seq_len] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+seq_len+1] for i in ix])\n",
    "    return x.to(device), y.to(device)\n",
    "\n",
    "xb, yb = get_batch('train', 64, 2)\n",
    "print('inputs:')\n",
    "print(xb.shape)\n",
    "print(xb)\n",
    "print('targets:')\n",
    "print(yb.shape)\n",
    "print(yb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "327680000"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Make all steps, sequence lengths, and batch size the same\n",
    "total_steps = 5000\n",
    "seq_len = 256\n",
    "batch_size = 256 # these are small models so we can use large batch sizes to fully utilize the GPU\n",
    "# should cover around 2x the dataset\n",
    "total_steps * seq_len * batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test forward pass\n",
    "torch_compile_options = {\n",
    "    'epilogue_fusion': True, \n",
    "    'max_autotune': False, \n",
    "    'shape_padding': True, \n",
    "    'trace.enabled': False, \n",
    "    'triton.cudagraphs': False, \n",
    "    'debug': False, \n",
    "    'dce': True, \n",
    "    'memory_planning': True, \n",
    "    'coordinate_descent_tuning': False, \n",
    "    'trace.graph_diagram': False, \n",
    "    'compile_threads': 32, \n",
    "    'group_fusion': True, \n",
    "    'disable_progress': False, \n",
    "    'verbose_progress': False, \n",
    "    'triton.multi_kernel': 0, \n",
    "    'triton.use_block_ptr': False, \n",
    "    'triton.enable_persistent_tma_matmul': False, \n",
    "    'triton.autotune_at_compile_time': False, \n",
    "    'triton.cooperative_reductions': False, \n",
    "    'cuda.compile_opt_level': '-O2', \n",
    "    'cuda.enable_cuda_lto': True, \n",
    "    'combo_kernels': True, \n",
    "    'benchmark_combo_kernel': True, \n",
    "    'combo_kernel_foreach_dynamic_shapes': True\n",
    "}\n",
    "\n",
    "@torch.compile(fullgraph=False, options=torch_compile_options)\n",
    "def compile_optimizer_lr(opt, scheduler):\n",
    "    opt.step()\n",
    "    scheduler.step()\n",
    "\n",
    "\n",
    "def save_checkpoint(model, optimizer, scheduler, step, losses, val_losses, seq_len, batch_size, total_steps, save_dir='checkpoints'):\n",
    "    \"\"\"Save a complete checkpoint including model, optimizer, scheduler states and training metrics\"\"\"\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    \n",
    "    checkpoint = {\n",
    "        'step': step,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'scheduler_state_dict': scheduler.state_dict(),\n",
    "        'train_losses': losses,\n",
    "        'val_losses': val_losses,\n",
    "        'config': {\n",
    "            'seq_len': seq_len,\n",
    "            'batch_size': batch_size,\n",
    "            'total_steps': total_steps,\n",
    "            'vocab_size': model.token_embedding_table.num_embeddings,\n",
    "            'embed_size': model.blocks[0].sa_heads.head_size * model.head_num,\n",
    "            'head_num': model.head_num,\n",
    "            'layer_num': model.layer_num\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    checkpoint_path = os.path.join(save_dir, f'checkpoint_step_{step}.pt')\n",
    "    torch.save(checkpoint, checkpoint_path)\n",
    "    wandb.save(checkpoint_path)\n",
    "    print(f\"Checkpoint saved at step {step}: {checkpoint_path}\")\n",
    "    \n",
    "    return checkpoint_path\n",
    "\n",
    "def load_checkpoint(model, optimizer, scheduler, checkpoint_path):\n",
    "    \"\"\"Load a checkpoint and restore model, optimizer, scheduler states\"\"\"\n",
    "    checkpoint = torch.load(checkpoint_path, map_location=device)\n",
    "    \n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n",
    "    \n",
    "    print(f\"Checkpoint loaded from step {checkpoint['step']}\")\n",
    "    return checkpoint['step'], checkpoint['train_losses'], checkpoint['val_losses']\n",
    "\n",
    "def train(model, optimizer, scheduler, seq_len, batch_size, total_steps, val_steps=10, val_interval=50, checkpoint_interval=500, save_dir='checkpoints'):\n",
    "    losses = []\n",
    "    val_losses = []\n",
    "    # Create save directory\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    \n",
    "    # live plot\n",
    "    fig, ax = plt.subplots()\n",
    "    dh = display.display(fig, display_id=True)\n",
    "    \n",
    "    for step in (bar := tqdm(range(total_steps))):\n",
    "        # sample a batch of data\n",
    "        xb, yb = get_batch('train', seq_len=seq_len, batch_size=batch_size)\n",
    "\n",
    "        # evaluate the loss\n",
    "        logits, loss = model(xb, yb)\n",
    "        # backprop\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "        with torch._dynamo.compiled_autograd._enable(torch.compile()):\n",
    "            loss.backward()\n",
    "        compile_optimizer_lr(optimizer, scheduler)\n",
    "\n",
    "        bar.set_description(f\"loss: {loss.item():.2f}, val loss: {val_losses[-1] if val_losses else 0:.2f}, lr: {scheduler.get_last_lr()[0]:.2e}\")\n",
    "        losses.append(loss.item())\n",
    "        \n",
    "        # Log to wandb\n",
    "        if use_wandb:\n",
    "            wandb.log({\n",
    "                'train_loss': loss.item(),\n",
    "                'learning_rate': scheduler.get_last_lr()[0],\n",
    "                'step': step\n",
    "            })\n",
    "        \n",
    "        if step % val_interval == 0:\n",
    "            # Calculate validation loss\n",
    "            with torch.no_grad():\n",
    "                val_loss = 0\n",
    "                for _ in range(val_steps):\n",
    "                    xb, yb = get_batch('val', seq_len=seq_len, batch_size=batch_size)\n",
    "                    _, loss = model(xb, yb)\n",
    "                    val_loss += loss.item()\n",
    "                val_loss /= val_steps\n",
    "                val_losses.append(val_loss)\n",
    "                \n",
    "            # Log validation loss to wandb\n",
    "            if use_wandb:\n",
    "                wandb.log({\n",
    "                    'val_loss': val_loss,\n",
    "                    'step': step\n",
    "                })\n",
    "            \n",
    "            ax.clear()\n",
    "            ax.plot(losses, color='blue', label='train loss', alpha=0.7)\n",
    "            ax.plot(range(0, len(losses), val_interval), val_losses, color='red', label='val loss', alpha=0.7)\n",
    "            ax.set_ylim(1, 4)\n",
    "            ax.legend()\n",
    "            dh.update(fig)\n",
    "            \n",
    "        # Save checkpoint\n",
    "        if step % checkpoint_interval == 0 and step > 0:\n",
    "            save_checkpoint(model, optimizer, scheduler, step, losses, val_losses, seq_len, batch_size, total_steps, save_dir)\n",
    "            \n",
    "    print('final loss:', loss.item(), 'final val loss:', val_loss)\n",
    "    \n",
    "    # Save final checkpoint\n",
    "    save_checkpoint(model, optimizer, scheduler, total_steps, losses, val_losses, seq_len, batch_size, total_steps, save_dir)\n",
    "    \n",
    "    return losses, val_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Measure post training perplexity on validation set\n",
    "# Create function that receives a model, context length, and PPL sequence length, and returns the perplexity\n",
    "# The PPL sequence length is the number of characters the function uses to calculate the perplexity\n",
    "# We take the logits and calculate the cross entropy loss from scratch, then exponentiate it to get the perplexity\n",
    "# not only that, but we want the models to do this in actual inference\n",
    "def perplexity(model, seq_len, ppl_seq_len, batch_size=128, val_steps=1000):\n",
    "    with torch.no_grad():\n",
    "        val_loss = 0\n",
    "        for _ in tqdm(range(val_steps)):\n",
    "            xb, yb = get_batch('val', seq_len=seq_len, batch_size=batch_size)\n",
    "            with torch.autocast(device_type=\"cuda\", dtype=torch.bfloat16):\n",
    "                logits, _ = model(xb, yb)\n",
    "            logits = logits.reshape(batch_size, seq_len, vocab_size)\n",
    "            logits = logits[:, :ppl_seq_len]\n",
    "            yb = yb[:, :ppl_seq_len]\n",
    "            # flatten logits and targets\n",
    "            logits = logits.reshape(batch_size*ppl_seq_len, vocab_size)\n",
    "            yb = yb.reshape(batch_size*ppl_seq_len)\n",
    "            # calculate cross entropy loss from scratch\n",
    "            loss = F.cross_entropy(logits, yb)\n",
    "            val_loss += loss.item()\n",
    "        val_loss /= val_steps\n",
    "        ppl = torch.exp(torch.tensor(val_loss))\n",
    "        return ppl.item(), val_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classic Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "k6cA2WbrayyL"
   },
   "outputs": [],
   "source": [
    "class TransformerConfig:\n",
    "    def __init__(self, vocab_size, seq_len, embed_size, head_num, layer_num):\n",
    "        self.vocab_size = vocab_size\n",
    "        self.seq_len = seq_len\n",
    "        self.embed_size = embed_size\n",
    "        self.head_num = head_num\n",
    "        self.layer_num = layer_num\n",
    "\n",
    "def relu_softpick_2(x, dim=-1, eps=1e-8):\n",
    "    x_m = torch.max(x, dim=dim, keepdim=True).values\n",
    "    x_e = torch.exp(x - x_m)\n",
    "    r_x_e = F.relu(x_e)\n",
    "    return x_e / (torch.sum(r_x_e, dim=dim, keepdim=True) + eps) \n",
    "\n",
    "def precompute_freqs_cis(dim: int, end: int, theta: float = 10000.0):\n",
    "    freqs = 1.0 / (theta ** (torch.arange(0, dim, 2)[: (dim // 2)].float() / dim))\n",
    "    t = torch.arange(end, device=freqs.device, dtype=torch.float32)\n",
    "    freqs = torch.outer(t, freqs)\n",
    "    freqs_cis = torch.polar(torch.ones_like(freqs), freqs)  # complex64\n",
    "    return freqs_cis.to(device)\n",
    "\n",
    "def apply_rotary_emb(\n",
    "    xq: torch.Tensor,\n",
    "    xk: torch.Tensor,\n",
    "    freqs_cis: torch.Tensor,\n",
    "):\n",
    "    xq_ = torch.view_as_complex(xq.float().reshape(*xq.shape[:-1], -1, 2))\n",
    "    xk_ = torch.view_as_complex(xk.float().reshape(*xk.shape[:-1], -1, 2))\n",
    "    # freqs_cis = reshape_for_broadcast(freqs_cis, xq_)\n",
    "    q_shape = [d if i == xq_.ndim - 2 or i == xq_.ndim - 1 else 1 for i, d in enumerate(xq_.shape)]\n",
    "    k_shape = [d if i == xq_.ndim - 2 or i == xk_.ndim - 1 else 1 for i, d in enumerate(xk_.shape)]\n",
    "    T_q = xq_.shape[-2] \n",
    "    q_freqs_cis = freqs_cis[-T_q:].view(*q_shape)\n",
    "    k_freqs_cis = freqs_cis.view(*k_shape)\n",
    "    xq_out = torch.view_as_real(xq_ * q_freqs_cis).flatten(xq.dim() - 1)\n",
    "    xk_out = torch.view_as_real(xk_ * k_freqs_cis).flatten(xq.dim() - 1)\n",
    "    return xq_out.type_as(xq), xk_out.type_as(xk)\n",
    "\n",
    "class RMSNorm(torch.nn.Module):\n",
    "    def __init__(self, dim, eps=1e-6):\n",
    "        super().__init__()\n",
    "        self.eps = eps\n",
    "        self.weight = nn.Parameter(torch.ones(dim))\n",
    "\n",
    "    def _norm(self, x):\n",
    "        return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)\n",
    "\n",
    "    def forward(self, x):\n",
    "        output = self._norm(x.float()).type_as(x)\n",
    "        return output * self.weight\n",
    "    \n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.lin_1 = nn.Linear(config.embed_size, config.embed_size*4)\n",
    "        self.lin_2 = nn.Linear(config.embed_size*4, config.embed_size)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.lin_1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.lin_2(x)\n",
    "        return x\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\" multiple heads of self-attention with AliBi in parallel \"\"\"\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.seq_len = config.seq_len\n",
    "        self.head_num = config.head_num\n",
    "        self.head_size = config.embed_size // config.head_num\n",
    "        self.key = nn.Linear(config.embed_size, config.embed_size, bias=False)\n",
    "        self.query = nn.Linear(config.embed_size, config.embed_size, bias=False)\n",
    "        self.value = nn.Linear(config.embed_size, config.embed_size, bias=False)\n",
    "        self.o = nn.Linear(config.embed_size, config.embed_size)\n",
    "        # block_mask for FlexAttention\n",
    "        def causal(b, h, q_idx, kv_idx):\n",
    "            causal_mask = q_idx >= kv_idx\n",
    "            return causal_mask\n",
    "        self.causal_mask = create_block_mask(causal, B=None, H=None, Q_LEN=config.seq_len, KV_LEN=config.seq_len)\n",
    "        self.freqs_cis = precompute_freqs_cis(config.embed_size//config.head_num, config.seq_len)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(config.seq_len, config.seq_len)))\n",
    "\n",
    "    def forward(self, x, kv_cache=None):\n",
    "        B, T, C = x.shape\n",
    "        _, _, T_past, _ = kv_cache[0].shape if kv_cache is not None and kv_cache[0] is not None else (0, 0, 0, 0)\n",
    "        q = self.query(x) # (B,T,C)\n",
    "        k = self.key(x)   # (B,T,C)\n",
    "        v = self.value(x) # (B,T,C)\n",
    "\n",
    "        # Split into heads\n",
    "        q = q.view(B, T, self.head_num, self.head_size).transpose(1, 2) # (B, H, T, C/H)\n",
    "        k = k.view(B, T, self.head_num, self.head_size).transpose(1, 2) # (B, H, T, C/H)\n",
    "        v = v.view(B, T, self.head_num, self.head_size).transpose(1, 2) # (B, H, T, C/H)\n",
    "\n",
    "        if kv_cache is not None:\n",
    "            k_past, v_past = kv_cache\n",
    "            if k_past is not None:\n",
    "                k = torch.cat((k_past, k), dim=2)\n",
    "                v = torch.cat((v_past, v), dim=2)\n",
    "            if k.shape[-2] > self.seq_len:\n",
    "                k = k[:, :, -self.seq_len:]\n",
    "                v = v[:, :, -self.seq_len:]\n",
    "            kv_cache = (k, v)\n",
    "        T_k = k.shape[-2]\n",
    "        q, k = apply_rotary_emb(q, k, self.freqs_cis[:T_k])\n",
    "\n",
    "        wei = q @ k.transpose(-2,-1) # (B, H, 1, C/H) @ (B, H, C/H, T) -> (B, H, 1, T)\n",
    "        wei = wei * self.head_size ** -0.5 # scaled attention\n",
    "        wei = wei.masked_fill(self.tril[T_k-T:T_k, T_k-T:T_k] == 0, float('-inf')) # (B, T, T)\n",
    "        wei = relu_softpick_2(wei, dim=-1) # (B, H, T, T)\n",
    "        # apply attention to values\n",
    "        out = wei @ v # (B, H, 1, T) @ (B, H, T, C/H) -> (B, H, 1, C/H)\n",
    "\n",
    "        out = out.transpose(1, 2).contiguous().view(B, T, C) # (B, H, T, C/H) -> (B, T, H, C/H) -> (B, T, C)\n",
    "        out = self.o(out)\n",
    "        return out, kv_cache\n",
    "\n",
    "class Block(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.sa_heads = MultiHeadAttention(config)\n",
    "        self.ff_layer = FeedForward(config)\n",
    "        self.sa_norm = RMSNorm(config.embed_size)\n",
    "        self.ff_norm = RMSNorm(config.embed_size)\n",
    "    \n",
    "    def forward(self, x, kv_cache=None):\n",
    "        a, kv_cache = self.sa_heads(self.sa_norm(x), kv_cache)\n",
    "        h = x + a\n",
    "        o = h + self.ff_layer(self.ff_norm(h))\n",
    "        return o, kv_cache\n",
    "    \n",
    "class TransformerLM(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.layer_num = config.layer_num\n",
    "        self.head_num = config.head_num\n",
    "        self.seq_len = config.seq_len\n",
    "        # embed raw tokens to a lower dimensional embedding with embed_size\n",
    "        self.token_embedding_table = nn.Embedding(config.vocab_size, config.embed_size)\n",
    "        # Language Modelling (?) Head is a standard linear layer to go from \n",
    "        # embeddings back to logits of vocab_size\n",
    "        self.lm_head = nn.Linear(config.embed_size, config.vocab_size)\n",
    "        # transformer blocks\n",
    "        self.blocks = nn.ModuleList([Block(config) for _ in range(config.layer_num)])\n",
    "\n",
    "    def forward(self, idx, targets=None, kv_cache=None):\n",
    "        B, T = idx.shape\n",
    "        _, _, T_past, _ = kv_cache[0][0].shape if kv_cache is not None and kv_cache[0][0] is not None else (0, 0, 0, 0)\n",
    "        # idx and targets are both (B,T) tensor of integers\n",
    "        tok_embd = self.token_embedding_table(idx) # (B,T,C)\n",
    "        x = tok_embd\n",
    "        # go through blocks\n",
    "        for i, block in enumerate(self.blocks):\n",
    "            x, cache = block(x, None if kv_cache is None else kv_cache[i])\n",
    "            if kv_cache is not None:\n",
    "                kv_cache[i] = cache\n",
    "        # get logits with linear layer\n",
    "        logits = self.lm_head(x) # (B,T,V)\n",
    "        \n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, V = logits.shape\n",
    "            logits = logits.view(B*T, V)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "    \n",
    "    def generate(self, idx, max_new_tokens, temperature=1, use_cache=True):\n",
    "        if use_cache:\n",
    "            # initialize key-value cache\n",
    "            kv_cache = [(None, None) for _ in range(self.layer_num)]\n",
    "            # idx is (B, T) array of indices in the current context\n",
    "            # crop idx to the last seq_len tokens\n",
    "            idx_context = idx[:, -self.seq_len:]\n",
    "            for _ in range(max_new_tokens):\n",
    "                # get the predictions\n",
    "                logits, loss = self(idx_context, kv_cache=kv_cache)\n",
    "                # focus only on the last time step\n",
    "                logits = logits[:, -1, :] # becomes (B, C)\n",
    "                # apply temperature\n",
    "                logits = logits / temperature if temperature > 0 else logits\n",
    "                # apply relu softpick to get probabilities\n",
    "                probs = relu_softpick_2(logits, dim=-1) # (B, C)\n",
    "                # sample from the distribution\n",
    "                idx_next = torch.multinomial(probs, num_samples=1) if temperature > 0 else torch.argmax(probs, dim=-1, keepdim=True) # (B, 1)\n",
    "                # append sampled index to the running sequence\n",
    "                idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
    "                # since we have kv cache, only need to pass new token\n",
    "                idx_context = idx_next\n",
    "            return idx\n",
    "        else:\n",
    "            # idx is (B, T) array of indices in the current context\n",
    "            for _ in range(max_new_tokens):\n",
    "                #crop idx to the last seq_len tokens\n",
    "                idx_context = idx[:, -self.seq_len:]\n",
    "                # get the predictions\n",
    "                logits, loss = self(idx_context)\n",
    "                # focus only on the last time step\n",
    "                logits = logits[:, -1, :] # becomes (B, C)\n",
    "                # apply temperature\n",
    "                logits = logits / temperature if temperature > 0 else logits\n",
    "                # apply softpick to get probabilities\n",
    "                probs = relu_softpick_2(logits, dim=-1) # (B, C)\n",
    "                # sample from the distribution\n",
    "                idx_next = torch.multinomial(probs, num_samples=1) if temperature > 0 else torch.argmax(probs, dim=-1, keepdim=True) # (B, 1)\n",
    "                # append sampled index to the running sequence\n",
    "                idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
    "            return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/coder/Python_project/transformers_playground/.venv/lib/python3.12/site-packages/torch/_inductor/lowering.py:1988: UserWarning: Torchinductor does not support code generation for complex operators. Performance may be worse than eager.\n",
      "  warnings.warn(\n",
      "Inductor Compilation: 100%|██████████| 14/14 [00:00<00:00, 1962.84it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "4775511"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "config = TransformerConfig(\n",
    "    vocab_size=vocab_size,\n",
    "    seq_len=seq_len,\n",
    "    embed_size=256,\n",
    "    head_num=4,\n",
    "    layer_num=6\n",
    ")\n",
    "m = TransformerLM(config)\n",
    "m = torch.compile(m, options=torch_compile_options, fullgraph=True)\n",
    "m.to(device)\n",
    "xb, yb = get_batch('train', 5, 1)\n",
    "logits, loss = m(xb, yb)\n",
    "total_params = sum(p.numel() for p in m.parameters() if p.requires_grad)\n",
    "total_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inductor Compilation: 100%|██████████| 12/12 [00:00<00:00, 1921.20it/s]\n",
      "W1019 09:01:15.669000 1917437 torch/_inductor/codegen/triton_combo_kernel.py:110] [0/1] ComboKernels: 1 large pointwise nodes are separated\n",
      "W1019 09:01:15.846000 1917437 torch/_inductor/codegen/triton_combo_kernel.py:110] [0/1] ComboKernels: 1 large pointwise nodes are separated\n",
      "W1019 09:01:15.963000 1917437 torch/_inductor/codegen/triton_combo_kernel.py:110] [0/1] ComboKernels: 1 large pointwise nodes are separated\n",
      "W1019 09:01:16.077000 1917437 torch/_inductor/codegen/triton_combo_kernel.py:110] [0/1] ComboKernels: 1 large pointwise nodes are separated\n",
      "W1019 09:01:16.190000 1917437 torch/_inductor/codegen/triton_combo_kernel.py:110] [0/1] ComboKernels: 1 large pointwise nodes are separated\n",
      "W1019 09:01:16.303000 1917437 torch/_inductor/codegen/triton_combo_kernel.py:110] [0/1] ComboKernels: 1 large pointwise nodes are separated\n",
      "Inductor Compilation: 100%|██████████| 24/24 [00:00<00:00, 309.14it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.22.2"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/coder/Python_project/transformers_playground/wandb/run-20251019_090119-8qzitty1</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/erlandpg/transformers-playground/runs/8qzitty1' target=\"_blank\">transformer-lm-animesubs-256seq-256embed-4head-6layer</a></strong> to <a href='https://wandb.ai/erlandpg/transformers-playground' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/erlandpg/transformers-playground' target=\"_blank\">https://wandb.ai/erlandpg/transformers-playground</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/erlandpg/transformers-playground/runs/8qzitty1' target=\"_blank\">https://wandb.ai/erlandpg/transformers-playground/runs/8qzitty1</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 400x300 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "47f63aece65b455f91d2a0ffa451d1bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1019 09:01:27.125000 1917437 torch/fx/experimental/symbolic_shapes.py:6833] [!0/1/0] _maybe_guard_rel() was called on non-relation expression Eq(s17, s88) | Eq(s88, 1)\n",
      "W1019 09:01:27.295000 1917437 torch/fx/experimental/symbolic_shapes.py:6833] [!0/1/0] _maybe_guard_rel() was called on non-relation expression Eq(s10, 1) | Eq(s17, s10)\n",
      "W1019 09:01:27.442000 1917437 torch/fx/experimental/symbolic_shapes.py:6833] [!0/1/0] _maybe_guard_rel() was called on non-relation expression Eq(s17, s39) | Eq(s39, 1)\n",
      "W1019 09:01:27.580000 1917437 torch/fx/experimental/symbolic_shapes.py:6833] [!0/1/0] _maybe_guard_rel() was called on non-relation expression Eq(s17, s82) | Eq(s82, 1)\n",
      "W1019 09:01:27.697000 1917437 torch/fx/experimental/symbolic_shapes.py:6833] [!0/1/0] _maybe_guard_rel() was called on non-relation expression Eq(s17, s28) | Eq(s28, 1)\n",
      "W1019 09:01:27.814000 1917437 torch/fx/experimental/symbolic_shapes.py:6833] [!0/1/0] _maybe_guard_rel() was called on non-relation expression Eq(s17, s3) | Eq(s3, 1)\n",
      "W1019 09:01:31.634000 1917437 torch/_logging/_internal.py:1199] [3/0] Profiler function <class 'torch.autograd.profiler.record_function'> will be ignored\n",
      "Inductor Compilation: 100%|██████████| 13/13 [00:00<00:00, 2072.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint saved at step 500: checkpoints/checkpoint_step_500.pt\n",
      "Checkpoint saved at step 1000: checkpoints/checkpoint_step_1000.pt\n",
      "Checkpoint saved at step 1500: checkpoints/checkpoint_step_1500.pt\n",
      "Checkpoint saved at step 2000: checkpoints/checkpoint_step_2000.pt\n",
      "Checkpoint saved at step 2500: checkpoints/checkpoint_step_2500.pt\n",
      "Checkpoint saved at step 3000: checkpoints/checkpoint_step_3000.pt\n",
      "Checkpoint saved at step 3500: checkpoints/checkpoint_step_3500.pt\n",
      "Checkpoint saved at step 4000: checkpoints/checkpoint_step_4000.pt\n",
      "Checkpoint saved at step 4500: checkpoints/checkpoint_step_4500.pt\n",
      "final loss: 1.032421350479126 final val loss: 1.1859864354133607\n",
      "Checkpoint saved at step 5000: checkpoints/checkpoint_step_5000.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m The nbformat package was not found. It is required to save notebook history.\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>learning_rate</td><td>██████▇▇▇▇▆▆▆▆▆▅▅▅▅▄▄▄▄▃▃▃▂▂▂▂▂▁▁▁▁▁▁▁▁▁</td></tr><tr><td>step</td><td>▁▁▁▁▁▂▂▂▂▃▃▃▃▃▃▄▄▄▅▅▅▆▆▆▆▆▆▆▇▇▇▇▇▇██████</td></tr><tr><td>train_loss</td><td>██▅▅▅▃▃▃▃▃▃▂▃▂▃▃▂▂▂▂▂▂▂▂▂▂▂▁▁▂▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>█▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>learning_rate</td><td>0.0</td></tr><tr><td>step</td><td>4999</td></tr><tr><td>train_loss</td><td>1.03242</td></tr><tr><td>val_loss</td><td>1.18599</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">transformer-lm-animesubs-256seq-256embed-4head-6layer</strong> at: <a href='https://wandb.ai/erlandpg/transformers-playground/runs/8qzitty1' target=\"_blank\">https://wandb.ai/erlandpg/transformers-playground/runs/8qzitty1</a><br> View project at: <a href='https://wandb.ai/erlandpg/transformers-playground' target=\"_blank\">https://wandb.ai/erlandpg/transformers-playground</a><br>Synced 5 W&B file(s), 0 media file(s), 2 artifact file(s) and 10 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20251019_090119-8qzitty1/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 400x300 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "def save_notebook_to_wandb(notebook_path, run_name=None):\n",
    "    \"\"\"Save notebook without outputs to wandb\"\"\"\n",
    "    with open(notebook_path, 'r') as f:\n",
    "        nb = json.load(f)\n",
    "\n",
    "    for cell in nb['cells']:\n",
    "        if 'outputs' in cell:\n",
    "            cell['outputs'] = []\n",
    "        if 'execution_count' in cell:\n",
    "            cell['execution_count'] = None\n",
    "\n",
    "    clean_path = notebook_path.replace('.ipynb', '_clean.ipynb')\n",
    "    with open(clean_path, 'w') as f:\n",
    "        json.dump(nb, f, indent=1)\n",
    "\n",
    "    artifact = wandb.Artifact('training-notebook', type='code')\n",
    "    artifact.add_file(clean_path)\n",
    "    wandb.log_artifact(artifact)\n",
    "\n",
    "    return clean_path\n",
    "\n",
    "def get_wandb_config(model, optimizer, scheduler, seq_len, batch_size, total_steps):\n",
    "    \"\"\"Automatically extract wandb config from model and training parameters\"\"\"\n",
    "    config = {\n",
    "        \"architecture\": \"Transformer\",\n",
    "        \"dataset\": \"anime-subs\",\n",
    "        \"seq_len\": seq_len,\n",
    "        \"batch_size\": batch_size,\n",
    "        \"total_steps\": total_steps,\n",
    "        \"optimizer\": optimizer.__class__.__name__,\n",
    "        \"scheduler\": scheduler.__class__.__name__,\n",
    "        \"initial_lr\": optimizer.param_groups[0]['lr'],\n",
    "    }\n",
    "    \n",
    "    # Add model configuration\n",
    "    config.update({\n",
    "        \"vocab_size\": model.token_embedding_table.num_embeddings,\n",
    "        \"embed_size\": model.blocks[0].sa_heads.head_size * model.head_num,\n",
    "        \"head_num\": model.head_num,\n",
    "        \"layer_num\": model.layer_num,\n",
    "        \"total_params\": sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    })\n",
    "    \n",
    "    # Add scheduler-specific config\n",
    "    if hasattr(scheduler, 'T_max'):\n",
    "        config['scheduler_T_max'] = scheduler.T_max\n",
    "    if hasattr(scheduler, 'eta_min'):\n",
    "        config['scheduler_eta_min'] = scheduler.eta_min\n",
    "    \n",
    "    return config\n",
    "\n",
    "# Initialize wandb with automated config\n",
    "model = TransformerLM(config)\n",
    "model = torch.compile(model, options=torch_compile_options, fullgraph=True)\n",
    "model.to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=2e-3, fused=True)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=total_steps, eta_min=1e-6)\n",
    "\n",
    "# warmup\n",
    "for _ in range(10):\n",
    "    model(*get_batch('train', seq_len=seq_len, batch_size=batch_size))\n",
    "\n",
    "wandb_config = get_wandb_config(model, optimizer, scheduler, seq_len, batch_size, total_steps)\n",
    "wandb.init(\n",
    "    project=\"transformers-playground\",\n",
    "    config=wandb_config,\n",
    "    name=\"relu_softpick_2-lm-animesubs-256seq-256embed-4head-6layer.AMD\"\n",
    ")\n",
    "\n",
    "losses, val_losses = train(model, optimizer, scheduler, seq_len, batch_size, total_steps)\n",
    "\n",
    "if use_wandb:\n",
    "    save_notebook_to_wandb('./transformer_playground.ipynb')\n",
    "    wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save\n",
    "torch.save(model.state_dict(), 'models/TransformerLM.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OptimizedModule(\n",
       "  (_orig_mod): TransformerLM(\n",
       "    (token_embedding_table): Embedding(87, 256)\n",
       "    (lm_head): Linear(in_features=256, out_features=87, bias=True)\n",
       "    (blocks): ModuleList(\n",
       "      (0-5): 6 x Block(\n",
       "        (sa_heads): MultiHeadAttention(\n",
       "          (key): Linear(in_features=256, out_features=256, bias=False)\n",
       "          (query): Linear(in_features=256, out_features=256, bias=False)\n",
       "          (value): Linear(in_features=256, out_features=256, bias=False)\n",
       "          (o): Linear(in_features=256, out_features=256, bias=True)\n",
       "        )\n",
       "        (ff_layer): FeedForward(\n",
       "          (lin_1): Linear(in_features=256, out_features=1024, bias=True)\n",
       "          (lin_2): Linear(in_features=1024, out_features=256, bias=True)\n",
       "          (relu): ReLU()\n",
       "        )\n",
       "        (sa_norm): RMSNorm()\n",
       "        (ff_norm): RMSNorm()\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load model\n",
    "model = TransformerLM(config)\n",
    "model = torch.compile(model, options=torch_compile_options, fullgraph=True)\n",
    "model.load_state_dict(torch.load('models/TransformerLM.pt'))\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b2696da7fac426783fac8eb8a8bd6fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inductor Compilation: 100%|██████████| 21/21 [00:00<00:00, 27.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "perplexity: 3.2762105464935303 loss: 1.1866875\n"
     ]
    }
   ],
   "source": [
    "# calculate perplexity\n",
    "ppl, loss = perplexity(model, seq_len, seq_len)\n",
    "print(\"perplexity:\", ppl, \"loss:\", loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "00r0pbm3b5eX",
    "outputId": "bdd3b34e-32a0-4724-b528-c162c0fd0c3e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[53, 72, 78,  1, 80, 66, 69, 69,  1, 71, 62, 79, 62, 75]])\n",
      "You will never do this myself.\n",
      "\n",
      "Makes sense, the world is a little while ago.\n",
      "\n",
      "I was surprised that every day was at peace,\n",
      "\n",
      "but I still have the same way to my girlfriend.\n",
      "\n",
      "The one I want to see you again today.\n",
      "\n",
      "I was worried about the sports match,\n",
      "\n",
      "but I was able to talk to you about your clothes.\n",
      "\n",
      "I was thinking I want to talk to you about it too.\n",
      "\n",
      "That way I was the one who told me to help you.\n",
      "\n",
      "I was so surprised as a trainer won't be able to realize the enemy for the cost of the stars.\n",
      "\n",
      "I was able to help you know how to handle the cartency to the contract.\n",
      "\n",
      "I won't let you off the rest of the train so you can call me a movie.\n",
      "\n",
      "But we didn't think that we could come out to the end.\n",
      "\n",
      "I wanted to tell you that.\n",
      "\n",
      "And you're so cool.\n",
      "\n",
      "You want to know what the world is all the best, right?\n",
      "\n",
      "It's the same as my concern.\n",
      "\n",
      "I think that if I can get them out of the second chapter that I was a bit really mature.\n",
      "\n",
      "The second year of the world is dead.\n",
      "\n",
      "It was a great successful for a man who can get a\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "idx = encode(\"You will never\")\n",
    "print(torch.tensor([idx]))\n",
    "print(decode(model.generate(idx=torch.tensor([idx], dtype=torch.long).to(device), max_new_tokens=1000, temperature=0.5, use_cache=True)[0].tolist()))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "flame",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
