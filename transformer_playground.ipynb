{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "sdtDsu1Y0EqL"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import math\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "from torch.nn.attention.flex_attention import flex_attention, create_block_mask\n",
    "import triton\n",
    "import triton.language as tl\n",
    "from tqdm.notebook import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython import display\n",
    "import wandb\n",
    "torch.manual_seed(69)\n",
    "torch.set_printoptions(profile=\"short\", sci_mode=False, linewidth=100000)\n",
    "torch.set_float32_matmul_precision('high')\n",
    "# this script is configured to run on a RTX 3060 12GB GPU. you'll want to adjust the model sizes and batch sizes for other devices\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# device = torch.device('cpu')\n",
    "plt.rcParams['figure.figsize'] = [8, 6]\n",
    "plt.rcParams['figure.dpi'] = 50\n",
    "plt.rcParams['axes.grid'] = True\n",
    "plt.rcParams['xtick.minor.visible'] = True\n",
    "plt.rcParams['ytick.minor.visible'] = True\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ANr5dn7W0EqR",
    "outputId": "a00cbe8d-e1c6-454b-b552-4ffd17ce17e5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of dataset in characters:  39526018\n"
     ]
    }
   ],
   "source": [
    "# we use this 40mb file of concatenated anime subtitles as our dataset\n",
    "# just the right size for toy experiments like this I think\n",
    "with open('animesubs.txt', 'r', encoding='latin') as f:\n",
    "    text = f.read()\n",
    "print(\"length of dataset in characters: \", len(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pANiObIZ0EqU",
    "outputId": "98f70469-1c89-4341-89e8-c142a14331b1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Open your mind. Open your mind.\n",
      "\n",
      "Far beyond the deep blue Earth, you and I shall meet...\n",
      "\n",
      "AH! MY GODDESS\n",
      "\n",
      "A snow-white feather comes fluttering down, swaying gently in the air.\n",
      "\n",
      "Without holding back, I want to envelope you, my one and only love.\n",
      "\n",
      "I know I have the power to protect the one I love, right here in my hands.\n",
      "\n",
      "Open your mind. Just as I've always dreamed.\n",
      "\n",
      "Let the wind carry off your hopes, faraway.\n",
      "\n",
      "I have wings nobody can see. Look, you have them, too.\n",
      "\n",
      "They'll take us to where we ca\n"
     ]
    }
   ],
   "source": [
    "print(text[:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "WvM5h6_i3KsM"
   },
   "outputs": [],
   "source": [
    "# remove japanese characters\n",
    "text = ''.join(filter(lambda character:ord(character) < 0x3000, text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "U7SOcWJM0EqW",
    "outputId": "a82a4da8-a8ed-47bf-cfb5-2e4c59dee2cf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unique characters: 86 \n",
      " !'()*+,-./0123456789:;<=>?@ABCDEFGHIJKLMNOPQRSTUVWXYZ[]_abcdefghijklmnopqrstuvwxyz|Â”\n"
     ]
    }
   ],
   "source": [
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "print(\"unique characters:\", vocab_size, ''.join(chars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yobmmaeK0EqX",
    "outputId": "26669f38-4b26-4b53-f642-ee7543e7c578"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'\\n': 0, ' ': 1, '!': 2, \"'\": 3, '(': 4, ')': 5, '*': 6, '+': 7, ',': 8, '-': 9, '.': 10, '/': 11, '0': 12, '1': 13, '2': 14, '3': 15, '4': 16, '5': 17, '6': 18, '7': 19, '8': 20, '9': 21, ':': 22, ';': 23, '<': 24, '=': 25, '>': 26, '?': 27, '@': 28, 'A': 29, 'B': 30, 'C': 31, 'D': 32, 'E': 33, 'F': 34, 'G': 35, 'H': 36, 'I': 37, 'J': 38, 'K': 39, 'L': 40, 'M': 41, 'N': 42, 'O': 43, 'P': 44, 'Q': 45, 'R': 46, 'S': 47, 'T': 48, 'U': 49, 'V': 50, 'W': 51, 'X': 52, 'Y': 53, 'Z': 54, '[': 55, ']': 56, '_': 57, 'a': 58, 'b': 59, 'c': 60, 'd': 61, 'e': 62, 'f': 63, 'g': 64, 'h': 65, 'i': 66, 'j': 67, 'k': 68, 'l': 69, 'm': 70, 'n': 71, 'o': 72, 'p': 73, 'q': 74, 'r': 75, 's': 76, 't': 77, 'u': 78, 'v': 79, 'w': 80, 'x': 81, 'y': 82, 'z': 83, '|': 84, '\\x94': 85, '': 86}\n",
      "{0: '\\n', 1: ' ', 2: '!', 3: \"'\", 4: '(', 5: ')', 6: '*', 7: '+', 8: ',', 9: '-', 10: '.', 11: '/', 12: '0', 13: '1', 14: '2', 15: '3', 16: '4', 17: '5', 18: '6', 19: '7', 20: '8', 21: '9', 22: ':', 23: ';', 24: '<', 25: '=', 26: '>', 27: '?', 28: '@', 29: 'A', 30: 'B', 31: 'C', 32: 'D', 33: 'E', 34: 'F', 35: 'G', 36: 'H', 37: 'I', 38: 'J', 39: 'K', 40: 'L', 41: 'M', 42: 'N', 43: 'O', 44: 'P', 45: 'Q', 46: 'R', 47: 'S', 48: 'T', 49: 'U', 50: 'V', 51: 'W', 52: 'X', 53: 'Y', 54: 'Z', 55: '[', 56: ']', 57: '_', 58: 'a', 59: 'b', 60: 'c', 61: 'd', 62: 'e', 63: 'f', 64: 'g', 65: 'h', 66: 'i', 67: 'j', 68: 'k', 69: 'l', 70: 'm', 71: 'n', 72: 'o', 73: 'p', 74: 'q', 75: 'r', 76: 's', 77: 't', 78: 'u', 79: 'v', 80: 'w', 81: 'x', 82: 'y', 83: 'z', 84: '|', 85: '\\x94', 86: ''}\n",
      "encoded: [43, 73, 62, 71, 1, 82, 72, 78, 75, 1, 70, 66, 71, 61, 10, 1, 43, 73, 62, 71]\n",
      "decoded: Open your mind. Open\n",
      "vocab size: 87\n"
     ]
    }
   ],
   "source": [
    "# yes, all language models will be character level, which isn't ideal but it's good for simplicity\n",
    "# very simple tokenizer\n",
    "stoi = {ch:i for i,ch in enumerate(chars)}\n",
    "itos = {i:ch for i,ch in enumerate(chars)}\n",
    "# add special token for padding\n",
    "stoi[''] = len(stoi)\n",
    "itos[len(itos)] = ''\n",
    "print(stoi)\n",
    "print(itos)\n",
    "encode = lambda s: [stoi[ch] for ch in s]\n",
    "decode = lambda l: ''.join([itos[i] for i in l])\n",
    "print(\"encoded:\", encode(text[:20]))\n",
    "print(\"decoded:\", decode(encode(text[:20])))\n",
    "vocab_size = len(itos)\n",
    "print(\"vocab size:\", vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6Pnf9KfP0EqY",
    "outputId": "ff581168-335a-4f0f-efeb-0d4bd5b225ca"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([39526018])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = torch.tensor(encode(text), dtype=torch.int64)\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lZ2fY1pR0EqY",
    "outputId": "5eb599e0-fb79-4cdb-c4b9-52a781d67151"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([43, 73, 62, 71,  1, 82, 72, 78, 75,  1, 70, 66, 71, 61, 10,  1, 43, 73, 62, 71,  1, 82, 72, 78, 75,  1, 70, 66, 71, 61, 10,  0,  0, 34, 58, 75,  1, 59, 62, 82, 72, 71, 61,  1, 77, 65, 62,  1, 61, 62, 62, 73,  1, 59, 69, 78, 62,  1, 33, 58, 75, 77, 65,  8,  1, 82, 72, 78,  1, 58, 71, 61,  1, 37,  1, 76, 65, 58, 69, 69,  1, 70, 62, 62, 77, 10, 10, 10,  0,  0, 29, 36,  2,  1, 41, 53,  1, 35, 43, 32])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oIaYesPh0Eqa",
    "outputId": "caa27cbb-5ae3-4406-8194-646264d4ba99"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([39130757]) torch.Size([395261])\n"
     ]
    }
   ],
   "source": [
    "n = int(len(data) * 0.99)\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]\n",
    "print(train_data.shape, val_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4bFhizcI0Eqa",
    "outputId": "c93a4d43-6fb2-4de7-aefc-8fed47e4a861"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([43, 73, 62, 71,  1, 82, 72, 78, 75])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq_len = 8\n",
    "train_data[:seq_len+1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "skFCPvQC0Eqc",
    "outputId": "08990c5b-88af-4a51-ce37-3c59989d6d74"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs:\n",
      "torch.Size([2, 64])\n",
      "tensor([[64,  1, 72, 78, 75,  1, 66, 71, 77, 62, 75, 71, 76, 65, 66, 73, 76, 10,  0,  0, 48, 65, 62,  1, 71, 66, 64, 65, 77,  1, 37, 66, 61, 58,  1, 64, 72, 77,  1, 77, 65, 62,  1, 75, 62, 76, 78, 69, 77, 76,  1, 72, 63,  1, 77, 65, 62,  1, 62, 81, 58, 70, 66, 71],\n",
      "        [62, 70,  2,  0,  0, 37, 77,  1, 69, 72, 72, 68, 76,  1, 69, 66, 68, 62,  1, 80, 62,  1, 77, 72, 72, 68,  1, 60, 58, 75, 62,  1, 72, 63,  1, 77, 65, 62, 70, 10,  0,  0, 48, 65, 72, 76, 62,  1, 64, 78, 82, 76,  1, 80, 62, 75, 62,  1, 66, 71, 60, 75, 62, 61]], device='cuda:0')\n",
      "targets:\n",
      "torch.Size([2, 64])\n",
      "tensor([[ 1, 72, 78, 75,  1, 66, 71, 77, 62, 75, 71, 76, 65, 66, 73, 76, 10,  0,  0, 48, 65, 62,  1, 71, 66, 64, 65, 77,  1, 37, 66, 61, 58,  1, 64, 72, 77,  1, 77, 65, 62,  1, 75, 62, 76, 78, 69, 77, 76,  1, 72, 63,  1, 77, 65, 62,  1, 62, 81, 58, 70, 66, 71, 58],\n",
      "        [70,  2,  0,  0, 37, 77,  1, 69, 72, 72, 68, 76,  1, 69, 66, 68, 62,  1, 80, 62,  1, 77, 72, 72, 68,  1, 60, 58, 75, 62,  1, 72, 63,  1, 77, 65, 62, 70, 10,  0,  0, 48, 65, 72, 76, 62,  1, 64, 78, 82, 76,  1, 80, 62, 75, 62,  1, 66, 71, 60, 75, 62, 61, 66]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "def get_batch(split, seq_len, batch_size=4):\n",
    "    # generate a small batch of data of inputs x and targets y\n",
    "    # targets are just inputs shifted by 1\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - seq_len, (batch_size,))\n",
    "    x = torch.stack([data[i:i+seq_len] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+seq_len+1] for i in ix])\n",
    "    return x.to(device), y.to(device)\n",
    "\n",
    "xb, yb = get_batch('train', 64, 2)\n",
    "print('inputs:')\n",
    "print(xb.shape)\n",
    "print(xb)\n",
    "print('targets:')\n",
    "print(yb.shape)\n",
    "print(yb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "327680000"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Make all steps, sequence lengths, and batch size the same\n",
    "total_steps = 5000\n",
    "seq_len = 256\n",
    "batch_size = 256 # these are small models so we can use large batch sizes to fully utilize the GPU\n",
    "# should cover around 2x the dataset\n",
    "total_steps * seq_len * batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_checkpoint(model, optimizer, scheduler, step, losses, val_losses, seq_len, batch_size, total_steps, save_dir='checkpoints'):\n",
    "    \"\"\"Save a complete checkpoint including model, optimizer, scheduler states and training metrics\"\"\"\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    \n",
    "    checkpoint = {\n",
    "        'step': step,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'scheduler_state_dict': scheduler.state_dict(),\n",
    "        'train_losses': losses,\n",
    "        'val_losses': val_losses,\n",
    "        'config': {\n",
    "            'seq_len': seq_len,\n",
    "            'batch_size': batch_size,\n",
    "            'total_steps': total_steps,\n",
    "            'vocab_size': model.token_embedding_table.num_embeddings,\n",
    "            'embed_size': model.blocks[0].sa_heads.head_size * model.head_num,\n",
    "            'head_num': model.head_num,\n",
    "            'layer_num': model.layer_num\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    checkpoint_path = os.path.join(save_dir, f'checkpoint_step_{step}.pt')\n",
    "    torch.save(checkpoint, checkpoint_path)\n",
    "    wandb.save(checkpoint_path)\n",
    "    print(f\"Checkpoint saved at step {step}: {checkpoint_path}\")\n",
    "    \n",
    "    return checkpoint_path\n",
    "\n",
    "def load_checkpoint(model, optimizer, scheduler, checkpoint_path):\n",
    "    \"\"\"Load a checkpoint and restore model, optimizer, scheduler states\"\"\"\n",
    "    checkpoint = torch.load(checkpoint_path, map_location=device)\n",
    "    \n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n",
    "    \n",
    "    print(f\"Checkpoint loaded from step {checkpoint['step']}\")\n",
    "    return checkpoint['step'], checkpoint['train_losses'], checkpoint['val_losses']\n",
    "\n",
    "def train(model, optimizer, scheduler, seq_len, batch_size, total_steps, val_steps=10, val_interval=50, checkpoint_interval=500, save_dir='checkpoints'):\n",
    "    losses = []\n",
    "    val_losses = []\n",
    "    # Create save directory\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    \n",
    "    # live plot\n",
    "    fig, ax = plt.subplots()\n",
    "    dh = display.display(fig, display_id=True)\n",
    "    \n",
    "    for step in (bar := tqdm(range(total_steps))):\n",
    "        # sample a batch of data\n",
    "        xb, yb = get_batch('train', seq_len=seq_len, batch_size=batch_size)\n",
    "\n",
    "        # evaluate the loss\n",
    "        logits, loss = model(xb, yb)\n",
    "\n",
    "        # backprop\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "        bar.set_description(f\"loss: {loss.item():.2f}, val loss: {val_losses[-1] if val_losses else 0:.2f}, lr: {scheduler.get_last_lr()[0]:.2e}\")\n",
    "        losses.append(loss.item())\n",
    "        \n",
    "        # Log to wandb\n",
    "        wandb.log({\n",
    "            'train_loss': loss.item(),\n",
    "            'learning_rate': scheduler.get_last_lr()[0],\n",
    "            'step': step\n",
    "        })\n",
    "        \n",
    "        if step % val_interval == 0:\n",
    "            # Calculate validation loss\n",
    "            with torch.no_grad():\n",
    "                val_loss = 0\n",
    "                for _ in range(val_steps):\n",
    "                    xb, yb = get_batch('val', seq_len=seq_len, batch_size=batch_size)\n",
    "                    _, loss = model(xb, yb)\n",
    "                    val_loss += loss.item()\n",
    "                val_loss /= val_steps\n",
    "                val_losses.append(val_loss)\n",
    "                \n",
    "            # Log validation loss to wandb\n",
    "            wandb.log({\n",
    "                'val_loss': val_loss,\n",
    "                'step': step\n",
    "            })\n",
    "            \n",
    "            ax.clear()\n",
    "            ax.plot(losses, color='blue', label='train loss', alpha=0.7)\n",
    "            ax.plot(range(0, len(losses), val_interval), val_losses, color='red', label='val loss', alpha=0.7)\n",
    "            ax.set_ylim(1, 4)\n",
    "            ax.legend()\n",
    "            dh.update(fig)\n",
    "            \n",
    "        # Save checkpoint\n",
    "        if step % checkpoint_interval == 0 and step > 0:\n",
    "            save_checkpoint(model, optimizer, scheduler, step, losses, val_losses, seq_len, batch_size, total_steps, save_dir)\n",
    "            \n",
    "    print('final loss:', loss.item(), 'final val loss:', val_loss)\n",
    "    \n",
    "    # Save final checkpoint\n",
    "    save_checkpoint(model, optimizer, scheduler, total_steps, losses, val_losses, seq_len, batch_size, total_steps, save_dir)\n",
    "    \n",
    "    return losses, val_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Measure post training perplexity on validation set\n",
    "# Create function that receives a model, context length, and PPL sequence length, and returns the perplexity\n",
    "# The PPL sequence length is the number of characters the function uses to calculate the perplexity\n",
    "# We take the logits and calculate the cross entropy loss from scratch, then exponentiate it to get the perplexity\n",
    "# not only that, but we want the models to do this in actual inference\n",
    "def perplexity(model, seq_len, ppl_seq_len, batch_size=128, val_steps=1000):\n",
    "    with torch.no_grad():\n",
    "        val_loss = 0\n",
    "        for _ in tqdm(range(val_steps)):\n",
    "            xb, yb = get_batch('val', seq_len=seq_len, batch_size=batch_size)\n",
    "            with torch.autocast(device_type=\"cuda\", dtype=torch.bfloat16):\n",
    "                logits, _ = model(xb, yb)\n",
    "            logits = logits.reshape(batch_size, seq_len, vocab_size)\n",
    "            logits = logits[:, :ppl_seq_len]\n",
    "            yb = yb[:, :ppl_seq_len]\n",
    "            # flatten logits and targets\n",
    "            logits = logits.reshape(batch_size*ppl_seq_len, vocab_size)\n",
    "            yb = yb.reshape(batch_size*ppl_seq_len)\n",
    "            # calculate cross entropy loss from scratch\n",
    "            loss = F.cross_entropy(logits, yb)\n",
    "            val_loss += loss.item()\n",
    "        val_loss /= val_steps\n",
    "        ppl = torch.exp(torch.tensor(val_loss))\n",
    "        return ppl.item(), val_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classic Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "k6cA2WbrayyL"
   },
   "outputs": [],
   "source": [
    "class TransformerConfig:\n",
    "    def __init__(self, vocab_size, seq_len, embed_size, head_num, layer_num):\n",
    "        self.vocab_size = vocab_size\n",
    "        self.seq_len = seq_len\n",
    "        self.embed_size = embed_size\n",
    "        self.head_num = head_num\n",
    "        self.layer_num = layer_num\n",
    "\n",
    "def precompute_freqs_cis(dim: int, end: int, theta: float = 10000.0):\n",
    "    freqs = 1.0 / (theta ** (torch.arange(0, dim, 2)[: (dim // 2)].float() / dim))\n",
    "    t = torch.arange(end, device=freqs.device, dtype=torch.float32)\n",
    "    freqs = torch.outer(t, freqs)\n",
    "    freqs_cis = torch.polar(torch.ones_like(freqs), freqs)  # complex64\n",
    "    return freqs_cis.to(device)\n",
    "\n",
    "def apply_rotary_emb(\n",
    "    xq: torch.Tensor,\n",
    "    xk: torch.Tensor,\n",
    "    freqs_cis: torch.Tensor,\n",
    "):\n",
    "    xq_ = torch.view_as_complex(xq.float().reshape(*xq.shape[:-1], -1, 2))\n",
    "    xk_ = torch.view_as_complex(xk.float().reshape(*xk.shape[:-1], -1, 2))\n",
    "    # freqs_cis = reshape_for_broadcast(freqs_cis, xq_)\n",
    "    q_shape = [d if i == xq_.ndim - 2 or i == xq_.ndim - 1 else 1 for i, d in enumerate(xq_.shape)]\n",
    "    k_shape = [d if i == xq_.ndim - 2 or i == xk_.ndim - 1 else 1 for i, d in enumerate(xk_.shape)]\n",
    "    T_q = xq_.shape[-2] \n",
    "    q_freqs_cis = freqs_cis[-T_q:].view(*q_shape)\n",
    "    k_freqs_cis = freqs_cis.view(*k_shape)\n",
    "    xq_out = torch.view_as_real(xq_ * q_freqs_cis).flatten(xq.dim() - 1)\n",
    "    xk_out = torch.view_as_real(xk_ * k_freqs_cis).flatten(xq.dim() - 1)\n",
    "    return xq_out.type_as(xq), xk_out.type_as(xk)\n",
    "\n",
    "class RMSNorm(torch.nn.Module):\n",
    "    def __init__(self, dim, eps=1e-6):\n",
    "        super().__init__()\n",
    "        self.eps = eps\n",
    "        self.weight = nn.Parameter(torch.ones(dim))\n",
    "\n",
    "    def _norm(self, x):\n",
    "        return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)\n",
    "\n",
    "    def forward(self, x):\n",
    "        output = self._norm(x.float()).type_as(x)\n",
    "        return output * self.weight\n",
    "    \n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.lin_1 = nn.Linear(config.embed_size, config.embed_size*4)\n",
    "        self.lin_2 = nn.Linear(config.embed_size*4, config.embed_size)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.lin_1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.lin_2(x)\n",
    "        return x\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\" multiple heads of self-attention with AliBi in parallel \"\"\"\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.seq_len = config.seq_len\n",
    "        self.head_num = config.head_num\n",
    "        self.head_size = config.embed_size // config.head_num\n",
    "        self.key = nn.Linear(config.embed_size, config.embed_size, bias=False)\n",
    "        self.query = nn.Linear(config.embed_size, config.embed_size, bias=False)\n",
    "        self.value = nn.Linear(config.embed_size, config.embed_size, bias=False)\n",
    "        self.o = nn.Linear(config.embed_size, config.embed_size)\n",
    "        # block_mask for FlexAttention\n",
    "        def causal(b, h, q_idx, kv_idx):\n",
    "            causal_mask = q_idx >= kv_idx\n",
    "            return causal_mask\n",
    "        self.causal_mask = create_block_mask(causal, B=None, H=None, Q_LEN=config.seq_len, KV_LEN=config.seq_len)\n",
    "        self.freqs_cis = precompute_freqs_cis(config.embed_size//config.head_num, config.seq_len)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(config.seq_len, config.seq_len)))\n",
    "\n",
    "    def forward(self, x, kv_cache=None):\n",
    "        B, T, C = x.shape\n",
    "        _, _, T_past, _ = kv_cache[0].shape if kv_cache is not None and kv_cache[0] is not None else (0, 0, 0, 0)\n",
    "        q = self.query(x) # (B,T,C)\n",
    "        k = self.key(x)   # (B,T,C)\n",
    "        v = self.value(x) # (B,T,C)\n",
    "\n",
    "        # Split into heads\n",
    "        q = q.view(B, T, self.head_num, self.head_size).transpose(1, 2) # (B, H, T, C/H)\n",
    "        k = k.view(B, T, self.head_num, self.head_size).transpose(1, 2) # (B, H, T, C/H)\n",
    "        v = v.view(B, T, self.head_num, self.head_size).transpose(1, 2) # (B, H, T, C/H)\n",
    "\n",
    "        if kv_cache is not None:\n",
    "            k_past, v_past = kv_cache\n",
    "            if k_past is not None:\n",
    "                k = torch.cat((k_past, k), dim=2)\n",
    "                v = torch.cat((v_past, v), dim=2)\n",
    "            if k.shape[-2] > self.seq_len:\n",
    "                k = k[:, :, -self.seq_len:]\n",
    "                v = v[:, :, -self.seq_len:]\n",
    "            kv_cache = (k, v)\n",
    "        T_k = k.shape[-2]\n",
    "        q, k = apply_rotary_emb(q, k, self.freqs_cis[:T_k])\n",
    "\n",
    "        if T == self.seq_len:\n",
    "            out = flex_attention(q, k, v, block_mask=self.causal_mask)\n",
    "        else:\n",
    "            # compute attention scores (\"affinities\")\n",
    "            wei = q @ k.transpose(-2,-1) # (B, H, 1, C/H) @ (B, H, C/H, T) -> (B, H, 1, T)\n",
    "            wei = wei * self.head_size ** -0.5 # scaled attention\n",
    "            wei = wei.masked_fill(self.tril[T_k-T:T_k, T_k-T:T_k] == 0, float('-inf')) # (B, T, T)\n",
    "            wei = F.softmax(wei, dim=-1) # (B, H, T, T)\n",
    "            # apply attention to values\n",
    "            out = wei @ v # (B, H, 1, T) @ (B, H, T, C/H) -> (B, H, 1, C/H)\n",
    "\n",
    "        out = out.transpose(1, 2).contiguous().view(B, T, C) # (B, H, T, C/H) -> (B, T, H, C/H) -> (B, T, C)\n",
    "        out = self.o(out)\n",
    "        return out, kv_cache\n",
    "\n",
    "class Block(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.sa_heads = MultiHeadAttention(config)\n",
    "        self.ff_layer = FeedForward(config)\n",
    "        self.sa_norm = RMSNorm(config.embed_size)\n",
    "        self.ff_norm = RMSNorm(config.embed_size)\n",
    "    \n",
    "    def forward(self, x, kv_cache=None):\n",
    "        a, kv_cache = self.sa_heads(self.sa_norm(x), kv_cache)\n",
    "        h = x + a\n",
    "        o = h + self.ff_layer(self.ff_norm(h))\n",
    "        return o, kv_cache\n",
    "    \n",
    "class TransformerLM(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.layer_num = config.layer_num\n",
    "        self.head_num = config.head_num\n",
    "        self.seq_len = config.seq_len\n",
    "        # embed raw tokens to a lower dimensional embedding with embed_size\n",
    "        self.token_embedding_table = nn.Embedding(config.vocab_size, config.embed_size)\n",
    "        # Language Modelling (?) Head is a standard linear layer to go from \n",
    "        # embeddings back to logits of vocab_size\n",
    "        self.lm_head = nn.Linear(config.embed_size, config.vocab_size)\n",
    "        # transformer blocks\n",
    "        self.blocks = nn.ModuleList([Block(config) for _ in range(config.layer_num)])\n",
    "\n",
    "    def forward(self, idx, targets=None, kv_cache=None):\n",
    "        B, T = idx.shape\n",
    "        _, _, T_past, _ = kv_cache[0][0].shape if kv_cache is not None and kv_cache[0][0] is not None else (0, 0, 0, 0)\n",
    "        # idx and targets are both (B,T) tensor of integers\n",
    "        tok_embd = self.token_embedding_table(idx) # (B,T,C)\n",
    "        x = tok_embd\n",
    "        # go through blocks\n",
    "        for i, block in enumerate(self.blocks):\n",
    "            x, cache = block(x, None if kv_cache is None else kv_cache[i])\n",
    "            if kv_cache is not None:\n",
    "                kv_cache[i] = cache\n",
    "        # get logits with linear layer\n",
    "        logits = self.lm_head(x) # (B,T,V)\n",
    "        \n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, V = logits.shape\n",
    "            logits = logits.view(B*T, V)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "    \n",
    "    def generate(self, idx, max_new_tokens, temperature=1, use_cache=True):\n",
    "        if use_cache:\n",
    "            # initialize key-value cache\n",
    "            kv_cache = [(None, None) for _ in range(self.layer_num)]\n",
    "            # idx is (B, T) array of indices in the current context\n",
    "            # crop idx to the last seq_len tokens\n",
    "            idx_context = idx[:, -self.seq_len:]\n",
    "            for _ in range(max_new_tokens):\n",
    "                # get the predictions\n",
    "                logits, loss = self(idx_context, kv_cache=kv_cache)\n",
    "                # focus only on the last time step\n",
    "                logits = logits[:, -1, :] # becomes (B, C)\n",
    "                # apply temperature\n",
    "                logits = logits / temperature if temperature > 0 else logits\n",
    "                # apply softmax to get probabilities\n",
    "                probs = F.softmax(logits, dim=-1) # (B, C)\n",
    "                # sample from the distribution\n",
    "                idx_next = torch.multinomial(probs, num_samples=1) if temperature > 0 else torch.argmax(probs, dim=-1, keepdim=True) # (B, 1)\n",
    "                # append sampled index to the running sequence\n",
    "                idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
    "                # since we have kv cache, only need to pass new token\n",
    "                idx_context = idx_next\n",
    "            return idx\n",
    "        else:\n",
    "            # idx is (B, T) array of indices in the current context\n",
    "            for _ in range(max_new_tokens):\n",
    "                #crop idx to the last seq_len tokens\n",
    "                idx_context = idx[:, -self.seq_len:]\n",
    "                # get the predictions\n",
    "                logits, loss = self(idx_context)\n",
    "                # focus only on the last time step\n",
    "                logits = logits[:, -1, :] # becomes (B, C)\n",
    "                # apply temperature\n",
    "                logits = logits / temperature if temperature > 0 else logits\n",
    "                # apply softmax to get probabilities\n",
    "                probs = F.softmax(logits, dim=-1) # (B, C)\n",
    "                # sample from the distribution\n",
    "                idx_next = torch.multinomial(probs, num_samples=1) if temperature > 0 else torch.argmax(probs, dim=-1, keepdim=True) # (B, 1)\n",
    "                # append sampled index to the running sequence\n",
    "                idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
    "            return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4775511"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test forward pass\n",
    "config = TransformerConfig(\n",
    "    vocab_size=vocab_size,\n",
    "    seq_len=seq_len,\n",
    "    embed_size=256,\n",
    "    head_num=4,\n",
    "    layer_num=6\n",
    ")\n",
    "m = TransformerLM(config)\n",
    "m.to(device)\n",
    "xb, yb = get_batch('train', 5, 1)\n",
    "logits, loss = m(xb, yb)\n",
    "total_params = sum(p.numel() for p in m.parameters() if p.requires_grad)\n",
    "total_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33merlandpg\u001b[0m to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "creating run (0.1s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.21.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/coder/Python_project/transformers_playground/wandb/run-20251018_174849-16bvi74b</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/erlandpg/transformers-playground/runs/16bvi74b' target=\"_blank\">spec-decode-target</a></strong> to <a href='https://wandb.ai/erlandpg/transformers-playground' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/erlandpg/transformers-playground' target=\"_blank\">https://wandb.ai/erlandpg/transformers-playground</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/erlandpg/transformers-playground/runs/16bvi74b' target=\"_blank\">https://wandb.ai/erlandpg/transformers-playground/runs/16bvi74b</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 400x300 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "18f60acea3b146b68228c5c66fc54925",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/coder/Python_project/transformers_playground/.venv/lib/python3.12/site-packages/torch/_inductor/lowering.py:1917: UserWarning: Torchinductor does not support code generation for complex operators. Performance may be worse than eager.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint saved at step 500: checkpoints/checkpoint_step_500.pt\n",
      "Checkpoint saved at step 1000: checkpoints/checkpoint_step_1000.pt\n",
      "Checkpoint saved at step 1500: checkpoints/checkpoint_step_1500.pt\n",
      "Checkpoint saved at step 2000: checkpoints/checkpoint_step_2000.pt\n",
      "Checkpoint saved at step 2500: checkpoints/checkpoint_step_2500.pt\n",
      "Checkpoint saved at step 3000: checkpoints/checkpoint_step_3000.pt\n",
      "Checkpoint saved at step 3500: checkpoints/checkpoint_step_3500.pt\n",
      "Checkpoint saved at step 4000: checkpoints/checkpoint_step_4000.pt\n",
      "Checkpoint saved at step 4500: checkpoints/checkpoint_step_4500.pt\n",
      "final loss: 1.0442308187484741 final val loss: 1.182481586933136\n",
      "Checkpoint saved at step 5000: checkpoints/checkpoint_step_5000.pt\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>learning_rate</td><td>â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‡â–‡â–‡â–‡â–‡â–‡â–†â–†â–†â–†â–†â–…â–…â–…â–„â–„â–„â–„â–„â–„â–ƒâ–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–‚â–â–â–â–â–</td></tr><tr><td>step</td><td>â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ</td></tr><tr><td>train_loss</td><td>â–ˆâ–…â–„â–„â–„â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–‚â–‚â–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–</td></tr><tr><td>val_loss</td><td>â–ˆâ–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>learning_rate</td><td>0.0</td></tr><tr><td>step</td><td>4999</td></tr><tr><td>train_loss</td><td>1.04423</td></tr><tr><td>val_loss</td><td>1.18248</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">spec-decode-target</strong> at: <a href='https://wandb.ai/erlandpg/transformers-playground/runs/16bvi74b' target=\"_blank\">https://wandb.ai/erlandpg/transformers-playground/runs/16bvi74b</a><br> View project at: <a href='https://wandb.ai/erlandpg/transformers-playground' target=\"_blank\">https://wandb.ai/erlandpg/transformers-playground</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20251018_174849-16bvi74b/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 400x300 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def get_wandb_config(model, optimizer, scheduler, seq_len, batch_size, total_steps):\n",
    "    \"\"\"Automatically extract wandb config from model and training parameters\"\"\"\n",
    "    config = {\n",
    "        \"architecture\": \"Transformer\",\n",
    "        \"dataset\": \"anime-subs\",\n",
    "        \"seq_len\": seq_len,\n",
    "        \"batch_size\": batch_size,\n",
    "        \"total_steps\": total_steps,\n",
    "        \"optimizer\": optimizer.__class__.__name__,\n",
    "        \"scheduler\": scheduler.__class__.__name__,\n",
    "        \"initial_lr\": optimizer.param_groups[0]['lr'],\n",
    "    }\n",
    "    \n",
    "    # Add model configuration\n",
    "    config.update({\n",
    "        \"vocab_size\": model.token_embedding_table.num_embeddings,\n",
    "        \"embed_size\": model.blocks[0].sa_heads.head_size * model.head_num,\n",
    "        \"head_num\": model.head_num,\n",
    "        \"layer_num\": model.layer_num,\n",
    "        \"total_params\": sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    })\n",
    "    \n",
    "    # Add scheduler-specific config\n",
    "    if hasattr(scheduler, 'T_max'):\n",
    "        config['scheduler_T_max'] = scheduler.T_max\n",
    "    if hasattr(scheduler, 'eta_min'):\n",
    "        config['scheduler_eta_min'] = scheduler.eta_min\n",
    "    \n",
    "    return config\n",
    "\n",
    "# Initialize wandb with automated config\n",
    "model = TransformerLM(config)\n",
    "model = torch.compile(model)\n",
    "model.to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=2e-3)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=total_steps, eta_min=1e-6)\n",
    "\n",
    "wandb_config = get_wandb_config(model, optimizer, scheduler, seq_len, batch_size, total_steps)\n",
    "wandb.init(\n",
    "    project=\"transformers-playground\",\n",
    "    config=wandb_config,\n",
    "    name=\"spec-decode-target\"\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "losses, val_losses = train(model, optimizer, scheduler, seq_len, batch_size, total_steps)\n",
    "\n",
    "# Finish wandb run\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Parent directory models does not exist.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[18]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Save\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstate_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mmodels/TransformerLM.pt\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Python_project/transformers_playground/.venv/lib/python3.12/site-packages/torch/serialization.py:964\u001b[39m, in \u001b[36msave\u001b[39m\u001b[34m(obj, f, pickle_module, pickle_protocol, _use_new_zipfile_serialization, _disable_byteorder_record)\u001b[39m\n\u001b[32m    961\u001b[39m     f = os.fspath(f)\n\u001b[32m    963\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m _use_new_zipfile_serialization:\n\u001b[32m--> \u001b[39m\u001b[32m964\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43m_open_zipfile_writer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m opened_zipfile:\n\u001b[32m    965\u001b[39m         _save(\n\u001b[32m    966\u001b[39m             obj,\n\u001b[32m    967\u001b[39m             opened_zipfile,\n\u001b[32m   (...)\u001b[39m\u001b[32m    970\u001b[39m             _disable_byteorder_record,\n\u001b[32m    971\u001b[39m         )\n\u001b[32m    972\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Python_project/transformers_playground/.venv/lib/python3.12/site-packages/torch/serialization.py:828\u001b[39m, in \u001b[36m_open_zipfile_writer\u001b[39m\u001b[34m(name_or_buffer)\u001b[39m\n\u001b[32m    826\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    827\u001b[39m     container = _open_zipfile_writer_buffer\n\u001b[32m--> \u001b[39m\u001b[32m828\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcontainer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname_or_buffer\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Python_project/transformers_playground/.venv/lib/python3.12/site-packages/torch/serialization.py:792\u001b[39m, in \u001b[36m_open_zipfile_writer_file.__init__\u001b[39m\u001b[34m(self, name)\u001b[39m\n\u001b[32m    785\u001b[39m     \u001b[38;5;28msuper\u001b[39m().\u001b[34m__init__\u001b[39m(\n\u001b[32m    786\u001b[39m         torch._C.PyTorchFileWriter(\n\u001b[32m    787\u001b[39m             \u001b[38;5;28mself\u001b[39m.file_stream, get_crc32_options(), _get_storage_alignment()\n\u001b[32m    788\u001b[39m         )\n\u001b[32m    789\u001b[39m     )\n\u001b[32m    790\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    791\u001b[39m     \u001b[38;5;28msuper\u001b[39m().\u001b[34m__init__\u001b[39m(\n\u001b[32m--> \u001b[39m\u001b[32m792\u001b[39m         \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_C\u001b[49m\u001b[43m.\u001b[49m\u001b[43mPyTorchFileWriter\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    793\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mget_crc32_options\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_get_storage_alignment\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    794\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    795\u001b[39m     )\n",
      "\u001b[31mRuntimeError\u001b[39m: Parent directory models does not exist."
     ]
    }
   ],
   "source": [
    "# Save\n",
    "torch.save(model.state_dict(), 'models/TransformerLM.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_28159/96251971.py:4: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load('models/TransformerLM.pt'))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "OptimizedModule(\n",
       "  (_orig_mod): TransformerLM(\n",
       "    (token_embedding_table): Embedding(87, 256)\n",
       "    (lm_head): Linear(in_features=256, out_features=87, bias=True)\n",
       "    (blocks): ModuleList(\n",
       "      (0-5): 6 x Block(\n",
       "        (sa_heads): MultiHeadAttention(\n",
       "          (key): Linear(in_features=256, out_features=256, bias=False)\n",
       "          (query): Linear(in_features=256, out_features=256, bias=False)\n",
       "          (value): Linear(in_features=256, out_features=256, bias=False)\n",
       "          (o): Linear(in_features=256, out_features=256, bias=True)\n",
       "        )\n",
       "        (ff_layer): FeedForward(\n",
       "          (lin_1): Linear(in_features=256, out_features=1024, bias=True)\n",
       "          (lin_2): Linear(in_features=1024, out_features=256, bias=True)\n",
       "          (relu): ReLU()\n",
       "        )\n",
       "        (sa_norm): RMSNorm()\n",
       "        (ff_norm): RMSNorm()\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load model\n",
    "model = TransformerLM(config)\n",
    "model = torch.compile(model)\n",
    "model.load_state_dict(torch.load('models/TransformerLM.pt'))\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f1e6d09745c645cab97ce2a92133cf0c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "perplexity: 3.334986925125122 loss: 1.20446875\n"
     ]
    }
   ],
   "source": [
    "# calculate perplexity\n",
    "ppl, loss = perplexity(model, seq_len, seq_len)\n",
    "print(\"perplexity:\", ppl, \"loss:\", loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "00r0pbm3b5eX",
    "outputId": "bdd3b34e-32a0-4724-b528-c162c0fd0c3e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[53, 72, 78,  1, 80, 66, 69, 69,  1, 71, 62, 79, 62, 75]])\n",
      "You will never decide what to do if you will take to see me.\n",
      "\n",
      "I don't mind it.\n",
      "\n",
      "I wonder if it'll be to make it to the bunny today.\n",
      "\n",
      "I wonder if that happens at all.\n",
      "\n",
      "If I can continue fighting something that would be the memories of the moment.\n",
      "\n",
      "But it doesn't matter if I heard that.\n",
      "\n",
      "I don't want to hear that for you.\n",
      "\n",
      "I wonder if I can become this possible.\n",
      "\n",
      "I'm sure that I would like to hear you.\n",
      "\n",
      "I wonder if the things would look like this...\n",
      "\n",
      "I think it's the only one who continues to take a picture of him.\n",
      "\n",
      "Where's he going home?\n",
      "\n",
      "If that's the worst true power to continue the entire training companies,\n",
      "\n",
      "I would have come to the train somewhere seriously.\n",
      "\n",
      "What do you think there are someone who can come to the country of the world line before we met.\n",
      "\n",
      "The next step is the power of the country with the moment I came to the prize again.\n",
      "\n",
      "I am the super bad and she could say that.\n",
      "\n",
      "I can explode the world from the fact that she couldn't make it to the manga she was a man for a manner.\n",
      "\n",
      "I'm a go\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "idx = encode(\"You will never\")\n",
    "print(torch.tensor([idx]))\n",
    "print(decode(model.generate(idx=torch.tensor([idx], dtype=torch.long).to(device), max_new_tokens=1000, temperature=0.5, use_cache=True)[0].tolist()))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
