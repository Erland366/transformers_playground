{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "sdtDsu1Y0EqL"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import os\n",
        "import re\n",
        "import math\n",
        "import random\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "from torch.nn.attention.flex_attention import flex_attention, create_block_mask\n",
        "import triton\n",
        "import triton.language as tl\n",
        "from tqdm.notebook import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython import display\n",
        "from einops import rearrange, repeat, reduce\n",
        "# import lovely_tensors as lt; lt.monkey_patch() # INTRODUCE GRAPH BREAK!\n",
        "\n",
        "torch.manual_seed(69)\n",
        "torch.set_printoptions(profile=\"short\", sci_mode=False, linewidth=100000)\n",
        "torch.set_float32_matmul_precision('high')\n",
        "# this script is configured to run on a RTX 3060 12GB GPU. you'll want to adjust the model sizes and batch sizes for other devices\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "# device = torch.device('cpu')\n",
        "plt.rcParams['figure.figsize'] = [8, 6]\n",
        "plt.rcParams['figure.dpi'] = 50\n",
        "plt.rcParams['axes.grid'] = True\n",
        "plt.rcParams['xtick.minor.visible'] = True\n",
        "plt.rcParams['ytick.minor.visible'] = True\n",
        "\n",
        "USE_SDPA = False\n",
        "USE_TORCH_COMPILE = False\n",
        "\n",
        "device"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Data Prep"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ANr5dn7W0EqR",
        "outputId": "a00cbe8d-e1c6-454b-b552-4ffd17ce17e5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "length of dataset in characters:  39526018\n"
          ]
        }
      ],
      "source": [
        "# we use this 40mb file of concatenated anime subtitles as our dataset\n",
        "# just the right size for toy experiments like this I think\n",
        "with open('animesubs.txt', 'r', encoding='latin') as f:\n",
        "    text = f.read()\n",
        "print(\"length of dataset in characters: \", len(text))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pANiObIZ0EqU",
        "outputId": "98f70469-1c89-4341-89e8-c142a14331b1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Open your mind. Open your mind.\n",
            "\n",
            "Far beyond the deep blue Earth, you and I shall meet...\n",
            "\n",
            "AH! MY GODDESS\n",
            "\n",
            "A snow-white feather comes fluttering down, swaying gently in the air.\n",
            "\n",
            "Without holding back, I want to envelope you, my one and only love.\n",
            "\n",
            "I know I have the power to protect the one I love, right here in my hands.\n",
            "\n",
            "Open your mind. Just as I've always dreamed.\n",
            "\n",
            "Let the wind carry off your hopes, faraway.\n",
            "\n",
            "I have wings nobody can see. Look, you have them, too.\n",
            "\n",
            "They'll take us to where we ca\n"
          ]
        }
      ],
      "source": [
        "print(text[:500])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "WvM5h6_i3KsM"
      },
      "outputs": [],
      "source": [
        "# remove japanese characters\n",
        "text = ''.join(filter(lambda character:ord(character) < 0x3000, text))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U7SOcWJM0EqW",
        "outputId": "a82a4da8-a8ed-47bf-cfb5-2e4c59dee2cf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "unique characters: 86 \n",
            " !'()*+,-./0123456789:;<=>?@ABCDEFGHIJKLMNOPQRSTUVWXYZ[]_abcdefghijklmnopqrstuvwxyz|\n"
          ]
        }
      ],
      "source": [
        "chars = sorted(list(set(text)))\n",
        "vocab_size = len(chars)\n",
        "print(\"unique characters:\", vocab_size, ''.join(chars))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yobmmaeK0EqX",
        "outputId": "26669f38-4b26-4b53-f642-ee7543e7c578"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'\\n': 0, ' ': 1, '!': 2, \"'\": 3, '(': 4, ')': 5, '*': 6, '+': 7, ',': 8, '-': 9, '.': 10, '/': 11, '0': 12, '1': 13, '2': 14, '3': 15, '4': 16, '5': 17, '6': 18, '7': 19, '8': 20, '9': 21, ':': 22, ';': 23, '<': 24, '=': 25, '>': 26, '?': 27, '@': 28, 'A': 29, 'B': 30, 'C': 31, 'D': 32, 'E': 33, 'F': 34, 'G': 35, 'H': 36, 'I': 37, 'J': 38, 'K': 39, 'L': 40, 'M': 41, 'N': 42, 'O': 43, 'P': 44, 'Q': 45, 'R': 46, 'S': 47, 'T': 48, 'U': 49, 'V': 50, 'W': 51, 'X': 52, 'Y': 53, 'Z': 54, '[': 55, ']': 56, '_': 57, 'a': 58, 'b': 59, 'c': 60, 'd': 61, 'e': 62, 'f': 63, 'g': 64, 'h': 65, 'i': 66, 'j': 67, 'k': 68, 'l': 69, 'm': 70, 'n': 71, 'o': 72, 'p': 73, 'q': 74, 'r': 75, 's': 76, 't': 77, 'u': 78, 'v': 79, 'w': 80, 'x': 81, 'y': 82, 'z': 83, '|': 84, '\\x94': 85, '': 86}\n",
            "{0: '\\n', 1: ' ', 2: '!', 3: \"'\", 4: '(', 5: ')', 6: '*', 7: '+', 8: ',', 9: '-', 10: '.', 11: '/', 12: '0', 13: '1', 14: '2', 15: '3', 16: '4', 17: '5', 18: '6', 19: '7', 20: '8', 21: '9', 22: ':', 23: ';', 24: '<', 25: '=', 26: '>', 27: '?', 28: '@', 29: 'A', 30: 'B', 31: 'C', 32: 'D', 33: 'E', 34: 'F', 35: 'G', 36: 'H', 37: 'I', 38: 'J', 39: 'K', 40: 'L', 41: 'M', 42: 'N', 43: 'O', 44: 'P', 45: 'Q', 46: 'R', 47: 'S', 48: 'T', 49: 'U', 50: 'V', 51: 'W', 52: 'X', 53: 'Y', 54: 'Z', 55: '[', 56: ']', 57: '_', 58: 'a', 59: 'b', 60: 'c', 61: 'd', 62: 'e', 63: 'f', 64: 'g', 65: 'h', 66: 'i', 67: 'j', 68: 'k', 69: 'l', 70: 'm', 71: 'n', 72: 'o', 73: 'p', 74: 'q', 75: 'r', 76: 's', 77: 't', 78: 'u', 79: 'v', 80: 'w', 81: 'x', 82: 'y', 83: 'z', 84: '|', 85: '\\x94', 86: ''}\n",
            "encoded: [43, 73, 62, 71, 1, 82, 72, 78, 75, 1, 70, 66, 71, 61, 10, 1, 43, 73, 62, 71]\n",
            "decoded: Open your mind. Open\n",
            "vocab size: 87\n"
          ]
        }
      ],
      "source": [
        "# yes, all language models will be character level, which isn't ideal but it's good for simplicity\n",
        "# very simple tokenizer\n",
        "stoi = {ch:i for i,ch in enumerate(chars)}\n",
        "itos = {i:ch for i,ch in enumerate(chars)}\n",
        "# add special token for padding\n",
        "stoi[''] = len(stoi)\n",
        "itos[len(itos)] = ''\n",
        "print(stoi)\n",
        "print(itos)\n",
        "encode = lambda s: [stoi[ch] for ch in s]\n",
        "decode = lambda l: ''.join([itos[i] for i in l])\n",
        "print(\"encoded:\", encode(text[:20]))\n",
        "print(\"decoded:\", decode(encode(text[:20])))\n",
        "vocab_size = len(itos)\n",
        "print(\"vocab size:\", vocab_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6Pnf9KfP0EqY",
        "outputId": "ff581168-335a-4f0f-efeb-0d4bd5b225ca"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([39526018])"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "data = torch.tensor(encode(text), dtype=torch.int64)\n",
        "data.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lZ2fY1pR0EqY",
        "outputId": "5eb599e0-fb79-4cdb-c4b9-52a781d67151"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([43, 73, 62, 71,  1, 82, 72, 78, 75,  1, 70, 66, 71, 61, 10,  1, 43, 73, 62, 71,  1, 82, 72, 78, 75,  1, 70, 66, 71, 61, 10,  0,  0, 34, 58, 75,  1, 59, 62, 82, 72, 71, 61,  1, 77, 65, 62,  1, 61, 62, 62, 73,  1, 59, 69, 78, 62,  1, 33, 58, 75, 77, 65,  8,  1, 82, 72, 78,  1, 58, 71, 61,  1, 37,  1, 76, 65, 58, 69, 69,  1, 70, 62, 62, 77, 10, 10, 10,  0,  0, 29, 36,  2,  1, 41, 53,  1, 35, 43, 32])"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "data[:100]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oIaYesPh0Eqa",
        "outputId": "caa27cbb-5ae3-4406-8194-646264d4ba99"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([39130757]) torch.Size([395261])\n"
          ]
        }
      ],
      "source": [
        "n = int(len(data) * 0.99)\n",
        "train_data = data[:n]\n",
        "val_data = data[n:]\n",
        "print(train_data.shape, val_data.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4bFhizcI0Eqa",
        "outputId": "c93a4d43-6fb2-4de7-aefc-8fed47e4a861"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([43, 73, 62, 71,  1, 82, 72, 78, 75])"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "seq_len = 8\n",
        "train_data[:seq_len+1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "skFCPvQC0Eqc",
        "outputId": "08990c5b-88af-4a51-ce37-3c59989d6d74"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "inputs:\n",
            "torch.Size([2, 64])\n",
            "tensor([[62, 71, 77,  1, 59, 58, 60, 68,  1, 80, 66, 77, 65,  1, 65, 66, 76,  1, 70, 72, 70,  1, 77, 72,  1, 65, 66, 76,  1, 73, 58, 75, 62, 71, 77, 76,  3,  1, 65, 72, 78, 76, 62, 10,  0,  0, 29, 76,  1, 63, 72, 75,  1, 48, 72, 61, 72, 75, 72, 68, 66,  8,  1, 80],\n",
            "        [58, 75, 62,  1, 66, 71, 79, 78, 69, 71, 62, 75, 58, 59, 69, 62,  2,  0,  0, 51, 65, 62, 75, 62,  1, 61, 66, 61,  1, 77, 65, 62, 82,  1, 61, 66, 76, 58, 73, 73, 62, 58, 75,  1, 77, 72,  2, 27,  0,  0, 40, 62, 77,  3, 76,  1, 64, 72,  1, 76, 58, 79, 62,  1]], device='cuda:0')\n",
            "targets:\n",
            "torch.Size([2, 3, 64])\n",
            "tensor([[[71, 77,  1, 59, 58, 60, 68,  1, 80, 66, 77, 65,  1, 65, 66, 76,  1, 70, 72, 70,  1, 77, 72,  1, 65, 66, 76,  1, 73, 58, 75, 62, 71, 77, 76,  3,  1, 65, 72, 78, 76, 62, 10,  0,  0, 29, 76,  1, 63, 72, 75,  1, 48, 72, 61, 72, 75, 72, 68, 66,  8,  1, 80, 65],\n",
            "         [77,  1, 59, 58, 60, 68,  1, 80, 66, 77, 65,  1, 65, 66, 76,  1, 70, 72, 70,  1, 77, 72,  1, 65, 66, 76,  1, 73, 58, 75, 62, 71, 77, 76,  3,  1, 65, 72, 78, 76, 62, 10,  0,  0, 29, 76,  1, 63, 72, 75,  1, 48, 72, 61, 72, 75, 72, 68, 66,  8,  1, 80, 65, 72],\n",
            "         [ 1, 59, 58, 60, 68,  1, 80, 66, 77, 65,  1, 65, 66, 76,  1, 70, 72, 70,  1, 77, 72,  1, 65, 66, 76,  1, 73, 58, 75, 62, 71, 77, 76,  3,  1, 65, 72, 78, 76, 62, 10,  0,  0, 29, 76,  1, 63, 72, 75,  1, 48, 72, 61, 72, 75, 72, 68, 66,  8,  1, 80, 65, 72,  1]],\n",
            "\n",
            "        [[75, 62,  1, 66, 71, 79, 78, 69, 71, 62, 75, 58, 59, 69, 62,  2,  0,  0, 51, 65, 62, 75, 62,  1, 61, 66, 61,  1, 77, 65, 62, 82,  1, 61, 66, 76, 58, 73, 73, 62, 58, 75,  1, 77, 72,  2, 27,  0,  0, 40, 62, 77,  3, 76,  1, 64, 72,  1, 76, 58, 79, 62,  1, 65],\n",
            "         [62,  1, 66, 71, 79, 78, 69, 71, 62, 75, 58, 59, 69, 62,  2,  0,  0, 51, 65, 62, 75, 62,  1, 61, 66, 61,  1, 77, 65, 62, 82,  1, 61, 66, 76, 58, 73, 73, 62, 58, 75,  1, 77, 72,  2, 27,  0,  0, 40, 62, 77,  3, 76,  1, 64, 72,  1, 76, 58, 79, 62,  1, 65, 66],\n",
            "         [ 1, 66, 71, 79, 78, 69, 71, 62, 75, 58, 59, 69, 62,  2,  0,  0, 51, 65, 62, 75, 62,  1, 61, 66, 61,  1, 77, 65, 62, 82,  1, 61, 66, 76, 58, 73, 73, 62, 58, 75,  1, 77, 72,  2, 27,  0,  0, 40, 62, 77,  3, 76,  1, 64, 72,  1, 76, 58, 79, 62,  1, 65, 66, 70]]], device='cuda:0')\n"
          ]
        }
      ],
      "source": [
        "def get_batch(split, seq_len, batch_size=4, n_future_tokens=1):\n",
        "    data = train_data if split == 'train' else val_data\n",
        "    max_start_index = len(data) - seq_len - n_future_tokens\n",
        "    ix = torch.randint(max_start_index, (batch_size,))\n",
        "    # --- Vectorized Indexing ---\n",
        "    # Instead of looping, we compute all indices at once.\n",
        "\n",
        "    # 1. Create indices for the input sequences `x`.\n",
        "    # `t_range` creates the offsets for each token in a sequence [0, 1, ..., seq_len-1]\n",
        "    t_range = torch.arange(seq_len)\n",
        "    # Use broadcasting to create a grid of indices for the entire batch.\n",
        "    # `ix.unsqueeze(1)` has shape (batch_size, 1)\n",
        "    # `t_range` has shape (seq_len)\n",
        "    # The result `x_indices` has shape (batch_size, seq_len)\n",
        "    x_indices = ix.unsqueeze(1) + t_range\n",
        "\n",
        "    # 2. Create indices for the target sequences `y`.\n",
        "    # `f_range` creates offsets for the future tokens [0, 1, ..., n_future_tokens-1]\n",
        "    f_range = torch.arange(n_future_tokens)\n",
        "    # We build upon `x_indices` to get the target indices.\n",
        "    # `x_indices.unsqueeze(2)` has shape (batch_size, seq_len, 1)\n",
        "    # `f_range` has shape (n_future_tokens)\n",
        "    # Broadcasting creates `y_indices` with shape (batch_size, seq_len, n_future_tokens)\n",
        "    # Each y_indices[b, t, :] corresponds to the targets for input x[b, t].\n",
        "    y_indices = x_indices.unsqueeze(2) + f_range + 1\n",
        "\n",
        "    x = data[x_indices]\n",
        "    y = data[y_indices]\n",
        "    y = y.transpose(1, 2)\n",
        "\n",
        "    if n_future_tokens == 1:\n",
        "        y = y.squeeze(1)\n",
        "    return x.to(device), y.to(device)\n",
        "\n",
        "xb, yb = get_batch('train', 64, 2, 3)\n",
        "print('inputs:')\n",
        "print(xb.shape)\n",
        "print(xb)\n",
        "print('targets:')\n",
        "print(yb.shape)\n",
        "print(yb)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Training Prep"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "327680000"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Make all steps, sequence lengths, and batch size the same\n",
        "total_steps = 5000\n",
        "seq_len = 256\n",
        "batch_size = 256 # these are small models so we can use large batch sizes to fully utilize the GPU\n",
        "# should cover around 2x the dataset\n",
        "total_steps * seq_len * batch_size"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [],
      "source": [
        "def train(model, optimizer, seq_len, batch_size, total_steps, val_steps=10, val_interval=50, n_future_tokens=1):\n",
        "    losses = []\n",
        "    val_losses = []\n",
        "    # live plot\n",
        "    fig, ax = plt.subplots()\n",
        "    dh = display.display(fig, display_id=True)\n",
        "    for steps in (bar := tqdm(range(total_steps))):  # increase number of steps for good results...\n",
        "        # sample a batch of data\n",
        "        xb, yb = get_batch('train', seq_len=seq_len, batch_size=batch_size, n_future_tokens=n_future_tokens)\n",
        "\n",
        "        # evaluate the loss\n",
        "        logits, loss = model(xb, yb)\n",
        "\n",
        "        # backprop\n",
        "        optimizer.zero_grad(set_to_none=True)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        bar.set_description(f\"loss: {loss.item():.2f}, val loss: {val_losses[-1] if val_losses else 0:.2f}\")\n",
        "        losses.append(loss.item())\n",
        "        if steps % val_interval == 0:\n",
        "            # Calculate validation loss\n",
        "            with torch.no_grad():\n",
        "                val_loss = 0\n",
        "                for _ in range(val_steps):\n",
        "                    xb, yb = get_batch('val', seq_len=seq_len, batch_size=batch_size, n_future_tokens=n_future_tokens)\n",
        "                    _, loss = model(xb, yb)\n",
        "                    val_loss += loss.item()\n",
        "                val_loss /= val_steps\n",
        "                val_losses.append(val_loss)\n",
        "            ax.clear()\n",
        "            ax.plot(losses, color='blue', label='train loss', alpha=0.7)\n",
        "            ax.plot(range(0, len(losses), val_interval), val_losses, color='red', label='val loss', alpha=0.7)\n",
        "            ax.set_ylim(1, 4)\n",
        "            ax.legend()\n",
        "            dh.update(fig)\n",
        "    print('final loss:', loss.item(), 'final val loss:', val_loss)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Measure post training perplexity on validation set\n",
        "# Create function that receives a model, context length, and PPL sequence length, and returns the perplexity\n",
        "# The PPL sequence length is the number of characters the function uses to calculate the perplexity\n",
        "# We take the logits and calculate the cross entropy loss from scratch, then exponentiate it to get the perplexity\n",
        "# not only that, but we want the models to do this in actual inference\n",
        "def perplexity(model, seq_len, ppl_seq_len, batch_size=128, val_steps=1000):\n",
        "    with torch.no_grad():\n",
        "        val_loss = 0\n",
        "        for _ in tqdm(range(val_steps)):\n",
        "            xb, yb = get_batch('val', seq_len=seq_len, batch_size=batch_size)\n",
        "            with torch.autocast(device_type=\"cuda\", dtype=torch.bfloat16):\n",
        "                logits, _ = model(xb, yb)\n",
        "            logits = logits.reshape(batch_size, seq_len, vocab_size)\n",
        "            logits = logits[:, :ppl_seq_len]\n",
        "            yb = yb[:, :ppl_seq_len]\n",
        "            # flatten logits and targets\n",
        "            logits = logits.reshape(batch_size*ppl_seq_len, vocab_size)\n",
        "            yb = yb.reshape(batch_size*ppl_seq_len)\n",
        "            # calculate cross entropy loss from scratch\n",
        "            loss = F.cross_entropy(logits, yb)\n",
        "            val_loss += loss.item()\n",
        "        val_loss /= val_steps\n",
        "        ppl = torch.exp(torch.tensor(val_loss))\n",
        "        return ppl.item(), val_loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Transformers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Classic Transformer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "k6cA2WbrayyL"
      },
      "outputs": [],
      "source": [
        "class TransformerConfig:\n",
        "    def __init__(self, vocab_size, seq_len, embed_size, head_num, layer_num):\n",
        "        self.vocab_size = vocab_size\n",
        "        self.seq_len = seq_len\n",
        "        self.embed_size = embed_size\n",
        "        self.head_num = head_num\n",
        "        self.layer_num = layer_num\n",
        "\n",
        "def precompute_freqs_cis(dim: int, end: int, theta: float = 10000.0):\n",
        "    freqs = 1.0 / (theta ** (torch.arange(0, dim, 2)[: (dim // 2)].float() / dim))\n",
        "    t = torch.arange(end, device=freqs.device, dtype=torch.float32)\n",
        "    freqs = torch.outer(t, freqs)\n",
        "    freqs_cis = torch.polar(torch.ones_like(freqs), freqs)  # complex64\n",
        "    return freqs_cis.to(device)\n",
        "\n",
        "def apply_rotary_emb(\n",
        "    xq: torch.Tensor,\n",
        "    xk: torch.Tensor,\n",
        "    freqs_cis: torch.Tensor,\n",
        "):\n",
        "    xq_ = torch.view_as_complex(xq.float().reshape(*xq.shape[:-1], -1, 2))\n",
        "    xk_ = torch.view_as_complex(xk.float().reshape(*xk.shape[:-1], -1, 2))\n",
        "    # freqs_cis = reshape_for_broadcast(freqs_cis, xq_)\n",
        "    q_shape = [d if i == xq_.ndim - 2 or i == xq_.ndim - 1 else 1 for i, d in enumerate(xq_.shape)]\n",
        "    k_shape = [d if i == xq_.ndim - 2 or i == xk_.ndim - 1 else 1 for i, d in enumerate(xk_.shape)]\n",
        "    T_q = xq_.shape[-2] \n",
        "    q_freqs_cis = freqs_cis[-T_q:].view(*q_shape)\n",
        "    k_freqs_cis = freqs_cis.view(*k_shape)\n",
        "    xq_out = torch.view_as_real(xq_ * q_freqs_cis).flatten(xq.dim() - 1)\n",
        "    xk_out = torch.view_as_real(xk_ * k_freqs_cis).flatten(xq.dim() - 1)\n",
        "    return xq_out.type_as(xq), xk_out.type_as(xk)\n",
        "\n",
        "class RMSNorm(torch.nn.Module):\n",
        "    def __init__(self, dim, eps=1e-6):\n",
        "        super().__init__()\n",
        "        self.eps = eps\n",
        "        self.weight = nn.Parameter(torch.ones(dim))\n",
        "\n",
        "    def _norm(self, x):\n",
        "        return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)\n",
        "\n",
        "    def forward(self, x):\n",
        "        output = self._norm(x.float()).type_as(x)\n",
        "        return output * self.weight\n",
        "    \n",
        "class FeedForward(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.lin_1 = nn.Linear(config.embed_size, config.embed_size*4)\n",
        "        self.lin_2 = nn.Linear(config.embed_size*4, config.embed_size)\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.lin_1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.lin_2(x)\n",
        "        return x\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    \"\"\" multiple heads of self-attention with AliBi in parallel \"\"\"\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.seq_len = config.seq_len\n",
        "        self.head_num = config.head_num\n",
        "        self.head_size = config.embed_size // config.head_num\n",
        "        self.key = nn.Linear(config.embed_size, config.embed_size, bias=False)\n",
        "        self.query = nn.Linear(config.embed_size, config.embed_size, bias=False)\n",
        "        self.value = nn.Linear(config.embed_size, config.embed_size, bias=False)\n",
        "        self.o = nn.Linear(config.embed_size, config.embed_size)\n",
        "        # block_mask for FlexAttention\n",
        "        def causal(b, h, q_idx, kv_idx):\n",
        "            causal_mask = q_idx >= kv_idx\n",
        "            return causal_mask\n",
        "        self.causal_mask = create_block_mask(causal, B=None, H=None, Q_LEN=config.seq_len, KV_LEN=config.seq_len)\n",
        "        self.freqs_cis = precompute_freqs_cis(config.embed_size//config.head_num, config.seq_len)\n",
        "        self.register_buffer('tril', torch.tril(torch.ones(config.seq_len, config.seq_len)))\n",
        "\n",
        "    def forward(self, x, kv_cache=None):\n",
        "        B, T, C = x.shape\n",
        "        _, _, T_past, _ = kv_cache[0].shape if kv_cache is not None and kv_cache[0] is not None else (0, 0, 0, 0)\n",
        "        q = self.query(x) # (B,T,C)\n",
        "        k = self.key(x)   # (B,T,C)\n",
        "        v = self.value(x) # (B,T,C)\n",
        "\n",
        "        # Split into heads\n",
        "        q = q.view(B, T, self.head_num, self.head_size).transpose(1, 2) # (B, H, T, C/H)\n",
        "        k = k.view(B, T, self.head_num, self.head_size).transpose(1, 2) # (B, H, T, C/H)\n",
        "        v = v.view(B, T, self.head_num, self.head_size).transpose(1, 2) # (B, H, T, C/H)\n",
        "\n",
        "        if kv_cache is not None:\n",
        "            k_past, v_past = kv_cache\n",
        "            if k_past is not None:\n",
        "                k = torch.cat((k_past, k), dim=2)\n",
        "                v = torch.cat((v_past, v), dim=2)\n",
        "            if k.shape[-2] > self.seq_len:\n",
        "                k = k[:, :, -self.seq_len:]\n",
        "                v = v[:, :, -self.seq_len:]\n",
        "            kv_cache = (k, v)\n",
        "        T_k = k.shape[-2]\n",
        "        q, k = apply_rotary_emb(q, k, self.freqs_cis[:T_k])\n",
        "\n",
        "        if T == self.seq_len:\n",
        "            out = flex_attention(q, k, v, block_mask=self.causal_mask)\n",
        "        else:\n",
        "            # compute attention scores (\"affinities\")\n",
        "            wei = q @ k.transpose(-2,-1) # (B, H, 1, C/H) @ (B, H, C/H, T) -> (B, H, 1, T)\n",
        "            wei = wei * self.head_size ** -0.5 # scaled attention\n",
        "            wei = wei.masked_fill(self.tril[T_k-T:T_k, T_k-T:T_k] == 0, float('-inf')) # (B, T, T)\n",
        "            wei = F.softmax(wei, dim=-1) # (B, H, T, T)\n",
        "            # apply attention to values\n",
        "            out = wei @ v # (B, H, 1, T) @ (B, H, T, C/H) -> (B, H, 1, C/H)\n",
        "\n",
        "        out = out.transpose(1, 2).contiguous().view(B, T, C) # (B, H, T, C/H) -> (B, T, H, C/H) -> (B, T, C)\n",
        "        out = self.o(out)\n",
        "        return out, kv_cache\n",
        "\n",
        "class Block(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.sa_heads = MultiHeadAttention(config)\n",
        "        self.ff_layer = FeedForward(config)\n",
        "        self.sa_norm = RMSNorm(config.embed_size)\n",
        "        self.ff_norm = RMSNorm(config.embed_size)\n",
        "    \n",
        "    def forward(self, x, kv_cache=None):\n",
        "        a, kv_cache = self.sa_heads(self.sa_norm(x), kv_cache)\n",
        "        h = x + a\n",
        "        o = h + self.ff_layer(self.ff_norm(h))\n",
        "        return o, kv_cache\n",
        "    \n",
        "class TransformerLM(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.layer_num = config.layer_num\n",
        "        self.head_num = config.head_num\n",
        "        self.seq_len = config.seq_len\n",
        "        # embed raw tokens to a lower dimensional embedding with embed_size\n",
        "        self.token_embedding_table = nn.Embedding(config.vocab_size, config.embed_size)\n",
        "        # Language Modelling (?) Head is a standard linear layer to go from \n",
        "        # embeddings back to logits of vocab_size\n",
        "        self.lm_head = nn.Linear(config.embed_size, config.vocab_size)\n",
        "        # transformer blocks\n",
        "        self.blocks = nn.ModuleList([Block(config) for _ in range(config.layer_num)])\n",
        "\n",
        "    def forward(self, idx, targets=None, kv_cache=None):\n",
        "        B, T = idx.shape\n",
        "        _, _, T_past, _ = kv_cache[0][0].shape if kv_cache is not None and kv_cache[0][0] is not None else (0, 0, 0, 0)\n",
        "        # idx and targets are both (B,T) tensor of integers\n",
        "        tok_embd = self.token_embedding_table(idx) # (B,T,C)\n",
        "        x = tok_embd\n",
        "        # go through blocks\n",
        "        for i, block in enumerate(self.blocks):\n",
        "            x, cache = block(x, None if kv_cache is None else kv_cache[i])\n",
        "            if kv_cache is not None:\n",
        "                kv_cache[i] = cache\n",
        "        # get logits with linear layer\n",
        "        logits = self.lm_head(x) # (B,T,V)\n",
        "        \n",
        "        if targets is None:\n",
        "            loss = None\n",
        "        else:\n",
        "            B, T, V = logits.shape\n",
        "            logits = logits.view(B*T, V)\n",
        "            targets = targets.view(B*T)\n",
        "            loss = F.cross_entropy(logits, targets)\n",
        "\n",
        "        return logits, loss\n",
        "    \n",
        "    def generate(self, idx, max_new_tokens, temperature=1, use_cache=True):\n",
        "        if use_cache:\n",
        "            # initialize key-value cache\n",
        "            kv_cache = [(None, None) for _ in range(self.layer_num)]\n",
        "            # idx is (B, T) array of indices in the current context\n",
        "            # crop idx to the last seq_len tokens\n",
        "            idx_context = idx[:, -self.seq_len:]\n",
        "            for _ in range(max_new_tokens):\n",
        "                # get the predictions\n",
        "                logits, loss = self(idx_context, kv_cache=kv_cache)\n",
        "                # focus only on the last time step\n",
        "                logits = logits[:, -1, :] # becomes (B, C)\n",
        "                # apply temperature\n",
        "                logits = logits / temperature if temperature > 0 else logits\n",
        "                # apply softmax to get probabilities\n",
        "                probs = F.softmax(logits, dim=-1) # (B, C)\n",
        "                # sample from the distribution\n",
        "                idx_next = torch.multinomial(probs, num_samples=1) if temperature > 0 else torch.argmax(probs, dim=-1, keepdim=True) # (B, 1)\n",
        "                # append sampled index to the running sequence\n",
        "                idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
        "                # since we have kv cache, only need to pass new token\n",
        "                idx_context = idx_next\n",
        "            return idx\n",
        "        else:\n",
        "            # idx is (B, T) array of indices in the current context\n",
        "            for _ in range(max_new_tokens):\n",
        "                #crop idx to the last seq_len tokens\n",
        "                idx_context = idx[:, -self.seq_len:]\n",
        "                # get the predictions\n",
        "                logits, loss = self(idx_context)\n",
        "                # focus only on the last time step\n",
        "                logits = logits[:, -1, :] # becomes (B, C)\n",
        "                # apply temperature\n",
        "                logits = logits / temperature if temperature > 0 else logits\n",
        "                # apply softmax to get probabilities\n",
        "                probs = F.softmax(logits, dim=-1) # (B, C)\n",
        "                # sample from the distribution\n",
        "                idx_next = torch.multinomial(probs, num_samples=1) if temperature > 0 else torch.argmax(probs, dim=-1, keepdim=True) # (B, 1)\n",
        "                # append sampled index to the running sequence\n",
        "                idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
        "            return idx"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "4775511"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Test forward pass\n",
        "config = TransformerConfig(\n",
        "    vocab_size=vocab_size,\n",
        "    seq_len=seq_len,\n",
        "    embed_size=256,\n",
        "    head_num=4,\n",
        "    layer_num=6\n",
        ")\n",
        "m = TransformerLM(config)\n",
        "m.to(device)\n",
        "xb, yb = get_batch('train', 5, 1)\n",
        "logits, loss = m(xb, yb)\n",
        "total_params = sum(p.numel() for p in m.parameters() if p.requires_grad)\n",
        "total_params"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 400x300 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "de0c1946b5324752b0fe5bdcfb9a51cb",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/5000 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/erland-home/Python_project/transformers_playground/.venv/lib/python3.12/site-packages/torch/_inductor/lowering.py:1917: UserWarning: Torchinductor does not support code generation for complex operators. Performance may be worse than eager.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "final loss: 1.0691111087799072 final val loss: 1.2072466731071472\n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 400x300 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# training!\n",
        "model = TransformerLM(config)\n",
        "model = torch.compile(model)\n",
        "model.to(device)\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=2e-3)\n",
        "train(model, optimizer, seq_len, batch_size, total_steps)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save\n",
        "torch.save(model.state_dict(), 'models/TransformerLM.pt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "OptimizedModule(\n",
              "  (_orig_mod): TransformerLM(\n",
              "    (token_embedding_table): Embedding(87, 256)\n",
              "    (lm_head): Linear(in_features=256, out_features=87, bias=True)\n",
              "    (blocks): ModuleList(\n",
              "      (0-5): 6 x Block(\n",
              "        (sa_heads): MultiHeadAttention(\n",
              "          (key): Linear(in_features=256, out_features=256, bias=False)\n",
              "          (query): Linear(in_features=256, out_features=256, bias=False)\n",
              "          (value): Linear(in_features=256, out_features=256, bias=False)\n",
              "          (o): Linear(in_features=256, out_features=256, bias=True)\n",
              "        )\n",
              "        (ff_layer): FeedForward(\n",
              "          (lin_1): Linear(in_features=256, out_features=1024, bias=True)\n",
              "          (lin_2): Linear(in_features=1024, out_features=256, bias=True)\n",
              "          (relu): ReLU()\n",
              "        )\n",
              "        (sa_norm): RMSNorm()\n",
              "        (ff_norm): RMSNorm()\n",
              "      )\n",
              "    )\n",
              "  )\n",
              ")"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Load model\n",
        "model = TransformerLM(config)\n",
        "model = torch.compile(model)\n",
        "model.load_state_dict(torch.load('models/TransformerLM.pt'))\n",
        "model.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "5e0eededd891485b863673fe8e348032",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/1000 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "perplexity: 3.3366286754608154 loss: 1.2049609375\n"
          ]
        }
      ],
      "source": [
        "# calculate perplexity\n",
        "ppl, loss = perplexity(model, seq_len, seq_len)\n",
        "print(\"perplexity:\", ppl, \"loss:\", loss)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "00r0pbm3b5eX",
        "outputId": "bdd3b34e-32a0-4724-b528-c162c0fd0c3e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[53, 72, 78,  1, 80, 66, 69, 69,  1, 71, 62, 79, 62, 75]])\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "You will never do that!\n",
            "\n",
            "If you can be saying that the rest of us will be to save us...\n",
            "\n",
            "I can't forgive you because you came to see me.\n",
            "\n",
            "I'm sure you'll be a pretty good friend with me.\n",
            "\n",
            "I thought I was the only one who dreamed of me.\n",
            "\n",
            "I'd like to ask you this time.\n",
            "\n",
            "The book is a fantasy side that you're pretty surprised.\n",
            "\n",
            "If you have no idea what you want to tell me.\n",
            "\n",
            "I mean, you should think of my own life.\n",
            "\n",
            "It feels so surprised.\n",
            "\n",
            "It was all a drink.\n",
            "\n",
            "Should I tell you to stop stopping the magic power once you're done.\n",
            "\n",
            "I don't want to go out with you.\n",
            "\n",
            "You're really a mistake.\n",
            "\n",
            "What are you talking about?\n",
            "\n",
            "I have a talk about what I said would think.\n",
            "\n",
            "I saw you will wear the top out of the magic of the magic words.\n",
            "\n",
            "Why don't you try to stop me with that face?\n",
            "\n",
            "I was always to be a coincidence.\n",
            "\n",
            "In the magic case, I tried to stop you.\n",
            "\n",
            "I was worried about you all about that proper thing.\n",
            "\n",
            "I was thinking about working at the first step toward the story.\n",
            "\n",
            "I can see that I felt like I can do it a\n"
          ]
        }
      ],
      "source": [
        "model.eval()\n",
        "idx = encode(\"You will never\")\n",
        "print(torch.tensor([idx]))\n",
        "print(decode(model.generate(idx=torch.tensor([idx], dtype=torch.long).to(device), max_new_tokens=1000, temperature=0.5, use_cache=True)[0].tolist()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## MTP"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class MTPTransformerConfig:\n",
        "    def __init__(self, vocab_size, seq_len, embed_size, head_num, layer_num, n_future_tokens):\n",
        "        self.vocab_size = vocab_size\n",
        "        self.seq_len = seq_len\n",
        "        self.embed_size = embed_size\n",
        "        self.head_num = head_num\n",
        "        self.layer_num = layer_num\n",
        "        self.n_future_tokens = n_future_tokens\n",
        "\n",
        "@torch.compiler.disable\n",
        "def precompute_freqs_cis(dim: int, end: int, theta: float = 10000.0):\n",
        "    freqs = 1.0 / (theta ** (torch.arange(0, dim, 2)[: (dim // 2)].float() / dim))\n",
        "    t = torch.arange(end, device=freqs.device, dtype=torch.float32)\n",
        "    freqs = torch.outer(t, freqs)\n",
        "    freqs_cis = torch.polar(torch.ones_like(freqs), freqs)  # complex64\n",
        "    return freqs_cis.to(device)\n",
        "\n",
        "@torch.compiler.disable\n",
        "def apply_rotary_emb(\n",
        "    xq: torch.Tensor,\n",
        "    xk: torch.Tensor,\n",
        "    freqs_cis: torch.Tensor,\n",
        "):\n",
        "    xq_ = torch.view_as_complex(xq.float().reshape(*xq.shape[:-1], -1, 2))\n",
        "    xk_ = torch.view_as_complex(xk.float().reshape(*xk.shape[:-1], -1, 2))\n",
        "    # freqs_cis = reshape_for_broadcast(freqs_cis, xq_)\n",
        "    q_shape = [d if i == xq_.ndim - 2 or i == xq_.ndim - 1 else 1 for i, d in enumerate(xq_.shape)]\n",
        "    k_shape = [d if i == xq_.ndim - 2 or i == xk_.ndim - 1 else 1 for i, d in enumerate(xk_.shape)]\n",
        "    T_q = xq_.shape[-2] \n",
        "    q_freqs_cis = freqs_cis[-T_q:].view(*q_shape)\n",
        "    k_freqs_cis = freqs_cis.view(*k_shape)\n",
        "    xq_out = torch.view_as_real(xq_ * q_freqs_cis).flatten(xq.dim() - 1)\n",
        "    xk_out = torch.view_as_real(xk_ * k_freqs_cis).flatten(xq.dim() - 1)\n",
        "    return xq_out.type_as(xq), xk_out.type_as(xk)\n",
        "\n",
        "class RMSNorm(torch.nn.Module):\n",
        "    def __init__(self, dim, eps=1e-6):\n",
        "        super().__init__()\n",
        "        self.eps = eps\n",
        "        self.weight = nn.Parameter(torch.ones(dim))\n",
        "\n",
        "    def _norm(self, x):\n",
        "        return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)\n",
        "\n",
        "    def forward(self, x):\n",
        "        output = self._norm(x.float()).type_as(x)\n",
        "        return output * self.weight\n",
        "    \n",
        "class FeedForward(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.lin_1 = nn.Linear(config.embed_size, config.embed_size*4)\n",
        "        self.lin_2 = nn.Linear(config.embed_size*4, config.embed_size)\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.lin_1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.lin_2(x)\n",
        "        return x\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    \"\"\" multiple heads of self-attention with AliBi in parallel \"\"\"\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.seq_len = config.seq_len\n",
        "        self.head_num = config.head_num\n",
        "        self.head_size = config.embed_size // config.head_num\n",
        "        self.key = nn.Linear(config.embed_size, config.embed_size, bias=False)\n",
        "        self.query = nn.Linear(config.embed_size, config.embed_size, bias=False)\n",
        "        self.value = nn.Linear(config.embed_size, config.embed_size, bias=False)\n",
        "        self.o = nn.Linear(config.embed_size, config.embed_size)\n",
        "        # block_mask for FlexAttention\n",
        "        def causal(b, h, q_idx, kv_idx):\n",
        "            causal_mask = q_idx >= kv_idx\n",
        "            return causal_mask\n",
        "        self.causal_mask = create_block_mask(causal, B=None, H=None, Q_LEN=config.seq_len, KV_LEN=config.seq_len)\n",
        "        self.freqs_cis = precompute_freqs_cis(config.embed_size//config.head_num, config.seq_len)\n",
        "        self.register_buffer('tril', torch.tril(torch.ones(config.seq_len, config.seq_len)))\n",
        "\n",
        "    def forward(self, x, kv_cache=None):\n",
        "        B, T, C = x.shape\n",
        "        _, _, T_past, _ = kv_cache[0].shape if kv_cache is not None and kv_cache[0] is not None else (0, 0, 0, 0)\n",
        "        q = self.query(x) # (B,T,C)\n",
        "        k = self.key(x)   # (B,T,C)\n",
        "        v = self.value(x) # (B,T,C)\n",
        "\n",
        "        # Split into heads\n",
        "        q = q.view(B, T, self.head_num, self.head_size).transpose(1, 2) # (B, H, T, C/H)\n",
        "        k = k.view(B, T, self.head_num, self.head_size).transpose(1, 2) # (B, H, T, C/H)\n",
        "        v = v.view(B, T, self.head_num, self.head_size).transpose(1, 2) # (B, H, T, C/H)\n",
        "\n",
        "        if kv_cache is not None:\n",
        "            k_past, v_past = kv_cache\n",
        "            if k_past is not None:\n",
        "                k = torch.cat((k_past, k), dim=2)\n",
        "                v = torch.cat((v_past, v), dim=2)\n",
        "            if k.shape[-2] > self.seq_len:\n",
        "                k = k[:, :, -self.seq_len:]\n",
        "                v = v[:, :, -self.seq_len:]\n",
        "            kv_cache = (k, v)\n",
        "        T_k = k.shape[-2]\n",
        "        q, k = apply_rotary_emb(q, k, self.freqs_cis[:T_k])\n",
        "\n",
        "        if T == self.seq_len:\n",
        "            out = flex_attention(q, k, v, block_mask=self.causal_mask)\n",
        "        else:\n",
        "            # compute attention scores (\"affinities\")\n",
        "            wei = q @ k.transpose(-2,-1) # (B, H, 1, C/H) @ (B, H, C/H, T) -> (B, H, 1, T)\n",
        "            wei = wei * self.head_size ** -0.5 # scaled attention\n",
        "            wei = wei.masked_fill(self.tril[T_k-T:T_k, T_k-T:T_k] == 0, float('-inf')) # (B, T, T)\n",
        "            wei = F.softmax(wei, dim=-1) # (B, H, T, T)\n",
        "            # apply attention to values\n",
        "            out = wei @ v # (B, H, 1, T) @ (B, H, T, C/H) -> (B, H, 1, C/H)\n",
        "\n",
        "        out = out.transpose(1, 2).contiguous().view(B, T, C) # (B, H, T, C/H) -> (B, T, H, C/H) -> (B, T, C)\n",
        "        out = self.o(out)\n",
        "        return out, kv_cache\n",
        "\n",
        "class Block(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.sa_heads = MultiHeadAttention(config)\n",
        "        self.ff_layer = FeedForward(config)\n",
        "        self.sa_norm = RMSNorm(config.embed_size)\n",
        "        self.ff_norm = RMSNorm(config.embed_size)\n",
        "    \n",
        "    def forward(self, x, kv_cache=None):\n",
        "        a, kv_cache = self.sa_heads(self.sa_norm(x), kv_cache)\n",
        "        h = x + a\n",
        "        o = h + self.ff_layer(self.ff_norm(h))\n",
        "        return o, kv_cache\n",
        "    \n",
        "class MTPTransformerLM(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.layer_num = config.layer_num\n",
        "        self.head_num = config.head_num\n",
        "        self.seq_len = config.seq_len\n",
        "        self.n_future_tokens = config.n_future_tokens\n",
        "        # embed raw tokens to a lower dimensional embedding with embed_size\n",
        "        self.token_embedding_table = nn.Embedding(config.vocab_size, config.embed_size)\n",
        "        # Language Modelling (?) Head is a standard linear layer to go from \n",
        "        # embeddings back to logits of vocab_size\n",
        "        self.lm_head = nn.Linear(config.embed_size, config.vocab_size)\n",
        "        # transformer blocks\n",
        "        self.blocks = nn.ModuleList([Block(config) for _ in range(config.layer_num - self.n_future_tokens)])\n",
        "        self.extra_heads = nn.ModuleList([Block(config) for _ in range(self.n_future_tokens)])\n",
        "\n",
        "    def forward(self, idx, targets=None, kv_cache=None, return_all_heads=False):\n",
        "        B, T = idx.shape\n",
        "        _, _, T_past, _ = kv_cache[0][0].shape if kv_cache is not None and kv_cache[0][0] is not None else (0, 0, 0, 0)\n",
        "        # idx and targets are both (B,T) tensor of integers\n",
        "        tok_embd = self.token_embedding_table(idx) # (B,T,C)\n",
        "        x = tok_embd\n",
        "        # go through blocks\n",
        "        for i, block in enumerate(self.blocks):\n",
        "            x, cache = block(x, None if kv_cache is None else kv_cache[i])\n",
        "            if kv_cache is not None:\n",
        "                kv_cache[i] = cache\n",
        "        # get logits with linear layer\n",
        "\n",
        "        # MTP\n",
        "        trunk = x # (B,T,C)\n",
        "\n",
        "        latents = []\n",
        "        n_heads_to_use = self.n_future_tokens if return_all_heads else 1\n",
        "        prediction_heads = [self.blocks[-1]] + list(self.extra_heads)\n",
        "\n",
        "        for i, block in enumerate(prediction_heads[:n_heads_to_use]):\n",
        "            x, cache = block(x, None if kv_cache is None else kv_cache[i + self.layer_num - self.n_future_tokens])\n",
        "            if kv_cache is not None:\n",
        "                kv_cache[i + self.layer_num - self.n_future_tokens] = cache\n",
        "            latents.append(x)\n",
        "\n",
        "        x = torch.stack(latents, dim=-2) # (B, T, n_heads_to_use, C)\n",
        "\n",
        "        all_logits = self.lm_head(x) # (B, T, n_heads_to_use, V)\n",
        "        \n",
        "        if targets is None:\n",
        "            loss = None\n",
        "        else:\n",
        "            B, T, n_heads_to_use, V = all_logits.shape\n",
        "            logits_flat = all_logits.view(-1, V)\n",
        "            targets_flat = targets.reshape(-1)\n",
        "            loss = F.cross_entropy(logits_flat, targets_flat)\n",
        "\n",
        "        logits = all_logits if return_all_heads else all_logits[:, :, 0, :] # Return the first head only\n",
        "        return logits, loss\n",
        "    \n",
        "    def generate(self, idx, max_new_tokens, temperature=1, use_cache=True):\n",
        "        if use_cache:\n",
        "            # initialize key-value cache\n",
        "            kv_cache = [(None, None) for _ in range(self.layer_num)]\n",
        "            # idx is (B, T) array of indices in the current context\n",
        "            # crop idx to the last seq_len tokens\n",
        "            idx_context = idx[:, -self.seq_len:]\n",
        "            for _ in range(max_new_tokens):\n",
        "                # get the predictions\n",
        "                logits, loss = self(idx_context, kv_cache=kv_cache)\n",
        "                # focus only on the last time step\n",
        "                logits = logits[:, -1, :] # becomes (B, C)\n",
        "                # apply temperature\n",
        "                logits = logits / temperature if temperature > 0 else logits\n",
        "                # apply softmax to get probabilities\n",
        "                probs = F.softmax(logits, dim=-1) # (B, C)\n",
        "                # sample from the distribution\n",
        "                idx_next = torch.multinomial(probs, num_samples=1) if temperature > 0 else torch.argmax(probs, dim=-1, keepdim=True) # (B, 1)\n",
        "                # append sampled index to the running sequence\n",
        "                idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
        "                # since we have kv cache, only need to pass new token\n",
        "                idx_context = idx_next\n",
        "            return idx\n",
        "        else:\n",
        "            # idx is (B, T) array of indices in the current context\n",
        "            for _ in range(max_new_tokens):\n",
        "                #crop idx to the last seq_len tokens\n",
        "                idx_context = idx[:, -self.seq_len:]\n",
        "                # get the predictions\n",
        "                logits, loss = self(idx_context)\n",
        "                # focus only on the last time step\n",
        "                logits = logits[:, -1, :] # becomes (B, C)\n",
        "                # apply temperature\n",
        "                logits = logits / temperature if temperature > 0 else logits\n",
        "                # apply softmax to get probabilities\n",
        "                probs = F.softmax(logits, dim=-1) # (B, C)\n",
        "                # sample from the distribution\n",
        "                idx_next = torch.multinomial(probs, num_samples=1) if temperature > 0 else torch.argmax(probs, dim=-1, keepdim=True) # (B, 1)\n",
        "                # append sampled index to the running sequence\n",
        "                idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
        "            return idx"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "4775511"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Test forward pass\n",
        "n_future_tokens = 1\n",
        "config = MTPTransformerConfig(\n",
        "    vocab_size=vocab_size,\n",
        "    seq_len=seq_len,\n",
        "    embed_size=256,\n",
        "    head_num=4,\n",
        "    layer_num=6,\n",
        "    n_future_tokens=n_future_tokens\n",
        ")\n",
        "m = MTPTransformerLM(config)\n",
        "m.to(device)\n",
        "xb, yb = get_batch('train', 5, 1, n_future_tokens)\n",
        "logits, loss = m(xb, yb, return_all_heads=True)\n",
        "total_params = sum(p.numel() for p in m.parameters() if p.requires_grad)\n",
        "total_params"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 400x300 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "1f236c66500b420fbbee0c612296966b",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/5000 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "final loss: 1.0742982625961304 final val loss: 1.213886284828186\n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 400x300 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# training!\n",
        "model = MTPTransformerLM(config)\n",
        "model = torch.compile(model)\n",
        "model.to(device)\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=2e-3)\n",
        "train(model, optimizer, seq_len, batch_size, total_steps)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save\n",
        "torch.save(model.state_dict(), 'models/MTPTransformerLM.pt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "OptimizedModule(\n",
              "  (_orig_mod): MTPTransformerLM(\n",
              "    (token_embedding_table): Embedding(87, 256)\n",
              "    (lm_head): Linear(in_features=256, out_features=87, bias=True)\n",
              "    (blocks): ModuleList(\n",
              "      (0-4): 5 x Block(\n",
              "        (sa_heads): MultiHeadAttention(\n",
              "          (key): Linear(in_features=256, out_features=256, bias=False)\n",
              "          (query): Linear(in_features=256, out_features=256, bias=False)\n",
              "          (value): Linear(in_features=256, out_features=256, bias=False)\n",
              "          (o): Linear(in_features=256, out_features=256, bias=True)\n",
              "        )\n",
              "        (ff_layer): FeedForward(\n",
              "          (lin_1): Linear(in_features=256, out_features=1024, bias=True)\n",
              "          (lin_2): Linear(in_features=1024, out_features=256, bias=True)\n",
              "          (relu): ReLU()\n",
              "        )\n",
              "        (sa_norm): RMSNorm()\n",
              "        (ff_norm): RMSNorm()\n",
              "      )\n",
              "    )\n",
              "    (extra_heads): ModuleList(\n",
              "      (0): Block(\n",
              "        (sa_heads): MultiHeadAttention(\n",
              "          (key): Linear(in_features=256, out_features=256, bias=False)\n",
              "          (query): Linear(in_features=256, out_features=256, bias=False)\n",
              "          (value): Linear(in_features=256, out_features=256, bias=False)\n",
              "          (o): Linear(in_features=256, out_features=256, bias=True)\n",
              "        )\n",
              "        (ff_layer): FeedForward(\n",
              "          (lin_1): Linear(in_features=256, out_features=1024, bias=True)\n",
              "          (lin_2): Linear(in_features=1024, out_features=256, bias=True)\n",
              "          (relu): ReLU()\n",
              "        )\n",
              "        (sa_norm): RMSNorm()\n",
              "        (ff_norm): RMSNorm()\n",
              "      )\n",
              "    )\n",
              "  )\n",
              ")"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Load model\n",
        "model = MTPTransformerLM(config)\n",
        "model = torch.compile(model)\n",
        "model.load_state_dict(torch.load('models/MTPTransformerLM.pt'))\n",
        "model.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "7032311b2b894316aa7dd649b6705882",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/1000 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "perplexity: 3.348928928375244 loss: 1.208640625\n"
          ]
        }
      ],
      "source": [
        "# calculate perplexity\n",
        "ppl, loss = perplexity(model, seq_len, seq_len)\n",
        "print(\"perplexity:\", ppl, \"loss:\", loss)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[53, 72, 78,  1, 80, 66, 69, 69,  1, 71, 62, 79, 62, 75]])\n",
            "You will never be sold by your friends and worked out.\n",
            "\n",
            "Why are they here?\n",
            "\n",
            "I won't let you try to survive the new land.\n",
            "\n",
            "You don't need to be the one who was a bit stronger than that?\n",
            "\n",
            "I won't be able to do that.\n",
            "\n",
            "Why don't you come to see that to help you?\n",
            "\n",
            "I think that I thought I was able to be that bad at the captain of the ground.\n",
            "\n",
            "I can't treat you to the control of the Secrets will be the worst position of the first place.\n",
            "\n",
            "I wonder if it can't be a total power to see the Secret of the Sea God.\n",
            "\n",
            "But I don't think that I want to see it on a day.\n",
            "\n",
            "The Secret said that we want to show the problem here.\n",
            "\n",
            "He said that he looked in a relationship with him at the first time on the same time.\n",
            "\n",
            "Allow us to have to do is think of it, control of the main station.\n",
            "\n",
            "The Secret is a mistake.\n",
            "\n",
            "The Sea God of Tenkaidou is a man or something else in the stars of the dead face.\n",
            "\n",
            "It's a perfect rest for the Soul Reaper\n",
            "\n",
            "of the Soul Reapers are already served to the very eyes and submitted to be a bit longer,\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "model.eval()\n",
        "idx = encode(\"You will never\")\n",
        "print(torch.tensor([idx]))\n",
        "print(decode(model.generate(idx=torch.tensor([idx], dtype=torch.long).to(device), max_new_tokens=1000, temperature=0.5, use_cache=True)[0].tolist()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Saving Memory MTP"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [],
      "source": [
        "class MTPTransformerConfig:\n",
        "    def __init__(self, vocab_size, seq_len, embed_size, head_num, layer_num, n_future_tokens):\n",
        "        self.vocab_size = vocab_size\n",
        "        self.seq_len = seq_len\n",
        "        self.embed_size = embed_size\n",
        "        self.head_num = head_num\n",
        "        self.layer_num = layer_num\n",
        "        self.n_future_tokens = n_future_tokens\n",
        "\n",
        "def precompute_freqs_cis(dim: int, end: int, theta: float = 10000.0):\n",
        "    freqs = 1.0 / (theta ** (torch.arange(0, dim, 2)[: (dim // 2)].float() / dim))\n",
        "    t = torch.arange(end, device=freqs.device, dtype=torch.float32)\n",
        "    freqs = torch.outer(t, freqs)\n",
        "    freqs_cis = torch.polar(torch.ones_like(freqs), freqs)\n",
        "    return freqs_cis.to(device)\n",
        "\n",
        "@torch.compiler.disable\n",
        "def apply_rotary_emb(xq: torch.Tensor, xk: torch.Tensor, freqs_cis: torch.Tensor):\n",
        "    xq_ = torch.view_as_complex(xq.float().reshape(*xq.shape[:-1], -1, 2))\n",
        "    xk_ = torch.view_as_complex(xk.float().reshape(*xk.shape[:-1], -1, 2))\n",
        "    \n",
        "    # Reshape freqs_cis for broadcasting\n",
        "    shape = [1] * (xq_.ndim - 2) + list(freqs_cis.shape)\n",
        "    freqs_cis = freqs_cis.view(*shape)\n",
        "\n",
        "    xq_out = torch.view_as_real(xq_ * freqs_cis).flatten(3)\n",
        "    xk_out = torch.view_as_real(xk_ * freqs_cis).flatten(3)\n",
        "    return xq_out.type_as(xq), xk_out.type_as(xk)\n",
        "\n",
        "class RMSNorm(torch.nn.Module):\n",
        "    def __init__(self, dim, eps=1e-6):\n",
        "        super().__init__()\n",
        "        self.eps = eps\n",
        "        self.weight = nn.Parameter(torch.ones(dim))\n",
        "\n",
        "    def _norm(self, x):\n",
        "        return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)\n",
        "\n",
        "    def forward(self, x):\n",
        "        output = self._norm(x.float()).type_as(x)\n",
        "        return output * self.weight\n",
        "\n",
        "class FeedForward(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.lin_1 = nn.Linear(config.embed_size, config.embed_size*4)\n",
        "        self.lin_2 = nn.Linear(config.embed_size*4, config.embed_size)\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.lin_1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.lin_2(x)\n",
        "        return x\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.seq_len = config.seq_len\n",
        "        self.head_num = config.head_num\n",
        "        self.head_size = config.embed_size // config.head_num\n",
        "        self.key = nn.Linear(config.embed_size, config.embed_size, bias=False)\n",
        "        self.query = nn.Linear(config.embed_size, config.embed_size, bias=False)\n",
        "        self.value = nn.Linear(config.embed_size, config.embed_size, bias=False)\n",
        "        self.o = nn.Linear(config.embed_size, config.embed_size)\n",
        "        \n",
        "        def causal(b, h, q_idx, kv_idx):\n",
        "            return q_idx >= kv_idx\n",
        "        self.causal_mask = create_block_mask(causal, B=None, H=None, Q_LEN=config.seq_len, KV_LEN=config.seq_len)\n",
        "        self.freqs_cis = precompute_freqs_cis(config.embed_size//config.head_num, config.seq_len)\n",
        "        self.register_buffer('tril', torch.tril(torch.ones(config.seq_len, config.seq_len)))\n",
        "\n",
        "    def forward(self, x, kv_cache=None):\n",
        "        B, T, C = x.shape\n",
        "        q = self.query(x)\n",
        "        k = self.key(x)\n",
        "        v = self.value(x)\n",
        "\n",
        "        q = q.view(B, T, self.head_num, self.head_size).transpose(1, 2)\n",
        "        k = k.view(B, T, self.head_num, self.head_size).transpose(1, 2)\n",
        "        v = v.view(B, T, self.head_num, self.head_size).transpose(1, 2)\n",
        "\n",
        "        if kv_cache is not None:\n",
        "            k_past, v_past = kv_cache\n",
        "            if k_past is not None:\n",
        "                k = torch.cat((k_past, k), dim=2)\n",
        "                v = torch.cat((v_past, v), dim=2)\n",
        "            if k.shape[-2] > self.seq_len:\n",
        "                k = k[:, :, -self.seq_len:]\n",
        "                v = v[:, :, -self.seq_len:]\n",
        "            kv_cache = (k, v)\n",
        "        \n",
        "        T_k = k.shape[-2]\n",
        "        q, k = apply_rotary_emb(q, k, self.freqs_cis[:T_k])\n",
        "        \n",
        "        if T == self.seq_len and self.causal_mask is not None:\n",
        "             out = flex_attention(q, k, v, block_mask=self.causal_mask)\n",
        "        else:\n",
        "            wei = q @ k.transpose(-2,-1) * (self.head_size**-0.5)\n",
        "            wei = wei.masked_fill(self.tril[:T, :T_k] == 0, float('-inf'))\n",
        "            wei = F.softmax(wei, dim=-1)\n",
        "            out = wei @ v\n",
        "\n",
        "        out = out.transpose(1, 2).contiguous().view(B, T, C)\n",
        "        out = self.o(out)\n",
        "        return out, kv_cache\n",
        "\n",
        "class Block(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.sa_heads = MultiHeadAttention(config)\n",
        "        self.ff_layer = FeedForward(config)\n",
        "        self.sa_norm = RMSNorm(config.embed_size)\n",
        "        self.ff_norm = RMSNorm(config.embed_size)\n",
        "\n",
        "    def forward(self, x, kv_cache=None):\n",
        "        a, kv_cache = self.sa_heads(self.sa_norm(x), kv_cache)\n",
        "        h = x + a\n",
        "        o = h + self.ff_layer(self.ff_norm(h))\n",
        "        return o, kv_cache\n",
        "\n",
        "# ---- NEW: Custom Autograd Function for MEMORY-EFFICIENT TRAINING ----\n",
        "\n",
        "class SequentialHeadsCustomBackward(torch.autograd.Function):\n",
        "    @staticmethod\n",
        "    def forward(ctx, trunk_output, lm_head, *prediction_heads):\n",
        "        # Save modules and non-tensor arguments for the backward pass.\n",
        "        ctx.prediction_heads = prediction_heads\n",
        "        ctx.lm_head = lm_head\n",
        "        # Save tensors needed for the backward pass.\n",
        "        ctx.save_for_backward(trunk_output)\n",
        "\n",
        "        # Standard sequential forward pass\n",
        "        latents = []\n",
        "        for head in prediction_heads:\n",
        "            latent, _ = head(trunk_output, kv_cache=None)\n",
        "            latents.append(latent)\n",
        "        \n",
        "        latents_stacked = torch.stack(latents, dim=-2)\n",
        "        all_logits = lm_head(latents_stacked)\n",
        "        return all_logits\n",
        "\n",
        "    @staticmethod\n",
        "    def backward(ctx, grad_output):\n",
        "        trunk_output, = ctx.saved_tensors\n",
        "        prediction_heads = ctx.prediction_heads\n",
        "        lm_head = ctx.lm_head\n",
        "        \n",
        "        d = trunk_output.detach().requires_grad_(True)\n",
        "        grad_output_per_head = grad_output.unbind(dim=2)\n",
        "\n",
        "        for i, head in enumerate(prediction_heads):\n",
        "            with torch.enable_grad():\n",
        "                head_latent, _ = head(d)\n",
        "                head_logits = lm_head(head_latent)\n",
        "            \n",
        "            head_logits.backward(gradient=grad_output_per_head[i])\n",
        "            \n",
        "        num_nones = 1 + len(prediction_heads) # For lm_head + *prediction_heads\n",
        "        return (d.grad,) + (None,) * num_nones\n",
        "\n",
        "# ---- REFACTORED MTPTransformerLM with custom backward logic ----\n",
        "\n",
        "class MTPTransformerLM(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "        self.token_embedding_table = nn.Embedding(config.vocab_size, config.embed_size)\n",
        "        self.lm_head = nn.Linear(config.embed_size, config.vocab_size)\n",
        "        \n",
        "        # Clean separation between trunk and heads\n",
        "        num_trunk_blocks = config.layer_num - config.n_future_tokens\n",
        "        self.trunk_blocks = nn.ModuleList([Block(config) for _ in range(num_trunk_blocks)])\n",
        "        self.prediction_heads = nn.ModuleList([Block(config) for _ in range(config.n_future_tokens)])\n",
        "\n",
        "    def forward(self, idx, targets=None, kv_cache=None, return_all_heads=False, use_custom_backward=True):\n",
        "        B, T = idx.shape\n",
        "        tok_embd = self.token_embedding_table(idx)\n",
        "        x = tok_embd\n",
        "        \n",
        "        # 1. Shared Trunk\n",
        "        for i, block in enumerate(self.trunk_blocks):\n",
        "            x, _ = block(x) # Simplified kv_cache for clarity\n",
        "        trunk = x\n",
        "\n",
        "        # 2. Prediction Heads\n",
        "        n_heads_to_use = self.config.n_future_tokens if return_all_heads else 1\n",
        "        \n",
        "        if self.training and return_all_heads and use_custom_backward:\n",
        "            # Pass only the heads we intend to use to the custom function\n",
        "            active_heads = self.prediction_heads[:n_heads_to_use]\n",
        "            all_logits = SequentialHeadsCustomBackward.apply(trunk, self.lm_head, *active_heads)\n",
        "        else:\n",
        "            latents = []\n",
        "            for i, block in enumerate(self.prediction_heads[:n_heads_to_use]):\n",
        "                x_head, _ = block(trunk)\n",
        "                latents.append(x_head)\n",
        "            x = torch.stack(latents, dim=-2)\n",
        "            all_logits = self.lm_head(x)\n",
        "        \n",
        "        # 3. Loss Calculation\n",
        "        loss = None\n",
        "        if targets is not None:\n",
        "            B, T, H, V = all_logits.shape\n",
        "            logits_flat = all_logits.view(B * T * H, V)\n",
        "            targets_flat = targets.reshape(-1)\n",
        "            loss = F.cross_entropy(logits_flat, targets_flat)\n",
        "\n",
        "        logits = all_logits if return_all_heads else all_logits.squeeze(2)\n",
        "        return logits, loss\n",
        "    \n",
        "    def generate(self, idx, max_new_tokens, temperature=1, use_cache=True):\n",
        "        for _ in range(max_new_tokens):\n",
        "            idx_cond = idx if idx.size(1) <= self.config.seq_len else idx[:, -self.config.seq_len:]\n",
        "            # Note: generate calls with return_all_heads=False, so it doesn't use the custom BWD path\n",
        "            logits, _ = self(idx_cond, return_all_heads=False)\n",
        "            logits = logits[:, -1, :] / temperature if temperature > 0 else logits[:, -1, :]\n",
        "            \n",
        "            probs = F.softmax(logits, dim=-1)\n",
        "            idx_next = torch.multinomial(probs, num_samples=1) if temperature > 0 else torch.argmax(probs, dim=-1, keepdim=True)\n",
        "            idx = torch.cat((idx, idx_next), dim=1)\n",
        "        return idx\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "4775511"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Test forward pass\n",
        "n_future_tokens = 3\n",
        "config = MTPTransformerConfig(\n",
        "    vocab_size=vocab_size,\n",
        "    seq_len=seq_len,\n",
        "    embed_size=256,\n",
        "    head_num=4,\n",
        "    layer_num=6,\n",
        "    n_future_tokens=n_future_tokens\n",
        ")\n",
        "m = MTPTransformerLM(config)\n",
        "m.to(device)\n",
        "xb, yb = get_batch('train', 5, 1, n_future_tokens)\n",
        "logits, loss = m(xb, yb, return_all_heads=True)\n",
        "total_params = sum(p.numel() for p in m.parameters() if p.requires_grad)\n",
        "total_params"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 400x300 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "58c5292dc7684b939949d9157311c0cd",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/5000 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# training!\n",
        "model = MTPTransformerLM(config)\n",
        "model = torch.compile(model)\n",
        "model.to(device)\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=2e-3)\n",
        "train(model, optimizer, seq_len, batch_size, total_steps)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save\n",
        "torch.save(model.state_dict(), 'models/SavingMemoryMTPTransformerLM.pt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "OptimizedModule(\n",
              "  (_orig_mod): MTPTransformerLM(\n",
              "    (token_embedding_table): Embedding(87, 256)\n",
              "    (lm_head): Linear(in_features=256, out_features=87, bias=True)\n",
              "    (blocks): ModuleList(\n",
              "      (0-4): 5 x Block(\n",
              "        (sa_heads): MultiHeadAttention(\n",
              "          (key): Linear(in_features=256, out_features=256, bias=False)\n",
              "          (query): Linear(in_features=256, out_features=256, bias=False)\n",
              "          (value): Linear(in_features=256, out_features=256, bias=False)\n",
              "          (o): Linear(in_features=256, out_features=256, bias=True)\n",
              "        )\n",
              "        (ff_layer): FeedForward(\n",
              "          (lin_1): Linear(in_features=256, out_features=1024, bias=True)\n",
              "          (lin_2): Linear(in_features=1024, out_features=256, bias=True)\n",
              "          (relu): ReLU()\n",
              "        )\n",
              "        (sa_norm): RMSNorm()\n",
              "        (ff_norm): RMSNorm()\n",
              "      )\n",
              "    )\n",
              "    (extra_heads): ModuleList(\n",
              "      (0): Block(\n",
              "        (sa_heads): MultiHeadAttention(\n",
              "          (key): Linear(in_features=256, out_features=256, bias=False)\n",
              "          (query): Linear(in_features=256, out_features=256, bias=False)\n",
              "          (value): Linear(in_features=256, out_features=256, bias=False)\n",
              "          (o): Linear(in_features=256, out_features=256, bias=True)\n",
              "        )\n",
              "        (ff_layer): FeedForward(\n",
              "          (lin_1): Linear(in_features=256, out_features=1024, bias=True)\n",
              "          (lin_2): Linear(in_features=1024, out_features=256, bias=True)\n",
              "          (relu): ReLU()\n",
              "        )\n",
              "        (sa_norm): RMSNorm()\n",
              "        (ff_norm): RMSNorm()\n",
              "      )\n",
              "    )\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Load model\n",
        "model = MTPTransformerLM(config)\n",
        "model = torch.compile(model)\n",
        "model.load_state_dict(torch.load('models/SavingMemoryMTPTransformerLM.pt'))\n",
        "model.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "7032311b2b894316aa7dd649b6705882",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/1000 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "perplexity: 3.348928928375244 loss: 1.208640625\n"
          ]
        }
      ],
      "source": [
        "# calculate perplexity\n",
        "ppl, loss = perplexity(model, seq_len, seq_len)\n",
        "print(\"perplexity:\", ppl, \"loss:\", loss)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[53, 72, 78,  1, 80, 66, 69, 69,  1, 71, 62, 79, 62, 75]])\n",
            "You will never be sold by your friends and worked out.\n",
            "\n",
            "Why are they here?\n",
            "\n",
            "I won't let you try to survive the new land.\n",
            "\n",
            "You don't need to be the one who was a bit stronger than that?\n",
            "\n",
            "I won't be able to do that.\n",
            "\n",
            "Why don't you come to see that to help you?\n",
            "\n",
            "I think that I thought I was able to be that bad at the captain of the ground.\n",
            "\n",
            "I can't treat you to the control of the Secrets will be the worst position of the first place.\n",
            "\n",
            "I wonder if it can't be a total power to see the Secret of the Sea God.\n",
            "\n",
            "But I don't think that I want to see it on a day.\n",
            "\n",
            "The Secret said that we want to show the problem here.\n",
            "\n",
            "He said that he looked in a relationship with him at the first time on the same time.\n",
            "\n",
            "Allow us to have to do is think of it, control of the main station.\n",
            "\n",
            "The Secret is a mistake.\n",
            "\n",
            "The Sea God of Tenkaidou is a man or something else in the stars of the dead face.\n",
            "\n",
            "It's a perfect rest for the Soul Reaper\n",
            "\n",
            "of the Soul Reapers are already served to the very eyes and submitted to be a bit longer,\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "model.eval()\n",
        "idx = encode(\"You will never\")\n",
        "print(torch.tensor([idx]))\n",
        "print(decode(model.generate(idx=torch.tensor([idx], dtype=torch.long).to(device), max_new_tokens=1000, temperature=0.5, use_cache=True)[0].tolist()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Investigate Saving Memory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "4775511"
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Test forward pass\n",
        "n_future_tokens = 1\n",
        "config = MTPTransformerConfig(\n",
        "    vocab_size=vocab_size,\n",
        "    seq_len=seq_len,\n",
        "    embed_size=256,\n",
        "    head_num=4,\n",
        "    layer_num=6,\n",
        "    n_future_tokens=n_future_tokens\n",
        ")\n",
        "m = MTPTransformerLM(config)\n",
        "m.to(device)\n",
        "xb, yb = get_batch('train', 5, 1, n_future_tokens)\n",
        "m.zero_grad()\n",
        "m.train()\n",
        "logits, loss = m(xb, yb, return_all_heads=True)\n",
        "total_params = sum(p.numel() for p in m.parameters() if p.requires_grad)\n",
        "total_params"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from typing import List\n",
        "import copy\n",
        "\n",
        "# --- Original Code Base (with minor fixes for standalone execution) ---\n",
        "\n",
        "class MTPTransformerConfig:\n",
        "    def __init__(self, vocab_size, seq_len, embed_size, head_num, layer_num, n_future_tokens):\n",
        "        self.vocab_size = vocab_size\n",
        "        self.seq_len = seq_len\n",
        "        self.embed_size = embed_size\n",
        "        self.head_num = head_num\n",
        "        self.layer_num = layer_num\n",
        "        self.n_future_tokens = n_future_tokens\n",
        "\n",
        "def precompute_freqs_cis(dim: int, end: int, theta: float = 10000.0):\n",
        "    freqs = 1.0 / (theta ** (torch.arange(0, dim, 2)[: (dim // 2)].float() / dim))\n",
        "    t = torch.arange(end, device=freqs.device, dtype=torch.float32)\n",
        "    freqs = torch.outer(t, freqs)\n",
        "    freqs_cis = torch.polar(torch.ones_like(freqs), freqs)\n",
        "    return freqs_cis.to(device)\n",
        "\n",
        "@torch.compiler.disable\n",
        "def apply_rotary_emb(xq: torch.Tensor, xk: torch.Tensor, freqs_cis: torch.Tensor):\n",
        "    xq_ = torch.view_as_complex(xq.float().reshape(*xq.shape[:-1], -1, 2))\n",
        "    xk_ = torch.view_as_complex(xk.float().reshape(*xk.shape[:-1], -1, 2))\n",
        "    \n",
        "    # Reshape freqs_cis for broadcasting\n",
        "    shape = [1] * (xq_.ndim - 2) + list(freqs_cis.shape)\n",
        "    freqs_cis = freqs_cis.view(*shape)\n",
        "\n",
        "    xq_out = torch.view_as_real(xq_ * freqs_cis).flatten(3)\n",
        "    xk_out = torch.view_as_real(xk_ * freqs_cis).flatten(3)\n",
        "    return xq_out.type_as(xq), xk_out.type_as(xk)\n",
        "\n",
        "class RMSNorm(torch.nn.Module):\n",
        "    def __init__(self, dim, eps=1e-6):\n",
        "        super().__init__()\n",
        "        self.eps = eps\n",
        "        self.weight = nn.Parameter(torch.ones(dim))\n",
        "\n",
        "    def _norm(self, x):\n",
        "        return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)\n",
        "\n",
        "    def forward(self, x):\n",
        "        output = self._norm(x.float()).type_as(x)\n",
        "        return output * self.weight\n",
        "\n",
        "class FeedForward(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.lin_1 = nn.Linear(config.embed_size, config.embed_size*4)\n",
        "        self.lin_2 = nn.Linear(config.embed_size*4, config.embed_size)\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.lin_1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.lin_2(x)\n",
        "        return x\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.seq_len = config.seq_len\n",
        "        self.head_num = config.head_num\n",
        "        self.head_size = config.embed_size // config.head_num\n",
        "        self.key = nn.Linear(config.embed_size, config.embed_size, bias=False)\n",
        "        self.query = nn.Linear(config.embed_size, config.embed_size, bias=False)\n",
        "        self.value = nn.Linear(config.embed_size, config.embed_size, bias=False)\n",
        "        self.o = nn.Linear(config.embed_size, config.embed_size)\n",
        "        \n",
        "        def causal(b, h, q_idx, kv_idx):\n",
        "            return q_idx >= kv_idx\n",
        "        self.causal_mask = create_block_mask(causal, B=None, H=None, Q_LEN=config.seq_len, KV_LEN=config.seq_len)\n",
        "        self.freqs_cis = precompute_freqs_cis(config.embed_size//config.head_num, config.seq_len)\n",
        "        self.register_buffer('tril', torch.tril(torch.ones(config.seq_len, config.seq_len)))\n",
        "\n",
        "    def forward(self, x, kv_cache=None):\n",
        "        B, T, C = x.shape\n",
        "        q = self.query(x)\n",
        "        k = self.key(x)\n",
        "        v = self.value(x)\n",
        "\n",
        "        q = q.view(B, T, self.head_num, self.head_size).transpose(1, 2)\n",
        "        k = k.view(B, T, self.head_num, self.head_size).transpose(1, 2)\n",
        "        v = v.view(B, T, self.head_num, self.head_size).transpose(1, 2)\n",
        "\n",
        "        if kv_cache is not None:\n",
        "            k_past, v_past = kv_cache\n",
        "            if k_past is not None:\n",
        "                k = torch.cat((k_past, k), dim=2)\n",
        "                v = torch.cat((v_past, v), dim=2)\n",
        "            if k.shape[-2] > self.seq_len:\n",
        "                k = k[:, :, -self.seq_len:]\n",
        "                v = v[:, :, -self.seq_len:]\n",
        "            kv_cache = (k, v)\n",
        "        \n",
        "        T_k = k.shape[-2]\n",
        "        q, k = apply_rotary_emb(q, k, self.freqs_cis[:T_k])\n",
        "        \n",
        "        if T == self.seq_len:\n",
        "             out = flex_attention(q, k, v, block_mask=self.causal_mask)\n",
        "        else:\n",
        "            wei = q @ k.transpose(-2,-1) * (self.head_size**-0.5)\n",
        "            wei = wei.masked_fill(self.tril[:T, :T_k] == 0, float('-inf'))\n",
        "            wei = F.softmax(wei, dim=-1)\n",
        "            out = wei @ v\n",
        "\n",
        "        out = out.transpose(1, 2).contiguous().view(B, T, C)\n",
        "        out = self.o(out)\n",
        "        return out, kv_cache\n",
        "\n",
        "class Block(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.sa_heads = MultiHeadAttention(config)\n",
        "        self.ff_layer = FeedForward(config)\n",
        "        self.sa_norm = RMSNorm(config.embed_size)\n",
        "        self.ff_norm = RMSNorm(config.embed_size)\n",
        "\n",
        "    def forward(self, x, kv_cache=None):\n",
        "        a, kv_cache = self.sa_heads(self.sa_norm(x), kv_cache)\n",
        "        h = x + a\n",
        "        o = h + self.ff_layer(self.ff_norm(h))\n",
        "        return o, kv_cache\n",
        "\n",
        "# ---- NEW: Custom Autograd Function for MEMORY-EFFICIENT TRAINING ----\n",
        "\n",
        "class SequentialHeadsCustomBackward(torch.autograd.Function):\n",
        "    @staticmethod\n",
        "    def forward(ctx, trunk_output, lm_head, *prediction_heads):\n",
        "        \"\"\"\n",
        "        Runs the standard sequential forward pass, but sets up for a custom backward pass.\n",
        "        \"\"\"\n",
        "        # Save modules and non-tensor arguments for the backward pass.\n",
        "        ctx.prediction_heads = prediction_heads\n",
        "        ctx.lm_head = lm_head\n",
        "        # Save tensors needed for the backward pass.\n",
        "        ctx.save_for_backward(trunk_output)\n",
        "\n",
        "        # Standard sequential forward pass\n",
        "        latents = []\n",
        "        # KV Caching is disabled within this function for simplicity.\n",
        "        for head in prediction_heads:\n",
        "            latent, _ = head(trunk_output, kv_cache=None)\n",
        "            latents.append(latent)\n",
        "        \n",
        "        latents_stacked = torch.stack(latents, dim=-2)\n",
        "        all_logits = lm_head(latents_stacked)\n",
        "        return all_logits\n",
        "\n",
        "    @staticmethod\n",
        "    def backward(ctx, grad_output):\n",
        "        \"\"\"\n",
        "        Implements the memory-efficient sequential backward pass from the paper.\n",
        "        \"\"\"\n",
        "        trunk_output, = ctx.saved_tensors\n",
        "        prediction_heads = ctx.prediction_heads\n",
        "        lm_head = ctx.lm_head\n",
        "        \n",
        "        # Detach trunk output to manually control gradient flow\n",
        "        d = trunk_output.detach().requires_grad_(True)\n",
        "        # Split the incoming gradient for each head's output\n",
        "        grad_output_per_head = grad_output.unbind(dim=2)\n",
        "\n",
        "        for i, head in enumerate(prediction_heads):\n",
        "            # We need to re-run the forward pass for each head on the detached\n",
        "            # trunk output to build a temporary computational graph for backprop.\n",
        "            with torch.enable_grad():\n",
        "                head_latent, _ = head(d)\n",
        "                head_logits = lm_head(head_latent)\n",
        "            \n",
        "            # Backpropagate through this single head's graph. This populates\n",
        "            # gradients for this head's parameters and accumulates them in `d.grad`.\n",
        "            head_logits.backward(gradient=grad_output_per_head[i])\n",
        "            \n",
        "        # The gradients for lm_head and each prediction_head have been accumulated.\n",
        "        # Now, d.grad contains the accumulated gradient for the trunk.\n",
        "        # We return it as the gradient for the original trunk_output.\n",
        "        # Gradients for non-tensor inputs (modules) should be None.\n",
        "        num_nones = 1 + len(prediction_heads) # For lm_head + *prediction_heads\n",
        "        return (d.grad,) + (None,) * num_nones\n",
        "\n",
        "# ---- UPDATED MTPTransformerLM with custom backward logic ----\n",
        "class MTPTransformerLM(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "        self.token_embedding_table = nn.Embedding(config.vocab_size, config.embed_size)\n",
        "        self.lm_head = nn.Linear(config.embed_size, config.vocab_size)\n",
        "        \n",
        "        # The main body of the model\n",
        "        self.blocks = nn.ModuleList([Block(config) for _ in range(config.layer_num - config.n_future_tokens)])\n",
        "        # The prediction heads that branch off\n",
        "        self.extra_heads = nn.ModuleList([Block(config) for _ in range(config.n_future_tokens)])\n",
        "\n",
        "    def forward(self, idx, targets=None, kv_cache=None, return_all_heads=False, use_custom_backward=True):\n",
        "        B, T = idx.shape\n",
        "        tok_embd = self.token_embedding_table(idx)\n",
        "        x = tok_embd\n",
        "        \n",
        "        # 1. Shared Trunk\n",
        "        for i, block in enumerate(self.blocks):\n",
        "            x, cache = block(x, None if kv_cache is None else kv_cache[i])\n",
        "            # Update cache if provided\n",
        "        trunk = x\n",
        "\n",
        "        # 2. Prediction Heads\n",
        "        n_heads_to_use = self.config.n_future_tokens if return_all_heads else 1\n",
        "        prediction_heads = [self.blocks[-1]] + list(self.extra_heads) if len(self.blocks) > 0 else list(self.extra_heads)\n",
        "\n",
        "        # Use memory-efficient custom backward pass during training for all heads\n",
        "        if self.training and return_all_heads and use_custom_backward:\n",
        "            # We must pass the heads as *args for the autograd function\n",
        "            all_logits = SequentialHeadsCustomBackward.apply(trunk, self.lm_head, *prediction_heads[:n_heads_to_use])\n",
        "        # Use standard forward pass for inference or single-head cases\n",
        "        else:\n",
        "            latents = []\n",
        "            for i, block in enumerate(prediction_heads[:n_heads_to_use]):\n",
        "                # Note: kv_cache logic for heads is simplified here.\n",
        "                x_head, _ = block(trunk)\n",
        "                latents.append(x_head)\n",
        "            x = torch.stack(latents, dim=-2)\n",
        "            all_logits = self.lm_head(x)\n",
        "        \n",
        "        # 3. Loss Calculation\n",
        "        loss = None\n",
        "        if targets is not None:\n",
        "            B, T, V = all_logits.shape[0], all_logits.shape[1], all_logits.shape[-1]\n",
        "            logits_flat = all_logits.view(B * T * n_heads_to_use, V)\n",
        "            targets_flat = targets.reshape(-1)\n",
        "            loss = F.cross_entropy(logits_flat, targets_flat)\n",
        "\n",
        "        logits = all_logits if return_all_heads else all_logits[:, :, 0, :]\n",
        "        return logits, loss\n",
        "    \n",
        "    def generate(self, idx, max_new_tokens, temperature=1, use_cache=True):\n",
        "        # This function remains largely the same, as it calls forward with return_all_heads=False\n",
        "        for _ in range(max_new_tokens):\n",
        "            idx_cond = idx if idx.size(1) <= self.config.seq_len else idx[:, -self.config.seq_len:]\n",
        "            logits, _ = self(idx_cond, return_all_heads=False)\n",
        "            logits = logits[:, -1, :] / temperature if temperature > 0 else logits[:, -1, :]\n",
        "            \n",
        "            probs = F.softmax(logits, dim=-1)\n",
        "            idx_next = torch.multinomial(probs, num_samples=1) if temperature > 0 else torch.argmax(probs, dim=-1, keepdim=True)\n",
        "            idx = torch.cat((idx, idx_next), dim=1)\n",
        "        return idx\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--- Running Gradient Comparison Test ---\n",
            "\n",
            "Losses are close: True\n",
            "Trunk gradients are the same: False\n",
            "Head gradients are the same: False\n",
            "\n",
            "❌ Gradient check failed. The custom backward implementation is incorrect.\n"
          ]
        }
      ],
      "source": [
        "config = MTPTransformerConfig(\n",
        "    vocab_size=1000, seq_len=128, embed_size=256,\n",
        "    head_num=8, layer_num=12, n_future_tokens=4\n",
        ")\n",
        "\n",
        "# --- GRADIENT COMPARISON TEST ---\n",
        "print(\"--- Running Gradient Comparison Test ---\")\n",
        "\n",
        "# Create two identical models\n",
        "model_ref = MTPTransformerLM(config).to(device)\n",
        "model_custom = copy.deepcopy(model_ref)\n",
        "\n",
        "# Dummy data\n",
        "dummy_input = torch.randint(0, config.vocab_size, (2, 64), device=device)\n",
        "dummy_targets = torch.randint(0, config.vocab_size, (2, 64, config.n_future_tokens), device=device)\n",
        "\n",
        "# --- 1. Get gradients from standard backward pass ---\n",
        "model_ref.train()\n",
        "model_ref.zero_grad()\n",
        "_, loss_ref = model_ref(dummy_input, targets=dummy_targets, return_all_heads=True, use_custom_backward=False)\n",
        "loss_ref.backward()\n",
        "\n",
        "# Store gradients from a trunk parameter and a head parameter\n",
        "trunk_grad_ref = model_ref.blocks[0].ff_layer.lin_1.weight.grad.clone()\n",
        "head_grad_ref = model_ref.extra_heads[0].ff_layer.lin_1.weight.grad.clone()\n",
        "\n",
        "# --- 2. Get gradients from custom backward pass ---\n",
        "model_custom.train()\n",
        "model_custom.zero_grad()\n",
        "_, loss_custom = model_custom(dummy_input, targets=dummy_targets, return_all_heads=True, use_custom_backward=True)\n",
        "loss_custom.backward()\n",
        "\n",
        "# Get gradients from the same parameters\n",
        "trunk_grad_custom = model_custom.blocks[0].ff_layer.lin_1.weight.grad\n",
        "head_grad_custom = model_custom.extra_heads[0].ff_layer.lin_1.weight.grad\n",
        "\n",
        "# --- 3. Compare the gradients ---\n",
        "print(f\"\\nLosses are close: {torch.allclose(loss_ref, loss_custom)}\")\n",
        "trunk_grads_are_close = torch.allclose(trunk_grad_ref, trunk_grad_custom)\n",
        "head_grads_are_close = torch.allclose(head_grad_ref, head_grad_custom)\n",
        "\n",
        "print(f\"Trunk gradients are the same: {trunk_grads_are_close}\")\n",
        "print(f\"Head gradients are the same: {head_grads_are_close}\")\n",
        "\n",
        "if trunk_grads_are_close and head_grads_are_close:\n",
        "    print(\"\\n✅ Gradient check passed. The custom backward implementation is correct.\")\n",
        "else:\n",
        "    print(\"\\n❌ Gradient check failed. The custom backward implementation is incorrect.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--- Running Gradient Comparison Test ---\n",
            "\n",
            "Losses are close: True\n",
            "Trunk gradients are the same: True\n",
            "Head gradients are the same: True\n",
            "\n",
            "✅ Gradient check passed. The custom backward implementation is correct.\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from typing import List\n",
        "import copy\n",
        "\n",
        "class MTPTransformerConfig:\n",
        "    def __init__(self, vocab_size, seq_len, embed_size, head_num, layer_num, n_future_tokens):\n",
        "        self.vocab_size = vocab_size\n",
        "        self.seq_len = seq_len\n",
        "        self.embed_size = embed_size\n",
        "        self.head_num = head_num\n",
        "        self.layer_num = layer_num\n",
        "        self.n_future_tokens = n_future_tokens\n",
        "\n",
        "def precompute_freqs_cis(dim: int, end: int, theta: float = 10000.0):\n",
        "    freqs = 1.0 / (theta ** (torch.arange(0, dim, 2)[: (dim // 2)].float() / dim))\n",
        "    t = torch.arange(end, device=freqs.device, dtype=torch.float32)\n",
        "    freqs = torch.outer(t, freqs)\n",
        "    freqs_cis = torch.polar(torch.ones_like(freqs), freqs)\n",
        "    return freqs_cis.to(device)\n",
        "\n",
        "@torch.compiler.disable\n",
        "def apply_rotary_emb(xq: torch.Tensor, xk: torch.Tensor, freqs_cis: torch.Tensor):\n",
        "    xq_ = torch.view_as_complex(xq.float().reshape(*xq.shape[:-1], -1, 2))\n",
        "    xk_ = torch.view_as_complex(xk.float().reshape(*xk.shape[:-1], -1, 2))\n",
        "    \n",
        "    # Reshape freqs_cis for broadcasting\n",
        "    shape = [1] * (xq_.ndim - 2) + list(freqs_cis.shape)\n",
        "    freqs_cis = freqs_cis.view(*shape)\n",
        "\n",
        "    xq_out = torch.view_as_real(xq_ * freqs_cis).flatten(3)\n",
        "    xk_out = torch.view_as_real(xk_ * freqs_cis).flatten(3)\n",
        "    return xq_out.type_as(xq), xk_out.type_as(xk)\n",
        "\n",
        "class RMSNorm(torch.nn.Module):\n",
        "    def __init__(self, dim, eps=1e-6):\n",
        "        super().__init__()\n",
        "        self.eps = eps\n",
        "        self.weight = nn.Parameter(torch.ones(dim))\n",
        "\n",
        "    def _norm(self, x):\n",
        "        return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)\n",
        "\n",
        "    def forward(self, x):\n",
        "        output = self._norm(x.float()).type_as(x)\n",
        "        return output * self.weight\n",
        "\n",
        "class FeedForward(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.lin_1 = nn.Linear(config.embed_size, config.embed_size*4)\n",
        "        self.lin_2 = nn.Linear(config.embed_size*4, config.embed_size)\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.lin_1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.lin_2(x)\n",
        "        return x\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.seq_len = config.seq_len\n",
        "        self.head_num = config.head_num\n",
        "        self.head_size = config.embed_size // config.head_num\n",
        "        self.key = nn.Linear(config.embed_size, config.embed_size, bias=False)\n",
        "        self.query = nn.Linear(config.embed_size, config.embed_size, bias=False)\n",
        "        self.value = nn.Linear(config.embed_size, config.embed_size, bias=False)\n",
        "        self.o = nn.Linear(config.embed_size, config.embed_size)\n",
        "        \n",
        "        def causal(b, h, q_idx, kv_idx):\n",
        "            return q_idx >= kv_idx\n",
        "        self.causal_mask = create_block_mask(causal, B=None, H=None, Q_LEN=config.seq_len, KV_LEN=config.seq_len)\n",
        "        self.freqs_cis = precompute_freqs_cis(config.embed_size//config.head_num, config.seq_len)\n",
        "        self.register_buffer('tril', torch.tril(torch.ones(config.seq_len, config.seq_len)))\n",
        "\n",
        "    def forward(self, x, kv_cache=None):\n",
        "        B, T, C = x.shape\n",
        "        q = self.query(x)\n",
        "        k = self.key(x)\n",
        "        v = self.value(x)\n",
        "\n",
        "        q = q.view(B, T, self.head_num, self.head_size).transpose(1, 2)\n",
        "        k = k.view(B, T, self.head_num, self.head_size).transpose(1, 2)\n",
        "        v = v.view(B, T, self.head_num, self.head_size).transpose(1, 2)\n",
        "\n",
        "        if kv_cache is not None:\n",
        "            k_past, v_past = kv_cache\n",
        "            if k_past is not None:\n",
        "                k = torch.cat((k_past, k), dim=2)\n",
        "                v = torch.cat((v_past, v), dim=2)\n",
        "            if k.shape[-2] > self.seq_len:\n",
        "                k = k[:, :, -self.seq_len:]\n",
        "                v = v[:, :, -self.seq_len:]\n",
        "            kv_cache = (k, v)\n",
        "        \n",
        "        T_k = k.shape[-2]\n",
        "        q, k = apply_rotary_emb(q, k, self.freqs_cis[:T_k])\n",
        "        \n",
        "        if T == self.seq_len and self.causal_mask is not None:\n",
        "             out = flex_attention(q, k, v, block_mask=self.causal_mask)\n",
        "        else:\n",
        "            wei = q @ k.transpose(-2,-1) * (self.head_size**-0.5)\n",
        "            wei = wei.masked_fill(self.tril[:T, :T_k] == 0, float('-inf'))\n",
        "            wei = F.softmax(wei, dim=-1)\n",
        "            out = wei @ v\n",
        "\n",
        "        out = out.transpose(1, 2).contiguous().view(B, T, C)\n",
        "        out = self.o(out)\n",
        "        return out, kv_cache\n",
        "\n",
        "class Block(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.sa_heads = MultiHeadAttention(config)\n",
        "        self.ff_layer = FeedForward(config)\n",
        "        self.sa_norm = RMSNorm(config.embed_size)\n",
        "        self.ff_norm = RMSNorm(config.embed_size)\n",
        "\n",
        "    def forward(self, x, kv_cache=None):\n",
        "        a, kv_cache = self.sa_heads(self.sa_norm(x), kv_cache)\n",
        "        h = x + a\n",
        "        o = h + self.ff_layer(self.ff_norm(h))\n",
        "        return o, kv_cache\n",
        "\n",
        "# ---- NEW: Custom Autograd Function for MEMORY-EFFICIENT TRAINING ----\n",
        "\n",
        "class SequentialHeadsCustomBackward(torch.autograd.Function):\n",
        "    @staticmethod\n",
        "    def forward(ctx, trunk_output, lm_head, *prediction_heads):\n",
        "        # Save modules and non-tensor arguments for the backward pass.\n",
        "        ctx.prediction_heads = prediction_heads\n",
        "        ctx.lm_head = lm_head\n",
        "        # Save tensors needed for the backward pass.\n",
        "        ctx.save_for_backward(trunk_output)\n",
        "\n",
        "        # Standard sequential forward pass\n",
        "        latents = []\n",
        "        for head in prediction_heads:\n",
        "            latent, _ = head(trunk_output, kv_cache=None)\n",
        "            latents.append(latent)\n",
        "        \n",
        "        latents_stacked = torch.stack(latents, dim=-2)\n",
        "        all_logits = lm_head(latents_stacked)\n",
        "        return all_logits\n",
        "\n",
        "    @staticmethod\n",
        "    def backward(ctx, grad_output):\n",
        "        trunk_output, = ctx.saved_tensors\n",
        "        prediction_heads = ctx.prediction_heads\n",
        "        lm_head = ctx.lm_head\n",
        "        \n",
        "        d = trunk_output.detach().requires_grad_(True)\n",
        "        grad_output_per_head = grad_output.unbind(dim=2)\n",
        "\n",
        "        for i, head in enumerate(prediction_heads):\n",
        "            with torch.enable_grad():\n",
        "                head_latent, _ = head(d)\n",
        "                head_logits = lm_head(head_latent)\n",
        "            \n",
        "            head_logits.backward(gradient=grad_output_per_head[i])\n",
        "            \n",
        "        num_nones = 1 + len(prediction_heads) # For lm_head + *prediction_heads\n",
        "        return (d.grad,) + (None,) * num_nones\n",
        "\n",
        "# ---- REFACTORED MTPTransformerLM with custom backward logic ----\n",
        "\n",
        "class MTPTransformerLM(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "        self.token_embedding_table = nn.Embedding(config.vocab_size, config.embed_size)\n",
        "        self.lm_head = nn.Linear(config.embed_size, config.vocab_size)\n",
        "        \n",
        "        # Clean separation between trunk and heads\n",
        "        num_trunk_blocks = config.layer_num - config.n_future_tokens\n",
        "        self.trunk_blocks = nn.ModuleList([Block(config) for _ in range(num_trunk_blocks)])\n",
        "        self.prediction_heads = nn.ModuleList([Block(config) for _ in range(config.n_future_tokens)])\n",
        "\n",
        "    def forward(self, idx, targets=None, kv_cache=None, return_all_heads=False, use_custom_backward=True):\n",
        "        B, T = idx.shape\n",
        "        tok_embd = self.token_embedding_table(idx)\n",
        "        x = tok_embd\n",
        "        \n",
        "        # 1. Shared Trunk\n",
        "        for i, block in enumerate(self.trunk_blocks):\n",
        "            x, _ = block(x) # Simplified kv_cache for clarity\n",
        "        trunk = x\n",
        "\n",
        "        # 2. Prediction Heads\n",
        "        n_heads_to_use = self.config.n_future_tokens if return_all_heads else 1\n",
        "        \n",
        "        if self.training and return_all_heads and use_custom_backward:\n",
        "            # Pass only the heads we intend to use to the custom function\n",
        "            active_heads = self.prediction_heads[:n_heads_to_use]\n",
        "            all_logits = SequentialHeadsCustomBackward.apply(trunk, self.lm_head, *active_heads)\n",
        "        else:\n",
        "            latents = []\n",
        "            for i, block in enumerate(self.prediction_heads[:n_heads_to_use]):\n",
        "                x_head, _ = block(trunk)\n",
        "                latents.append(x_head)\n",
        "            x = torch.stack(latents, dim=-2)\n",
        "            all_logits = self.lm_head(x)\n",
        "        \n",
        "        # 3. Loss Calculation\n",
        "        loss = None\n",
        "        if targets is not None:\n",
        "            B, T, H, V = all_logits.shape\n",
        "            logits_flat = all_logits.view(B * T * H, V)\n",
        "            targets_flat = targets.reshape(-1)\n",
        "            loss = F.cross_entropy(logits_flat, targets_flat)\n",
        "\n",
        "        logits = all_logits if return_all_heads else all_logits.squeeze(2)\n",
        "        return logits, loss\n",
        "    \n",
        "    def generate(self, idx, max_new_tokens, temperature=1, use_cache=True):\n",
        "        for _ in range(max_new_tokens):\n",
        "            idx_cond = idx if idx.size(1) <= self.config.seq_len else idx[:, -self.config.seq_len:]\n",
        "            # Note: generate calls with return_all_heads=False, so it doesn't use the custom BWD path\n",
        "            logits, _ = self(idx_cond, return_all_heads=False)\n",
        "            logits = logits[:, -1, :] / temperature if temperature > 0 else logits[:, -1, :]\n",
        "            \n",
        "            probs = F.softmax(logits, dim=-1)\n",
        "            idx_next = torch.multinomial(probs, num_samples=1) if temperature > 0 else torch.argmax(probs, dim=-1, keepdim=True)\n",
        "            idx = torch.cat((idx, idx_next), dim=1)\n",
        "        return idx\n",
        "\n",
        "config = MTPTransformerConfig(\n",
        "    vocab_size=1000, seq_len=32, embed_size=128,\n",
        "    head_num=4, layer_num=6, n_future_tokens=3\n",
        ")\n",
        "\n",
        "# --- GRADIENT COMPARISON TEST ---\n",
        "print(\"--- Running Gradient Comparison Test ---\")\n",
        "\n",
        "# Create two identical models\n",
        "model_ref = MTPTransformerLM(config).to(device)\n",
        "model_custom = copy.deepcopy(model_ref)\n",
        "\n",
        "# Dummy data\n",
        "dummy_input = torch.randint(0, config.vocab_size, (2, 16), device=device)\n",
        "dummy_targets = torch.randint(0, config.vocab_size, (2, 16, config.n_future_tokens), device=device)\n",
        "\n",
        "# --- 1. Get gradients from standard backward pass ---\n",
        "model_ref.train()\n",
        "model_ref.zero_grad()\n",
        "_, loss_ref = model_ref(dummy_input, targets=dummy_targets, return_all_heads=True, use_custom_backward=False)\n",
        "loss_ref.backward()\n",
        "\n",
        "# Store gradients from a trunk parameter and a head parameter\n",
        "trunk_grad_ref = model_ref.trunk_blocks[0].ff_layer.lin_1.weight.grad.clone()\n",
        "head_grad_ref = model_ref.prediction_heads[0].ff_layer.lin_1.weight.grad.clone()\n",
        "\n",
        "# --- 2. Get gradients from custom backward pass ---\n",
        "model_custom.train()\n",
        "model_custom.zero_grad()\n",
        "_, loss_custom = model_custom(dummy_input, targets=dummy_targets, return_all_heads=True, use_custom_backward=True)\n",
        "loss_custom.backward()\n",
        "\n",
        "# Get gradients from the same parameters\n",
        "trunk_grad_custom = model_custom.trunk_blocks[0].ff_layer.lin_1.weight.grad\n",
        "head_grad_custom = model_custom.prediction_heads[0].ff_layer.lin_1.weight.grad\n",
        "\n",
        "# --- 3. Compare the gradients ---\n",
        "print(f\"\\nLosses are close: {torch.allclose(loss_ref, loss_custom)}\")\n",
        "trunk_grads_are_close = torch.allclose(trunk_grad_ref, trunk_grad_custom, rtol=1e-4, atol=1e-4)\n",
        "head_grads_are_close = torch.allclose(head_grad_ref, head_grad_custom, rtol=1e-4, atol=1e-4)\n",
        "\n",
        "print(f\"Trunk gradients are the same: {trunk_grads_are_close}\")\n",
        "print(f\"Head gradients are the same: {head_grads_are_close}\")\n",
        "\n",
        "if trunk_grads_are_close and head_grads_are_close:\n",
        "    print(\"\\n✅ Gradient check passed. The custom backward implementation is correct.\")\n",
        "else:\n",
        "    print(\"\\n❌ Gradient check failed. The custom backward implementation is incorrect.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor(0.29, device='cuda:0')"
            ]
          },
          "execution_count": 35,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "trunk_grad_ref.sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor(0.29, device='cuda:0')"
            ]
          },
          "execution_count": 36,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "trunk_grad_custom.sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.9"
    },
    "orig_nbformat": 4
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
