{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "sdtDsu1Y0EqL"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /home/coder/.netrc\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33merlandpg\u001b[0m to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wandb initialized successfully\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import math\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "from torch.nn.attention.flex_attention import flex_attention, create_block_mask\n",
    "import triton\n",
    "import triton.language as tl\n",
    "from tqdm.notebook import tqdm\n",
    "from dotenv import load_dotenv\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython import display\n",
    "import wandb\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "use_wandb = True # set to False to disable wandb logging\n",
    "try:\n",
    "    import wandb\n",
    "    wandb_api_key = os.getenv('WANDB_API_KEY')\n",
    "    if wandb_api_key:  \n",
    "        wandb.login(key=wandb_api_key)\n",
    "        use_wandb = use_wandb and True\n",
    "        print(\"wandb initialized successfully\")\n",
    "    else:\n",
    "        print(\"WANDB_API_KEY not found - wandb logging disabled\")\n",
    "except ImportError:\n",
    "    print(\"wandb not installed - wandb logging disabled\")\n",
    "    wandb = None\n",
    "\n",
    "torch.manual_seed(69)\n",
    "torch.set_printoptions(profile=\"short\", sci_mode=False, linewidth=100000)\n",
    "torch.set_float32_matmul_precision('high')\n",
    "# this script is configured to run on a RTX 3060 12GB GPU. you'll want to adjust the model sizes and batch sizes for other devices\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# device = torch.device('cpu')\n",
    "plt.rcParams['figure.figsize'] = [8, 6]\n",
    "plt.rcParams['figure.dpi'] = 50\n",
    "plt.rcParams['axes.grid'] = True\n",
    "plt.rcParams['xtick.minor.visible'] = True\n",
    "plt.rcParams['ytick.minor.visible'] = True\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ANr5dn7W0EqR",
    "outputId": "a00cbe8d-e1c6-454b-b552-4ffd17ce17e5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of dataset in characters:  39526018\n"
     ]
    }
   ],
   "source": [
    "# we use this 40mb file of concatenated anime subtitles as our dataset\n",
    "# just the right size for toy experiments like this I think\n",
    "with open('animesubs.txt', 'r', encoding='latin') as f:\n",
    "    text = f.read()\n",
    "print(\"length of dataset in characters: \", len(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pANiObIZ0EqU",
    "outputId": "98f70469-1c89-4341-89e8-c142a14331b1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Open your mind. Open your mind.\n",
      "\n",
      "Far beyond the deep blue Earth, you and I shall meet...\n",
      "\n",
      "AH! MY GODDESS\n",
      "\n",
      "A snow-white feather comes fluttering down, swaying gently in the air.\n",
      "\n",
      "Without holding back, I want to envelope you, my one and only love.\n",
      "\n",
      "I know I have the power to protect the one I love, right here in my hands.\n",
      "\n",
      "Open your mind. Just as I've always dreamed.\n",
      "\n",
      "Let the wind carry off your hopes, faraway.\n",
      "\n",
      "I have wings nobody can see. Look, you have them, too.\n",
      "\n",
      "They'll take us to where we ca\n"
     ]
    }
   ],
   "source": [
    "print(text[:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "WvM5h6_i3KsM"
   },
   "outputs": [],
   "source": [
    "# remove japanese characters\n",
    "text = ''.join(filter(lambda character:ord(character) < 0x3000, text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "U7SOcWJM0EqW",
    "outputId": "a82a4da8-a8ed-47bf-cfb5-2e4c59dee2cf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unique characters: 86 \n",
      " !'()*+,-./0123456789:;<=>?@ABCDEFGHIJKLMNOPQRSTUVWXYZ[]_abcdefghijklmnopqrstuvwxyz|\n"
     ]
    }
   ],
   "source": [
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "print(\"unique characters:\", vocab_size, ''.join(chars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yobmmaeK0EqX",
    "outputId": "26669f38-4b26-4b53-f642-ee7543e7c578"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'\\n': 0, ' ': 1, '!': 2, \"'\": 3, '(': 4, ')': 5, '*': 6, '+': 7, ',': 8, '-': 9, '.': 10, '/': 11, '0': 12, '1': 13, '2': 14, '3': 15, '4': 16, '5': 17, '6': 18, '7': 19, '8': 20, '9': 21, ':': 22, ';': 23, '<': 24, '=': 25, '>': 26, '?': 27, '@': 28, 'A': 29, 'B': 30, 'C': 31, 'D': 32, 'E': 33, 'F': 34, 'G': 35, 'H': 36, 'I': 37, 'J': 38, 'K': 39, 'L': 40, 'M': 41, 'N': 42, 'O': 43, 'P': 44, 'Q': 45, 'R': 46, 'S': 47, 'T': 48, 'U': 49, 'V': 50, 'W': 51, 'X': 52, 'Y': 53, 'Z': 54, '[': 55, ']': 56, '_': 57, 'a': 58, 'b': 59, 'c': 60, 'd': 61, 'e': 62, 'f': 63, 'g': 64, 'h': 65, 'i': 66, 'j': 67, 'k': 68, 'l': 69, 'm': 70, 'n': 71, 'o': 72, 'p': 73, 'q': 74, 'r': 75, 's': 76, 't': 77, 'u': 78, 'v': 79, 'w': 80, 'x': 81, 'y': 82, 'z': 83, '|': 84, '\\x94': 85, '': 86}\n",
      "{0: '\\n', 1: ' ', 2: '!', 3: \"'\", 4: '(', 5: ')', 6: '*', 7: '+', 8: ',', 9: '-', 10: '.', 11: '/', 12: '0', 13: '1', 14: '2', 15: '3', 16: '4', 17: '5', 18: '6', 19: '7', 20: '8', 21: '9', 22: ':', 23: ';', 24: '<', 25: '=', 26: '>', 27: '?', 28: '@', 29: 'A', 30: 'B', 31: 'C', 32: 'D', 33: 'E', 34: 'F', 35: 'G', 36: 'H', 37: 'I', 38: 'J', 39: 'K', 40: 'L', 41: 'M', 42: 'N', 43: 'O', 44: 'P', 45: 'Q', 46: 'R', 47: 'S', 48: 'T', 49: 'U', 50: 'V', 51: 'W', 52: 'X', 53: 'Y', 54: 'Z', 55: '[', 56: ']', 57: '_', 58: 'a', 59: 'b', 60: 'c', 61: 'd', 62: 'e', 63: 'f', 64: 'g', 65: 'h', 66: 'i', 67: 'j', 68: 'k', 69: 'l', 70: 'm', 71: 'n', 72: 'o', 73: 'p', 74: 'q', 75: 'r', 76: 's', 77: 't', 78: 'u', 79: 'v', 80: 'w', 81: 'x', 82: 'y', 83: 'z', 84: '|', 85: '\\x94', 86: ''}\n",
      "encoded: [43, 73, 62, 71, 1, 82, 72, 78, 75, 1, 70, 66, 71, 61, 10, 1, 43, 73, 62, 71]\n",
      "decoded: Open your mind. Open\n",
      "vocab size: 87\n"
     ]
    }
   ],
   "source": [
    "# yes, all language models will be character level, which isn't ideal but it's good for simplicity\n",
    "# very simple tokenizer\n",
    "stoi = {ch:i for i,ch in enumerate(chars)}\n",
    "itos = {i:ch for i,ch in enumerate(chars)}\n",
    "# add special token for padding\n",
    "stoi[''] = len(stoi)\n",
    "itos[len(itos)] = ''\n",
    "print(stoi)\n",
    "print(itos)\n",
    "encode = lambda s: [stoi[ch] for ch in s]\n",
    "decode = lambda l: ''.join([itos[i] for i in l])\n",
    "print(\"encoded:\", encode(text[:20]))\n",
    "print(\"decoded:\", decode(encode(text[:20])))\n",
    "vocab_size = len(itos)\n",
    "print(\"vocab size:\", vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6Pnf9KfP0EqY",
    "outputId": "ff581168-335a-4f0f-efeb-0d4bd5b225ca"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([39526018])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = torch.tensor(encode(text), dtype=torch.int64)\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lZ2fY1pR0EqY",
    "outputId": "5eb599e0-fb79-4cdb-c4b9-52a781d67151"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([43, 73, 62, 71,  1, 82, 72, 78, 75,  1, 70, 66, 71, 61, 10,  1, 43, 73, 62, 71,  1, 82, 72, 78, 75,  1, 70, 66, 71, 61, 10,  0,  0, 34, 58, 75,  1, 59, 62, 82, 72, 71, 61,  1, 77, 65, 62,  1, 61, 62, 62, 73,  1, 59, 69, 78, 62,  1, 33, 58, 75, 77, 65,  8,  1, 82, 72, 78,  1, 58, 71, 61,  1, 37,  1, 76, 65, 58, 69, 69,  1, 70, 62, 62, 77, 10, 10, 10,  0,  0, 29, 36,  2,  1, 41, 53,  1, 35, 43, 32])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oIaYesPh0Eqa",
    "outputId": "caa27cbb-5ae3-4406-8194-646264d4ba99"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([39130757]) torch.Size([395261])\n"
     ]
    }
   ],
   "source": [
    "n = int(len(data) * 0.99)\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]\n",
    "print(train_data.shape, val_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4bFhizcI0Eqa",
    "outputId": "c93a4d43-6fb2-4de7-aefc-8fed47e4a861"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([43, 73, 62, 71,  1, 82, 72, 78, 75])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq_len = 8\n",
    "train_data[:seq_len+1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "skFCPvQC0Eqc",
    "outputId": "08990c5b-88af-4a51-ce37-3c59989d6d74"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs:\n",
      "torch.Size([2, 64])\n",
      "tensor([[64,  1, 72, 78, 75,  1, 66, 71, 77, 62, 75, 71, 76, 65, 66, 73, 76, 10,  0,  0, 48, 65, 62,  1, 71, 66, 64, 65, 77,  1, 37, 66, 61, 58,  1, 64, 72, 77,  1, 77, 65, 62,  1, 75, 62, 76, 78, 69, 77, 76,  1, 72, 63,  1, 77, 65, 62,  1, 62, 81, 58, 70, 66, 71],\n",
      "        [62, 70,  2,  0,  0, 37, 77,  1, 69, 72, 72, 68, 76,  1, 69, 66, 68, 62,  1, 80, 62,  1, 77, 72, 72, 68,  1, 60, 58, 75, 62,  1, 72, 63,  1, 77, 65, 62, 70, 10,  0,  0, 48, 65, 72, 76, 62,  1, 64, 78, 82, 76,  1, 80, 62, 75, 62,  1, 66, 71, 60, 75, 62, 61]], device='cuda:0')\n",
      "targets:\n",
      "torch.Size([2, 64])\n",
      "tensor([[ 1, 72, 78, 75,  1, 66, 71, 77, 62, 75, 71, 76, 65, 66, 73, 76, 10,  0,  0, 48, 65, 62,  1, 71, 66, 64, 65, 77,  1, 37, 66, 61, 58,  1, 64, 72, 77,  1, 77, 65, 62,  1, 75, 62, 76, 78, 69, 77, 76,  1, 72, 63,  1, 77, 65, 62,  1, 62, 81, 58, 70, 66, 71, 58],\n",
      "        [70,  2,  0,  0, 37, 77,  1, 69, 72, 72, 68, 76,  1, 69, 66, 68, 62,  1, 80, 62,  1, 77, 72, 72, 68,  1, 60, 58, 75, 62,  1, 72, 63,  1, 77, 65, 62, 70, 10,  0,  0, 48, 65, 72, 76, 62,  1, 64, 78, 82, 76,  1, 80, 62, 75, 62,  1, 66, 71, 60, 75, 62, 61, 66]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "def get_batch(split, seq_len, batch_size=4):\n",
    "    # generate a small batch of data of inputs x and targets y\n",
    "    # targets are just inputs shifted by 1\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - seq_len, (batch_size,))\n",
    "    x = torch.stack([data[i:i+seq_len] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+seq_len+1] for i in ix])\n",
    "    return x.to(device), y.to(device)\n",
    "\n",
    "xb, yb = get_batch('train', 64, 2)\n",
    "print('inputs:')\n",
    "print(xb.shape)\n",
    "print(xb)\n",
    "print('targets:')\n",
    "print(yb.shape)\n",
    "print(yb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "327680000"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Make all steps, sequence lengths, and batch size the same\n",
    "total_steps = 5000\n",
    "seq_len = 256\n",
    "batch_size = 256 # these are small models so we can use large batch sizes to fully utilize the GPU\n",
    "# should cover around 2x the dataset\n",
    "total_steps * seq_len * batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_checkpoint(model, optimizer, scheduler, step, losses, val_losses, seq_len, batch_size, total_steps, save_dir='checkpoints'):\n",
    "    \"\"\"Save a complete checkpoint including model, optimizer, scheduler states and training metrics\"\"\"\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    \n",
    "    checkpoint = {\n",
    "        'step': step,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'scheduler_state_dict': scheduler.state_dict(),\n",
    "        'train_losses': losses,\n",
    "        'val_losses': val_losses,\n",
    "        'config': {\n",
    "            'seq_len': seq_len,\n",
    "            'batch_size': batch_size,\n",
    "            'total_steps': total_steps,\n",
    "            'vocab_size': model.token_embedding_table.num_embeddings,\n",
    "            'embed_size': model.blocks[0].sa_heads.head_size * model.head_num,\n",
    "            'head_num': model.head_num,\n",
    "            'layer_num': model.layer_num\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    checkpoint_path = os.path.join(save_dir, f'checkpoint_step_{step}.pt')\n",
    "    torch.save(checkpoint, checkpoint_path)\n",
    "    wandb.save(checkpoint_path)\n",
    "    print(f\"Checkpoint saved at step {step}: {checkpoint_path}\")\n",
    "    \n",
    "    return checkpoint_path\n",
    "\n",
    "def load_checkpoint(model, optimizer, scheduler, checkpoint_path):\n",
    "    \"\"\"Load a checkpoint and restore model, optimizer, scheduler states\"\"\"\n",
    "    checkpoint = torch.load(checkpoint_path, map_location=device)\n",
    "    \n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n",
    "    \n",
    "    print(f\"Checkpoint loaded from step {checkpoint['step']}\")\n",
    "    return checkpoint['step'], checkpoint['train_losses'], checkpoint['val_losses']\n",
    "\n",
    "def train(model, optimizer, scheduler, seq_len, batch_size, total_steps, val_steps=10, val_interval=50, checkpoint_interval=500, save_dir='checkpoints'):\n",
    "    losses = []\n",
    "    val_losses = []\n",
    "    # Create save directory\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    \n",
    "    # live plot\n",
    "    fig, ax = plt.subplots()\n",
    "    dh = display.display(fig, display_id=True)\n",
    "    \n",
    "    for step in (bar := tqdm(range(total_steps))):\n",
    "        # sample a batch of data\n",
    "        xb, yb = get_batch('train', seq_len=seq_len, batch_size=batch_size)\n",
    "\n",
    "        # evaluate the loss\n",
    "        logits, loss = model(xb, yb)\n",
    "\n",
    "        # backprop\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "        bar.set_description(f\"loss: {loss.item():.2f}, val loss: {val_losses[-1] if val_losses else 0:.2f}, lr: {scheduler.get_last_lr()[0]:.2e}\")\n",
    "        losses.append(loss.item())\n",
    "        \n",
    "        # Log to wandb\n",
    "        if use_wandb:\n",
    "            wandb.log({\n",
    "                'train_loss': loss.item(),\n",
    "                'learning_rate': scheduler.get_last_lr()[0],\n",
    "                'step': step\n",
    "            })\n",
    "        \n",
    "        if step % val_interval == 0:\n",
    "            # Calculate validation loss\n",
    "            with torch.no_grad():\n",
    "                val_loss = 0\n",
    "                for _ in range(val_steps):\n",
    "                    xb, yb = get_batch('val', seq_len=seq_len, batch_size=batch_size)\n",
    "                    _, loss = model(xb, yb)\n",
    "                    val_loss += loss.item()\n",
    "                val_loss /= val_steps\n",
    "                val_losses.append(val_loss)\n",
    "                \n",
    "            # Log validation loss to wandb\n",
    "            if use_wandb:\n",
    "                wandb.log({\n",
    "                    'val_loss': val_loss,\n",
    "                    'step': step\n",
    "                })\n",
    "            \n",
    "            ax.clear()\n",
    "            ax.plot(losses, color='blue', label='train loss', alpha=0.7)\n",
    "            ax.plot(range(0, len(losses), val_interval), val_losses, color='red', label='val loss', alpha=0.7)\n",
    "            ax.set_ylim(1, 4)\n",
    "            ax.legend()\n",
    "            dh.update(fig)\n",
    "            \n",
    "        # Save checkpoint\n",
    "        if step % checkpoint_interval == 0 and step > 0:\n",
    "            save_checkpoint(model, optimizer, scheduler, step, losses, val_losses, seq_len, batch_size, total_steps, save_dir)\n",
    "            \n",
    "    print('final loss:', loss.item(), 'final val loss:', val_loss)\n",
    "    \n",
    "    # Save final checkpoint\n",
    "    save_checkpoint(model, optimizer, scheduler, total_steps, losses, val_losses, seq_len, batch_size, total_steps, save_dir)\n",
    "    \n",
    "    return losses, val_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Measure post training perplexity on validation set\n",
    "# Create function that receives a model, context length, and PPL sequence length, and returns the perplexity\n",
    "# The PPL sequence length is the number of characters the function uses to calculate the perplexity\n",
    "# We take the logits and calculate the cross entropy loss from scratch, then exponentiate it to get the perplexity\n",
    "# not only that, but we want the models to do this in actual inference\n",
    "def perplexity(model, seq_len, ppl_seq_len, batch_size=128, val_steps=1000):\n",
    "    with torch.no_grad():\n",
    "        val_loss = 0\n",
    "        for _ in tqdm(range(val_steps)):\n",
    "            xb, yb = get_batch('val', seq_len=seq_len, batch_size=batch_size)\n",
    "            with torch.autocast(device_type=\"cuda\", dtype=torch.bfloat16):\n",
    "                logits, _ = model(xb, yb)\n",
    "            logits = logits.reshape(batch_size, seq_len, vocab_size)\n",
    "            logits = logits[:, :ppl_seq_len]\n",
    "            yb = yb[:, :ppl_seq_len]\n",
    "            # flatten logits and targets\n",
    "            logits = logits.reshape(batch_size*ppl_seq_len, vocab_size)\n",
    "            yb = yb.reshape(batch_size*ppl_seq_len)\n",
    "            # calculate cross entropy loss from scratch\n",
    "            loss = F.cross_entropy(logits, yb)\n",
    "            val_loss += loss.item()\n",
    "        val_loss /= val_steps\n",
    "        ppl = torch.exp(torch.tensor(val_loss))\n",
    "        return ppl.item(), val_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classic Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "k6cA2WbrayyL"
   },
   "outputs": [],
   "source": [
    "class TransformerConfig:\n",
    "    def __init__(self, vocab_size, seq_len, embed_size, head_num, layer_num):\n",
    "        self.vocab_size = vocab_size\n",
    "        self.seq_len = seq_len\n",
    "        self.embed_size = embed_size\n",
    "        self.head_num = head_num\n",
    "        self.layer_num = layer_num\n",
    "\n",
    "def precompute_freqs_cis(dim: int, end: int, theta: float = 10000.0):\n",
    "    freqs = 1.0 / (theta ** (torch.arange(0, dim, 2)[: (dim // 2)].float() / dim))\n",
    "    t = torch.arange(end, device=freqs.device, dtype=torch.float32)\n",
    "    freqs = torch.outer(t, freqs)\n",
    "    freqs_cis = torch.polar(torch.ones_like(freqs), freqs)  # complex64\n",
    "    return freqs_cis.to(device)\n",
    "\n",
    "def apply_rotary_emb(\n",
    "    xq: torch.Tensor,\n",
    "    xk: torch.Tensor,\n",
    "    freqs_cis: torch.Tensor,\n",
    "):\n",
    "    xq_ = torch.view_as_complex(xq.float().reshape(*xq.shape[:-1], -1, 2))\n",
    "    xk_ = torch.view_as_complex(xk.float().reshape(*xk.shape[:-1], -1, 2))\n",
    "    # freqs_cis = reshape_for_broadcast(freqs_cis, xq_)\n",
    "    q_shape = [d if i == xq_.ndim - 2 or i == xq_.ndim - 1 else 1 for i, d in enumerate(xq_.shape)]\n",
    "    k_shape = [d if i == xq_.ndim - 2 or i == xk_.ndim - 1 else 1 for i, d in enumerate(xk_.shape)]\n",
    "    T_q = xq_.shape[-2] \n",
    "    q_freqs_cis = freqs_cis[-T_q:].view(*q_shape)\n",
    "    k_freqs_cis = freqs_cis.view(*k_shape)\n",
    "    xq_out = torch.view_as_real(xq_ * q_freqs_cis).flatten(xq.dim() - 1)\n",
    "    xk_out = torch.view_as_real(xk_ * k_freqs_cis).flatten(xq.dim() - 1)\n",
    "    return xq_out.type_as(xq), xk_out.type_as(xk)\n",
    "\n",
    "class RMSNorm(torch.nn.Module):\n",
    "    def __init__(self, dim, eps=1e-6):\n",
    "        super().__init__()\n",
    "        self.eps = eps\n",
    "        self.weight = nn.Parameter(torch.ones(dim))\n",
    "\n",
    "    def _norm(self, x):\n",
    "        return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)\n",
    "\n",
    "    def forward(self, x):\n",
    "        output = self._norm(x.float()).type_as(x)\n",
    "        return output * self.weight\n",
    "    \n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.lin_1 = nn.Linear(config.embed_size, config.embed_size*4)\n",
    "        self.lin_2 = nn.Linear(config.embed_size*4, config.embed_size)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.lin_1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.lin_2(x)\n",
    "        return x\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\" multiple heads of self-attention with AliBi in parallel \"\"\"\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.seq_len = config.seq_len\n",
    "        self.head_num = config.head_num\n",
    "        self.head_size = config.embed_size // config.head_num\n",
    "        self.key = nn.Linear(config.embed_size, config.embed_size, bias=False)\n",
    "        self.query = nn.Linear(config.embed_size, config.embed_size, bias=False)\n",
    "        self.value = nn.Linear(config.embed_size, config.embed_size, bias=False)\n",
    "        self.o = nn.Linear(config.embed_size, config.embed_size)\n",
    "        # block_mask for FlexAttention\n",
    "        def causal(b, h, q_idx, kv_idx):\n",
    "            causal_mask = q_idx >= kv_idx\n",
    "            return causal_mask\n",
    "        self.causal_mask = create_block_mask(causal, B=None, H=None, Q_LEN=config.seq_len, KV_LEN=config.seq_len)\n",
    "        self.freqs_cis = precompute_freqs_cis(config.embed_size//config.head_num, config.seq_len)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(config.seq_len, config.seq_len)))\n",
    "\n",
    "    def forward(self, x, kv_cache=None):\n",
    "        B, T, C = x.shape\n",
    "        _, _, T_past, _ = kv_cache[0].shape if kv_cache is not None and kv_cache[0] is not None else (0, 0, 0, 0)\n",
    "        q = self.query(x) # (B,T,C)\n",
    "        k = self.key(x)   # (B,T,C)\n",
    "        v = self.value(x) # (B,T,C)\n",
    "\n",
    "        # Split into heads\n",
    "        q = q.view(B, T, self.head_num, self.head_size).transpose(1, 2) # (B, H, T, C/H)\n",
    "        k = k.view(B, T, self.head_num, self.head_size).transpose(1, 2) # (B, H, T, C/H)\n",
    "        v = v.view(B, T, self.head_num, self.head_size).transpose(1, 2) # (B, H, T, C/H)\n",
    "\n",
    "        if kv_cache is not None:\n",
    "            k_past, v_past = kv_cache\n",
    "            if k_past is not None:\n",
    "                k = torch.cat((k_past, k), dim=2)\n",
    "                v = torch.cat((v_past, v), dim=2)\n",
    "            if k.shape[-2] > self.seq_len:\n",
    "                k = k[:, :, -self.seq_len:]\n",
    "                v = v[:, :, -self.seq_len:]\n",
    "            kv_cache = (k, v)\n",
    "        T_k = k.shape[-2]\n",
    "        q, k = apply_rotary_emb(q, k, self.freqs_cis[:T_k])\n",
    "\n",
    "        if T == self.seq_len:\n",
    "            out = flex_attention(q, k, v, block_mask=self.causal_mask)\n",
    "        else:\n",
    "            # compute attention scores (\"affinities\")\n",
    "            wei = q @ k.transpose(-2,-1) # (B, H, 1, C/H) @ (B, H, C/H, T) -> (B, H, 1, T)\n",
    "            wei = wei * self.head_size ** -0.5 # scaled attention\n",
    "            wei = wei.masked_fill(self.tril[T_k-T:T_k, T_k-T:T_k] == 0, float('-inf')) # (B, T, T)\n",
    "            wei = F.softmax(wei, dim=-1) # (B, H, T, T)\n",
    "            # apply attention to values\n",
    "            out = wei @ v # (B, H, 1, T) @ (B, H, T, C/H) -> (B, H, 1, C/H)\n",
    "\n",
    "        out = out.transpose(1, 2).contiguous().view(B, T, C) # (B, H, T, C/H) -> (B, T, H, C/H) -> (B, T, C)\n",
    "        out = self.o(out)\n",
    "        return out, kv_cache\n",
    "\n",
    "class Block(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.sa_heads = MultiHeadAttention(config)\n",
    "        self.ff_layer = FeedForward(config)\n",
    "        self.sa_norm = RMSNorm(config.embed_size)\n",
    "        self.ff_norm = RMSNorm(config.embed_size)\n",
    "    \n",
    "    def forward(self, x, kv_cache=None):\n",
    "        a, kv_cache = self.sa_heads(self.sa_norm(x), kv_cache)\n",
    "        h = x + a\n",
    "        o = h + self.ff_layer(self.ff_norm(h))\n",
    "        return o, kv_cache\n",
    "    \n",
    "class TransformerLM(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.layer_num = config.layer_num\n",
    "        self.head_num = config.head_num\n",
    "        self.seq_len = config.seq_len\n",
    "        # embed raw tokens to a lower dimensional embedding with embed_size\n",
    "        self.token_embedding_table = nn.Embedding(config.vocab_size, config.embed_size)\n",
    "        # Language Modelling (?) Head is a standard linear layer to go from \n",
    "        # embeddings back to logits of vocab_size\n",
    "        self.lm_head = nn.Linear(config.embed_size, config.vocab_size)\n",
    "        # transformer blocks\n",
    "        self.blocks = nn.ModuleList([Block(config) for _ in range(config.layer_num)])\n",
    "\n",
    "    def forward(self, idx, targets=None, kv_cache=None):\n",
    "        B, T = idx.shape\n",
    "        _, _, T_past, _ = kv_cache[0][0].shape if kv_cache is not None and kv_cache[0][0] is not None else (0, 0, 0, 0)\n",
    "        # idx and targets are both (B,T) tensor of integers\n",
    "        tok_embd = self.token_embedding_table(idx) # (B,T,C)\n",
    "        x = tok_embd\n",
    "        # go through blocks\n",
    "        for i, block in enumerate(self.blocks):\n",
    "            x, cache = block(x, None if kv_cache is None else kv_cache[i])\n",
    "            if kv_cache is not None:\n",
    "                kv_cache[i] = cache\n",
    "        # get logits with linear layer\n",
    "        logits = self.lm_head(x) # (B,T,V)\n",
    "        \n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, V = logits.shape\n",
    "            logits = logits.view(B*T, V)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "    \n",
    "    def generate(self, idx, max_new_tokens, temperature=1, use_cache=True):\n",
    "        if use_cache:\n",
    "            # initialize key-value cache\n",
    "            kv_cache = [(None, None) for _ in range(self.layer_num)]\n",
    "            # idx is (B, T) array of indices in the current context\n",
    "            # crop idx to the last seq_len tokens\n",
    "            idx_context = idx[:, -self.seq_len:]\n",
    "            for _ in range(max_new_tokens):\n",
    "                # get the predictions\n",
    "                logits, loss = self(idx_context, kv_cache=kv_cache)\n",
    "                # focus only on the last time step\n",
    "                logits = logits[:, -1, :] # becomes (B, C)\n",
    "                # apply temperature\n",
    "                logits = logits / temperature if temperature > 0 else logits\n",
    "                # apply softmax to get probabilities\n",
    "                probs = F.softmax(logits, dim=-1) # (B, C)\n",
    "                # sample from the distribution\n",
    "                idx_next = torch.multinomial(probs, num_samples=1) if temperature > 0 else torch.argmax(probs, dim=-1, keepdim=True) # (B, 1)\n",
    "                # append sampled index to the running sequence\n",
    "                idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
    "                # since we have kv cache, only need to pass new token\n",
    "                idx_context = idx_next\n",
    "            return idx\n",
    "        else:\n",
    "            # idx is (B, T) array of indices in the current context\n",
    "            for _ in range(max_new_tokens):\n",
    "                #crop idx to the last seq_len tokens\n",
    "                idx_context = idx[:, -self.seq_len:]\n",
    "                # get the predictions\n",
    "                logits, loss = self(idx_context)\n",
    "                # focus only on the last time step\n",
    "                logits = logits[:, -1, :] # becomes (B, C)\n",
    "                # apply temperature\n",
    "                logits = logits / temperature if temperature > 0 else logits\n",
    "                # apply softmax to get probabilities\n",
    "                probs = F.softmax(logits, dim=-1) # (B, C)\n",
    "                # sample from the distribution\n",
    "                idx_next = torch.multinomial(probs, num_samples=1) if temperature > 0 else torch.argmax(probs, dim=-1, keepdim=True) # (B, 1)\n",
    "                # append sampled index to the running sequence\n",
    "                idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
    "            return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/coder/Python_project/transformers_playground/.venv/lib/python3.12/site-packages/torch/_inductor/lowering.py:1917: UserWarning: Torchinductor does not support code generation for complex operators. Performance may be worse than eager.\n",
      "  warnings.warn(\n",
      "Inductor Compilation: 100%|██████████| 15/15 [00:00<00:00, 293.42it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "4775511"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test forward pass\n",
    "torch_compile_options = {\n",
    "    'epilogue_fusion': True, \n",
    "    'max_autotune': False, \n",
    "    'shape_padding': True, \n",
    "    'trace.enabled': False, \n",
    "    'triton.cudagraphs': False, \n",
    "    'debug': False, \n",
    "    'dce': True, \n",
    "    'memory_planning': True, \n",
    "    'coordinate_descent_tuning': False, \n",
    "    'trace.graph_diagram': False, \n",
    "    'compile_threads': 32, \n",
    "    'group_fusion': True, \n",
    "    'disable_progress': False, \n",
    "    'verbose_progress': False, \n",
    "    'triton.multi_kernel': 0, \n",
    "    'triton.use_block_ptr': False, \n",
    "    'triton.enable_persistent_tma_matmul': False, \n",
    "    'triton.autotune_at_compile_time': False, \n",
    "    'triton.cooperative_reductions': False, \n",
    "    'cuda.compile_opt_level': '-O2', \n",
    "    'cuda.enable_cuda_lto': True, \n",
    "    'combo_kernels': True, \n",
    "    'benchmark_combo_kernel': True, \n",
    "    'combo_kernel_foreach_dynamic_shapes': True\n",
    "}\n",
    "\n",
    "config = TransformerConfig(\n",
    "    vocab_size=vocab_size,\n",
    "    seq_len=seq_len,\n",
    "    embed_size=256,\n",
    "    head_num=4,\n",
    "    layer_num=6\n",
    ")\n",
    "m = TransformerLM(config)\n",
    "m = torch.compile(m, options=torch_compile_options, fullgraph=True)\n",
    "m.to(device)\n",
    "xb, yb = get_batch('train', 5, 1)\n",
    "logits, loss = m(xb, yb)\n",
    "total_params = sum(p.numel() for p in m.parameters() if p.requires_grad)\n",
    "total_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inductor Compilation: 100%|██████████| 13/13 [00:00<00:00, 247.81it/s]\n",
      "W1018 18:42:21.767000 1896225 torch/_inductor/codegen/triton_combo_kernel.py:110] [0/1] ComboKernels: 1 large pointwise nodes are separated\n",
      "W1018 18:42:22.285000 1896225 torch/_inductor/codegen/triton_combo_kernel.py:110] [0/1] ComboKernels: 1 large pointwise nodes are separated\n",
      "W1018 18:42:22.435000 1896225 torch/_inductor/codegen/triton_combo_kernel.py:110] [0/1] ComboKernels: 1 large pointwise nodes are separated\n",
      "W1018 18:42:22.592000 1896225 torch/_inductor/codegen/triton_combo_kernel.py:110] [0/1] ComboKernels: 1 large pointwise nodes are separated\n",
      "W1018 18:42:22.743000 1896225 torch/_inductor/codegen/triton_combo_kernel.py:110] [0/1] ComboKernels: 1 large pointwise nodes are separated\n",
      "W1018 18:42:22.891000 1896225 torch/_inductor/codegen/triton_combo_kernel.py:110] [0/1] ComboKernels: 1 large pointwise nodes are separated\n",
      "W1018 18:42:23.386000 1896225 torch/_inductor/codegen/triton_combo_kernel.py:110] [0/1] ComboKernels: 1 large pointwise nodes are separated\n",
      "W1018 18:42:23.592000 1896225 torch/_inductor/codegen/triton_combo_kernel.py:110] [0/1] ComboKernels: 1 large pointwise nodes are separated\n",
      "W1018 18:42:23.688000 1896225 torch/_inductor/codegen/triton_combo_kernel.py:110] [0/1] ComboKernels: 1 large pointwise nodes are separated\n",
      "W1018 18:42:23.775000 1896225 torch/_inductor/codegen/triton_combo_kernel.py:110] [0/1] ComboKernels: 1 large pointwise nodes are separated\n",
      "W1018 18:42:23.860000 1896225 torch/_inductor/codegen/triton_combo_kernel.py:110] [0/1] ComboKernels: 1 large pointwise nodes are separated\n",
      "W1018 18:42:23.943000 1896225 torch/_inductor/codegen/triton_combo_kernel.py:110] [0/1] ComboKernels: 1 large pointwise nodes are separated\n",
      "Inductor Compilation: 100%|██████████| 24/24 [00:00<00:00, 577.54it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.22.2"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/coder/Python_project/transformers_playground/wandb/run-20251018_184224-fk1ohdfw</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/erlandpg/transformers-playground/runs/fk1ohdfw' target=\"_blank\">transformer-lm-animesubs-256seq-256embed-4head-6layer</a></strong> to <a href='https://wandb.ai/erlandpg/transformers-playground' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/erlandpg/transformers-playground' target=\"_blank\">https://wandb.ai/erlandpg/transformers-playground</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/erlandpg/transformers-playground/runs/fk1ohdfw' target=\"_blank\">https://wandb.ai/erlandpg/transformers-playground/runs/fk1ohdfw</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 400x300 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1714c9b2e9da4a39b3bedd326a82a570",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inductor Compilation: 100%|██████████| 13/13 [00:00<00:00, 445.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint saved at step 500: checkpoints/checkpoint_step_500.pt\n",
      "Checkpoint saved at step 1000: checkpoints/checkpoint_step_1000.pt\n",
      "Checkpoint saved at step 1500: checkpoints/checkpoint_step_1500.pt\n",
      "Checkpoint saved at step 2000: checkpoints/checkpoint_step_2000.pt\n",
      "Checkpoint saved at step 2500: checkpoints/checkpoint_step_2500.pt\n",
      "Checkpoint saved at step 3000: checkpoints/checkpoint_step_3000.pt\n",
      "Checkpoint saved at step 3500: checkpoints/checkpoint_step_3500.pt\n",
      "Checkpoint saved at step 4000: checkpoints/checkpoint_step_4000.pt\n",
      "Checkpoint saved at step 4500: checkpoints/checkpoint_step_4500.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m The nbformat package was not found. It is required to save notebook history.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final loss: 1.0342273712158203 final val loss: 1.1862868070602417\n",
      "Checkpoint saved at step 5000: checkpoints/checkpoint_step_5000.pt\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>learning_rate</td><td>██████████▇▇▇▇▆▆▆▆▅▄▃▃▃▃▃▃▃▂▂▂▂▂▂▁▁▁▁▁▁▁</td></tr><tr><td>step</td><td>▁▁▁▁▂▂▂▂▃▃▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇██</td></tr><tr><td>train_loss</td><td>█▆▅▅▅▄▄▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>█▇▅▅▅▄▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▁▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>learning_rate</td><td>0.0</td></tr><tr><td>step</td><td>4999</td></tr><tr><td>train_loss</td><td>1.03423</td></tr><tr><td>val_loss</td><td>1.18629</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">transformer-lm-animesubs-256seq-256embed-4head-6layer</strong> at: <a href='https://wandb.ai/erlandpg/transformers-playground/runs/fk1ohdfw' target=\"_blank\">https://wandb.ai/erlandpg/transformers-playground/runs/fk1ohdfw</a><br> View project at: <a href='https://wandb.ai/erlandpg/transformers-playground' target=\"_blank\">https://wandb.ai/erlandpg/transformers-playground</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20251018_184224-fk1ohdfw/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 400x300 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def get_wandb_config(model, optimizer, scheduler, seq_len, batch_size, total_steps):\n",
    "    \"\"\"Automatically extract wandb config from model and training parameters\"\"\"\n",
    "    config = {\n",
    "        \"architecture\": \"Transformer\",\n",
    "        \"dataset\": \"anime-subs\",\n",
    "        \"seq_len\": seq_len,\n",
    "        \"batch_size\": batch_size,\n",
    "        \"total_steps\": total_steps,\n",
    "        \"optimizer\": optimizer.__class__.__name__,\n",
    "        \"scheduler\": scheduler.__class__.__name__,\n",
    "        \"initial_lr\": optimizer.param_groups[0]['lr'],\n",
    "    }\n",
    "    \n",
    "    # Add model configuration\n",
    "    config.update({\n",
    "        \"vocab_size\": model.token_embedding_table.num_embeddings,\n",
    "        \"embed_size\": model.blocks[0].sa_heads.head_size * model.head_num,\n",
    "        \"head_num\": model.head_num,\n",
    "        \"layer_num\": model.layer_num,\n",
    "        \"total_params\": sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    })\n",
    "    \n",
    "    # Add scheduler-specific config\n",
    "    if hasattr(scheduler, 'T_max'):\n",
    "        config['scheduler_T_max'] = scheduler.T_max\n",
    "    if hasattr(scheduler, 'eta_min'):\n",
    "        config['scheduler_eta_min'] = scheduler.eta_min\n",
    "    \n",
    "    return config\n",
    "\n",
    "# Initialize wandb with automated config\n",
    "model = TransformerLM(config)\n",
    "model = torch.compile(model, options=torch_compile_options, fullgraph=True)\n",
    "model.to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=2e-3)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=total_steps, eta_min=1e-6)\n",
    "\n",
    "# warmup\n",
    "for _ in range(10):\n",
    "    model(*get_batch('train', seq_len=seq_len, batch_size=batch_size))\n",
    "\n",
    "wandb_config = get_wandb_config(model, optimizer, scheduler, seq_len, batch_size, total_steps)\n",
    "wandb.init(\n",
    "    project=\"transformers-playground\",\n",
    "    config=wandb_config,\n",
    "    name=\"transformer-lm-animesubs-256seq-256embed-4head-6layer\"\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "losses, val_losses = train(model, optimizer, scheduler, seq_len, batch_size, total_steps)\n",
    "\n",
    "# Finish wandb run\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Parent directory models does not exist.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[18]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Save\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstate_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mmodels/TransformerLM.pt\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Python_project/transformers_playground/.venv/lib/python3.12/site-packages/torch/serialization.py:964\u001b[39m, in \u001b[36msave\u001b[39m\u001b[34m(obj, f, pickle_module, pickle_protocol, _use_new_zipfile_serialization, _disable_byteorder_record)\u001b[39m\n\u001b[32m    961\u001b[39m     f = os.fspath(f)\n\u001b[32m    963\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m _use_new_zipfile_serialization:\n\u001b[32m--> \u001b[39m\u001b[32m964\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43m_open_zipfile_writer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m opened_zipfile:\n\u001b[32m    965\u001b[39m         _save(\n\u001b[32m    966\u001b[39m             obj,\n\u001b[32m    967\u001b[39m             opened_zipfile,\n\u001b[32m   (...)\u001b[39m\u001b[32m    970\u001b[39m             _disable_byteorder_record,\n\u001b[32m    971\u001b[39m         )\n\u001b[32m    972\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Python_project/transformers_playground/.venv/lib/python3.12/site-packages/torch/serialization.py:828\u001b[39m, in \u001b[36m_open_zipfile_writer\u001b[39m\u001b[34m(name_or_buffer)\u001b[39m\n\u001b[32m    826\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    827\u001b[39m     container = _open_zipfile_writer_buffer\n\u001b[32m--> \u001b[39m\u001b[32m828\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcontainer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname_or_buffer\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Python_project/transformers_playground/.venv/lib/python3.12/site-packages/torch/serialization.py:792\u001b[39m, in \u001b[36m_open_zipfile_writer_file.__init__\u001b[39m\u001b[34m(self, name)\u001b[39m\n\u001b[32m    785\u001b[39m     \u001b[38;5;28msuper\u001b[39m().\u001b[34m__init__\u001b[39m(\n\u001b[32m    786\u001b[39m         torch._C.PyTorchFileWriter(\n\u001b[32m    787\u001b[39m             \u001b[38;5;28mself\u001b[39m.file_stream, get_crc32_options(), _get_storage_alignment()\n\u001b[32m    788\u001b[39m         )\n\u001b[32m    789\u001b[39m     )\n\u001b[32m    790\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    791\u001b[39m     \u001b[38;5;28msuper\u001b[39m().\u001b[34m__init__\u001b[39m(\n\u001b[32m--> \u001b[39m\u001b[32m792\u001b[39m         \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_C\u001b[49m\u001b[43m.\u001b[49m\u001b[43mPyTorchFileWriter\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    793\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mget_crc32_options\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_get_storage_alignment\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    794\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    795\u001b[39m     )\n",
      "\u001b[31mRuntimeError\u001b[39m: Parent directory models does not exist."
     ]
    }
   ],
   "source": [
    "# Save\n",
    "torch.save(model.state_dict(), 'models/TransformerLM.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_28159/96251971.py:4: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load('models/TransformerLM.pt'))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "OptimizedModule(\n",
       "  (_orig_mod): TransformerLM(\n",
       "    (token_embedding_table): Embedding(87, 256)\n",
       "    (lm_head): Linear(in_features=256, out_features=87, bias=True)\n",
       "    (blocks): ModuleList(\n",
       "      (0-5): 6 x Block(\n",
       "        (sa_heads): MultiHeadAttention(\n",
       "          (key): Linear(in_features=256, out_features=256, bias=False)\n",
       "          (query): Linear(in_features=256, out_features=256, bias=False)\n",
       "          (value): Linear(in_features=256, out_features=256, bias=False)\n",
       "          (o): Linear(in_features=256, out_features=256, bias=True)\n",
       "        )\n",
       "        (ff_layer): FeedForward(\n",
       "          (lin_1): Linear(in_features=256, out_features=1024, bias=True)\n",
       "          (lin_2): Linear(in_features=1024, out_features=256, bias=True)\n",
       "          (relu): ReLU()\n",
       "        )\n",
       "        (sa_norm): RMSNorm()\n",
       "        (ff_norm): RMSNorm()\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load model\n",
    "model = TransformerLM(config)\n",
    "model = torch.compile(model, options=torch_compile_options, fullgraph=True)\n",
    "model.load_state_dict(torch.load('models/TransformerLM.pt'))\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f1e6d09745c645cab97ce2a92133cf0c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "perplexity: 3.334986925125122 loss: 1.20446875\n"
     ]
    }
   ],
   "source": [
    "# calculate perplexity\n",
    "ppl, loss = perplexity(model, seq_len, seq_len)\n",
    "print(\"perplexity:\", ppl, \"loss:\", loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "00r0pbm3b5eX",
    "outputId": "bdd3b34e-32a0-4724-b528-c162c0fd0c3e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[53, 72, 78,  1, 80, 66, 69, 69,  1, 71, 62, 79, 62, 75]])\n",
      "You will never decide what to do if you will take to see me.\n",
      "\n",
      "I don't mind it.\n",
      "\n",
      "I wonder if it'll be to make it to the bunny today.\n",
      "\n",
      "I wonder if that happens at all.\n",
      "\n",
      "If I can continue fighting something that would be the memories of the moment.\n",
      "\n",
      "But it doesn't matter if I heard that.\n",
      "\n",
      "I don't want to hear that for you.\n",
      "\n",
      "I wonder if I can become this possible.\n",
      "\n",
      "I'm sure that I would like to hear you.\n",
      "\n",
      "I wonder if the things would look like this...\n",
      "\n",
      "I think it's the only one who continues to take a picture of him.\n",
      "\n",
      "Where's he going home?\n",
      "\n",
      "If that's the worst true power to continue the entire training companies,\n",
      "\n",
      "I would have come to the train somewhere seriously.\n",
      "\n",
      "What do you think there are someone who can come to the country of the world line before we met.\n",
      "\n",
      "The next step is the power of the country with the moment I came to the prize again.\n",
      "\n",
      "I am the super bad and she could say that.\n",
      "\n",
      "I can explode the world from the fact that she couldn't make it to the manga she was a man for a manner.\n",
      "\n",
      "I'm a go\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "idx = encode(\"You will never\")\n",
    "print(torch.tensor([idx]))\n",
    "print(decode(model.generate(idx=torch.tensor([idx], dtype=torch.long).to(device), max_new_tokens=1000, temperature=0.5, use_cache=True)[0].tolist()))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
