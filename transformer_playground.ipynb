{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sdtDsu1Y0EqL"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import math\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from torch.nn import functional as F\n",
    "from torch.nn.attention.flex_attention import flex_attention, create_block_mask\n",
    "import triton\n",
    "import triton.language as tl\n",
    "from tqdm.notebook import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython import display\n",
    "torch.manual_seed(69)\n",
    "torch.set_printoptions(profile=\"short\", sci_mode=False, linewidth=100000)\n",
    "torch.set_float32_matmul_precision('high')\n",
    "# this script is configured to run on a RTX 3060 12GB GPU. you'll want to adjust the model sizes and batch sizes for other devices\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# device = torch.device('cpu')\n",
    "plt.rcParams['figure.figsize'] = [8, 6]\n",
    "plt.rcParams['figure.dpi'] = 50\n",
    "plt.rcParams['axes.grid'] = True\n",
    "plt.rcParams['xtick.minor.visible'] = True\n",
    "plt.rcParams['ytick.minor.visible'] = True\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ANr5dn7W0EqR",
    "outputId": "a00cbe8d-e1c6-454b-b552-4ffd17ce17e5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of dataset in characters:  39526018\n"
     ]
    }
   ],
   "source": [
    "# we use this 40mb file of concatenated anime subtitles as our dataset\n",
    "# just the right size for toy experiments like this I think\n",
    "with open('animesubs.txt', 'r', encoding='latin') as f:\n",
    "    text = f.read()\n",
    "print(\"length of dataset in characters: \", len(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pANiObIZ0EqU",
    "outputId": "98f70469-1c89-4341-89e8-c142a14331b1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Open your mind. Open your mind.\n",
      "\n",
      "Far beyond the deep blue Earth, you and I shall meet...\n",
      "\n",
      "AH! MY GODDESS\n",
      "\n",
      "A snow-white feather comes fluttering down, swaying gently in the air.\n",
      "\n",
      "Without holding back, I want to envelope you, my one and only love.\n",
      "\n",
      "I know I have the power to protect the one I love, right here in my hands.\n",
      "\n",
      "Open your mind. Just as I've always dreamed.\n",
      "\n",
      "Let the wind carry off your hopes, faraway.\n",
      "\n",
      "I have wings nobody can see. Look, you have them, too.\n",
      "\n",
      "They'll take us to where we ca\n"
     ]
    }
   ],
   "source": [
    "print(text[:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WvM5h6_i3KsM"
   },
   "outputs": [],
   "source": [
    "# remove japanese characters\n",
    "text = ''.join(filter(lambda character:ord(character) < 0x3000, text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "U7SOcWJM0EqW",
    "outputId": "a82a4da8-a8ed-47bf-cfb5-2e4c59dee2cf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unique characters: 86 \n",
      " !'()*+,-./0123456789:;<=>?@ABCDEFGHIJKLMNOPQRSTUVWXYZ[]_abcdefghijklmnopqrstuvwxyz|\n"
     ]
    }
   ],
   "source": [
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "print(\"unique characters:\", vocab_size, ''.join(chars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yobmmaeK0EqX",
    "outputId": "26669f38-4b26-4b53-f642-ee7543e7c578"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'\\n': 0, ' ': 1, '!': 2, \"'\": 3, '(': 4, ')': 5, '*': 6, '+': 7, ',': 8, '-': 9, '.': 10, '/': 11, '0': 12, '1': 13, '2': 14, '3': 15, '4': 16, '5': 17, '6': 18, '7': 19, '8': 20, '9': 21, ':': 22, ';': 23, '<': 24, '=': 25, '>': 26, '?': 27, '@': 28, 'A': 29, 'B': 30, 'C': 31, 'D': 32, 'E': 33, 'F': 34, 'G': 35, 'H': 36, 'I': 37, 'J': 38, 'K': 39, 'L': 40, 'M': 41, 'N': 42, 'O': 43, 'P': 44, 'Q': 45, 'R': 46, 'S': 47, 'T': 48, 'U': 49, 'V': 50, 'W': 51, 'X': 52, 'Y': 53, 'Z': 54, '[': 55, ']': 56, '_': 57, 'a': 58, 'b': 59, 'c': 60, 'd': 61, 'e': 62, 'f': 63, 'g': 64, 'h': 65, 'i': 66, 'j': 67, 'k': 68, 'l': 69, 'm': 70, 'n': 71, 'o': 72, 'p': 73, 'q': 74, 'r': 75, 's': 76, 't': 77, 'u': 78, 'v': 79, 'w': 80, 'x': 81, 'y': 82, 'z': 83, '|': 84, '\\x94': 85, '': 86}\n",
      "{0: '\\n', 1: ' ', 2: '!', 3: \"'\", 4: '(', 5: ')', 6: '*', 7: '+', 8: ',', 9: '-', 10: '.', 11: '/', 12: '0', 13: '1', 14: '2', 15: '3', 16: '4', 17: '5', 18: '6', 19: '7', 20: '8', 21: '9', 22: ':', 23: ';', 24: '<', 25: '=', 26: '>', 27: '?', 28: '@', 29: 'A', 30: 'B', 31: 'C', 32: 'D', 33: 'E', 34: 'F', 35: 'G', 36: 'H', 37: 'I', 38: 'J', 39: 'K', 40: 'L', 41: 'M', 42: 'N', 43: 'O', 44: 'P', 45: 'Q', 46: 'R', 47: 'S', 48: 'T', 49: 'U', 50: 'V', 51: 'W', 52: 'X', 53: 'Y', 54: 'Z', 55: '[', 56: ']', 57: '_', 58: 'a', 59: 'b', 60: 'c', 61: 'd', 62: 'e', 63: 'f', 64: 'g', 65: 'h', 66: 'i', 67: 'j', 68: 'k', 69: 'l', 70: 'm', 71: 'n', 72: 'o', 73: 'p', 74: 'q', 75: 'r', 76: 's', 77: 't', 78: 'u', 79: 'v', 80: 'w', 81: 'x', 82: 'y', 83: 'z', 84: '|', 85: '\\x94', 86: ''}\n",
      "encoded: [43, 73, 62, 71, 1, 82, 72, 78, 75, 1, 70, 66, 71, 61, 10, 1, 43, 73, 62, 71]\n",
      "decoded: Open your mind. Open\n",
      "vocab size: 87\n"
     ]
    }
   ],
   "source": [
    "# yes, all language models will be character level, which isn't ideal but it's good for simplicity\n",
    "# very simple tokenizer\n",
    "stoi = {ch:i for i,ch in enumerate(chars)}\n",
    "itos = {i:ch for i,ch in enumerate(chars)}\n",
    "# add special token for padding\n",
    "stoi[''] = len(stoi)\n",
    "itos[len(itos)] = ''\n",
    "print(stoi)\n",
    "print(itos)\n",
    "encode = lambda s: [stoi[ch] for ch in s]\n",
    "decode = lambda l: ''.join([itos[i] for i in l])\n",
    "print(\"encoded:\", encode(text[:20]))\n",
    "print(\"decoded:\", decode(encode(text[:20])))\n",
    "vocab_size = len(itos)\n",
    "print(\"vocab size:\", vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6Pnf9KfP0EqY",
    "outputId": "ff581168-335a-4f0f-efeb-0d4bd5b225ca"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([39526018])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = torch.tensor(encode(text), dtype=torch.int64)\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lZ2fY1pR0EqY",
    "outputId": "5eb599e0-fb79-4cdb-c4b9-52a781d67151"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([43, 73, 62, 71,  1, 82, 72, 78, 75,  1, 70, 66, 71, 61, 10,  1, 43, 73, 62, 71,  1, 82, 72, 78, 75,  1, 70, 66, 71, 61, 10,  0,  0, 34, 58, 75,  1, 59, 62, 82, 72, 71, 61,  1, 77, 65, 62,  1, 61, 62, 62, 73,  1, 59, 69, 78, 62,  1, 33, 58, 75, 77, 65,  8,  1, 82, 72, 78,  1, 58, 71, 61,  1, 37,  1, 76, 65, 58, 69, 69,  1, 70, 62, 62, 77, 10, 10, 10,  0,  0, 29, 36,  2,  1, 41, 53,  1, 35, 43, 32])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oIaYesPh0Eqa",
    "outputId": "caa27cbb-5ae3-4406-8194-646264d4ba99"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([39130757]) torch.Size([395261])\n"
     ]
    }
   ],
   "source": [
    "n = int(len(data) * 0.99)\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]\n",
    "print(train_data.shape, val_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4bFhizcI0Eqa",
    "outputId": "c93a4d43-6fb2-4de7-aefc-8fed47e4a861"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([43, 73, 62, 71,  1, 82, 72, 78, 75])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq_len = 8\n",
    "train_data[:seq_len+1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "skFCPvQC0Eqc",
    "outputId": "08990c5b-88af-4a51-ce37-3c59989d6d74"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs:\n",
      "torch.Size([2, 64])\n",
      "tensor([[64,  1, 72, 78, 75,  1, 66, 71, 77, 62, 75, 71, 76, 65, 66, 73, 76, 10,  0,  0, 48, 65, 62,  1, 71, 66, 64, 65, 77,  1, 37, 66, 61, 58,  1, 64, 72, 77,  1, 77, 65, 62,  1, 75, 62, 76, 78, 69, 77, 76,  1, 72, 63,  1, 77, 65, 62,  1, 62, 81, 58, 70, 66, 71],\n",
      "        [62, 70,  2,  0,  0, 37, 77,  1, 69, 72, 72, 68, 76,  1, 69, 66, 68, 62,  1, 80, 62,  1, 77, 72, 72, 68,  1, 60, 58, 75, 62,  1, 72, 63,  1, 77, 65, 62, 70, 10,  0,  0, 48, 65, 72, 76, 62,  1, 64, 78, 82, 76,  1, 80, 62, 75, 62,  1, 66, 71, 60, 75, 62, 61]], device='cuda:0')\n",
      "targets:\n",
      "torch.Size([2, 64])\n",
      "tensor([[ 1, 72, 78, 75,  1, 66, 71, 77, 62, 75, 71, 76, 65, 66, 73, 76, 10,  0,  0, 48, 65, 62,  1, 71, 66, 64, 65, 77,  1, 37, 66, 61, 58,  1, 64, 72, 77,  1, 77, 65, 62,  1, 75, 62, 76, 78, 69, 77, 76,  1, 72, 63,  1, 77, 65, 62,  1, 62, 81, 58, 70, 66, 71, 58],\n",
      "        [70,  2,  0,  0, 37, 77,  1, 69, 72, 72, 68, 76,  1, 69, 66, 68, 62,  1, 80, 62,  1, 77, 72, 72, 68,  1, 60, 58, 75, 62,  1, 72, 63,  1, 77, 65, 62, 70, 10,  0,  0, 48, 65, 72, 76, 62,  1, 64, 78, 82, 76,  1, 80, 62, 75, 62,  1, 66, 71, 60, 75, 62, 61, 66]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "def get_batch(split, seq_len, batch_size=4):\n",
    "    # generate a small batch of data of inputs x and targets y\n",
    "    # targets are just inputs shifted by 1\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - seq_len, (batch_size,))\n",
    "    x = torch.stack([data[i:i+seq_len] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+seq_len+1] for i in ix])\n",
    "    return x.to(device), y.to(device)\n",
    "\n",
    "xb, yb = get_batch('train', 64, 2)\n",
    "print('inputs:')\n",
    "print(xb.shape)\n",
    "print(xb)\n",
    "print('targets:')\n",
    "print(yb.shape)\n",
    "print(yb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "327680000"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Make all steps, sequence lengths, and batch size the same\n",
    "total_steps = 5000\n",
    "seq_len = 256\n",
    "batch_size = 256 # these are small models so we can use large batch sizes to fully utilize the GPU\n",
    "# should cover around 2x the dataset\n",
    "total_steps * seq_len * batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, optimizer, seq_len, batch_size, total_steps, val_steps=10, val_interval=50):\n",
    "    losses = []\n",
    "    val_losses = []\n",
    "    # live plot\n",
    "    fig, ax = plt.subplots()\n",
    "    dh = display.display(fig, display_id=True)\n",
    "    for steps in (bar := tqdm(range(total_steps))):  # increase number of steps for good results...\n",
    "        # sample a batch of data\n",
    "        xb, yb = get_batch('train', seq_len=seq_len, batch_size=batch_size)\n",
    "\n",
    "        # evaluate the loss\n",
    "        logits, loss = model(xb, yb)\n",
    "\n",
    "        # backprop\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        bar.set_description(f\"loss: {loss.item():.2f}, val loss: {val_losses[-1] if val_losses else 0:.2f}\")\n",
    "        losses.append(loss.item())\n",
    "        if steps % val_interval == 0:\n",
    "            # Calculate validation loss\n",
    "            with torch.no_grad():\n",
    "                val_loss = 0\n",
    "                for _ in range(val_steps):\n",
    "                    xb, yb = get_batch('val', seq_len=seq_len, batch_size=batch_size)\n",
    "                    _, loss = model(xb, yb)\n",
    "                    val_loss += loss.item()\n",
    "                val_loss /= val_steps\n",
    "                val_losses.append(val_loss)\n",
    "            ax.clear()\n",
    "            ax.plot(losses, color='blue', label='train loss', alpha=0.7)\n",
    "            ax.plot(range(0, len(losses), val_interval), val_losses, color='red', label='val loss', alpha=0.7)\n",
    "            ax.set_ylim(1, 4)\n",
    "            ax.legend()\n",
    "            dh.update(fig)\n",
    "    print('final loss:', loss.item(), 'final val loss:', val_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Measure post training perplexity on validation set\n",
    "# Create function that receives a model, context length, and PPL sequence length, and returns the perplexity\n",
    "# The PPL sequence length is the number of characters the function uses to calculate the perplexity\n",
    "# We take the logits and calculate the cross entropy loss from scratch, then exponentiate it to get the perplexity\n",
    "# not only that, but we want the models to do this in actual inference\n",
    "def perplexity(model, seq_len, ppl_seq_len, batch_size=128, val_steps=1000):\n",
    "    with torch.no_grad():\n",
    "        val_loss = 0\n",
    "        for _ in tqdm(range(val_steps)):\n",
    "            xb, yb = get_batch('val', seq_len=seq_len, batch_size=batch_size)\n",
    "            with torch.autocast(device_type=\"cuda\", dtype=torch.bfloat16):\n",
    "                logits, _ = model(xb, yb)\n",
    "            logits = logits.reshape(batch_size, seq_len, vocab_size)\n",
    "            logits = logits[:, :ppl_seq_len]\n",
    "            yb = yb[:, :ppl_seq_len]\n",
    "            # flatten logits and targets\n",
    "            logits = logits.reshape(batch_size*ppl_seq_len, vocab_size)\n",
    "            yb = yb.reshape(batch_size*ppl_seq_len)\n",
    "            # calculate cross entropy loss from scratch\n",
    "            loss = F.cross_entropy(logits, yb)\n",
    "            val_loss += loss.item()\n",
    "        val_loss /= val_steps\n",
    "        ppl = torch.exp(torch.tensor(val_loss))\n",
    "        return ppl.item(), val_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classic Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "k6cA2WbrayyL"
   },
   "outputs": [],
   "source": [
    "class TransformerConfig:\n",
    "    def __init__(self, vocab_size, seq_len, embed_size, head_num, layer_num):\n",
    "        self.vocab_size = vocab_size\n",
    "        self.seq_len = seq_len\n",
    "        self.embed_size = embed_size\n",
    "        self.head_num = head_num\n",
    "        self.layer_num = layer_num\n",
    "\n",
    "def precompute_freqs_cis(dim: int, end: int, theta: float = 10000.0):\n",
    "    freqs = 1.0 / (theta ** (torch.arange(0, dim, 2)[: (dim // 2)].float() / dim))\n",
    "    t = torch.arange(end, device=freqs.device, dtype=torch.float32)\n",
    "    freqs = torch.outer(t, freqs)\n",
    "    freqs_cis = torch.polar(torch.ones_like(freqs), freqs)  # complex64\n",
    "    return freqs_cis.to(device)\n",
    "\n",
    "def apply_rotary_emb(\n",
    "    xq: torch.Tensor,\n",
    "    xk: torch.Tensor,\n",
    "    freqs_cis: torch.Tensor,\n",
    "):\n",
    "    xq_ = torch.view_as_complex(xq.float().reshape(*xq.shape[:-1], -1, 2))\n",
    "    xk_ = torch.view_as_complex(xk.float().reshape(*xk.shape[:-1], -1, 2))\n",
    "    # freqs_cis = reshape_for_broadcast(freqs_cis, xq_)\n",
    "    q_shape = [d if i == xq_.ndim - 2 or i == xq_.ndim - 1 else 1 for i, d in enumerate(xq_.shape)]\n",
    "    k_shape = [d if i == xq_.ndim - 2 or i == xk_.ndim - 1 else 1 for i, d in enumerate(xk_.shape)]\n",
    "    T_q = xq_.shape[-2] \n",
    "    q_freqs_cis = freqs_cis[-T_q:].view(*q_shape)\n",
    "    k_freqs_cis = freqs_cis.view(*k_shape)\n",
    "    xq_out = torch.view_as_real(xq_ * q_freqs_cis).flatten(xq.dim() - 1)\n",
    "    xk_out = torch.view_as_real(xk_ * k_freqs_cis).flatten(xq.dim() - 1)\n",
    "    return xq_out.type_as(xq), xk_out.type_as(xk)\n",
    "\n",
    "class RMSNorm(torch.nn.Module):\n",
    "    def __init__(self, dim, eps=1e-6):\n",
    "        super().__init__()\n",
    "        self.eps = eps\n",
    "        self.weight = nn.Parameter(torch.ones(dim))\n",
    "\n",
    "    def _norm(self, x):\n",
    "        return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)\n",
    "\n",
    "    def forward(self, x):\n",
    "        output = self._norm(x.float()).type_as(x)\n",
    "        return output * self.weight\n",
    "    \n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.lin_1 = nn.Linear(config.embed_size, config.embed_size*4)\n",
    "        self.lin_2 = nn.Linear(config.embed_size*4, config.embed_size)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.lin_1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.lin_2(x)\n",
    "        return x\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\" multiple heads of self-attention with AliBi in parallel \"\"\"\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.seq_len = config.seq_len\n",
    "        self.head_num = config.head_num\n",
    "        self.head_size = config.embed_size // config.head_num\n",
    "        self.key = nn.Linear(config.embed_size, config.embed_size, bias=False)\n",
    "        self.query = nn.Linear(config.embed_size, config.embed_size, bias=False)\n",
    "        self.value = nn.Linear(config.embed_size, config.embed_size, bias=False)\n",
    "        self.o = nn.Linear(config.embed_size, config.embed_size)\n",
    "        # block_mask for FlexAttention\n",
    "        def causal(b, h, q_idx, kv_idx):\n",
    "            causal_mask = q_idx >= kv_idx\n",
    "            return causal_mask\n",
    "        self.causal_mask = create_block_mask(causal, B=None, H=None, Q_LEN=config.seq_len, KV_LEN=config.seq_len)\n",
    "        self.freqs_cis = precompute_freqs_cis(config.embed_size//config.head_num, config.seq_len)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(config.seq_len, config.seq_len)))\n",
    "\n",
    "    def forward(self, x, kv_cache=None):\n",
    "        B, T, C = x.shape\n",
    "        _, _, T_past, _ = kv_cache[0].shape if kv_cache is not None and kv_cache[0] is not None else (0, 0, 0, 0)\n",
    "        q = self.query(x) # (B,T,C)\n",
    "        k = self.key(x)   # (B,T,C)\n",
    "        v = self.value(x) # (B,T,C)\n",
    "\n",
    "        # Split into heads\n",
    "        q = q.view(B, T, self.head_num, self.head_size).transpose(1, 2) # (B, H, T, C/H)\n",
    "        k = k.view(B, T, self.head_num, self.head_size).transpose(1, 2) # (B, H, T, C/H)\n",
    "        v = v.view(B, T, self.head_num, self.head_size).transpose(1, 2) # (B, H, T, C/H)\n",
    "\n",
    "        if kv_cache is not None:\n",
    "            k_past, v_past = kv_cache\n",
    "            if k_past is not None:\n",
    "                k = torch.cat((k_past, k), dim=2)\n",
    "                v = torch.cat((v_past, v), dim=2)\n",
    "            if k.shape[-2] > self.seq_len:\n",
    "                k = k[:, :, -self.seq_len:]\n",
    "                v = v[:, :, -self.seq_len:]\n",
    "            kv_cache = (k, v)\n",
    "        T_k = k.shape[-2]\n",
    "        q, k = apply_rotary_emb(q, k, self.freqs_cis[:T_k])\n",
    "\n",
    "        if T == self.seq_len:\n",
    "            out = flex_attention(q, k, v, block_mask=self.causal_mask)\n",
    "        else:\n",
    "            # compute attention scores (\"affinities\")\n",
    "            wei = q @ k.transpose(-2,-1) # (B, H, 1, C/H) @ (B, H, C/H, T) -> (B, H, 1, T)\n",
    "            wei = wei * self.head_size ** -0.5 # scaled attention\n",
    "            wei = wei.masked_fill(self.tril[T_k-T:T_k, T_k-T:T_k] == 0, float('-inf')) # (B, T, T)\n",
    "            wei = F.softmax(wei, dim=-1) # (B, H, T, T)\n",
    "            # apply attention to values\n",
    "            out = wei @ v # (B, H, 1, T) @ (B, H, T, C/H) -> (B, H, 1, C/H)\n",
    "\n",
    "        out = out.transpose(1, 2).contiguous().view(B, T, C) # (B, H, T, C/H) -> (B, T, H, C/H) -> (B, T, C)\n",
    "        out = self.o(out)\n",
    "        return out, kv_cache\n",
    "\n",
    "class Block(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.sa_heads = MultiHeadAttention(config)\n",
    "        self.ff_layer = FeedForward(config)\n",
    "        self.sa_norm = RMSNorm(config.embed_size)\n",
    "        self.ff_norm = RMSNorm(config.embed_size)\n",
    "    \n",
    "    def forward(self, x, kv_cache=None):\n",
    "        a, kv_cache = self.sa_heads(self.sa_norm(x), kv_cache)\n",
    "        h = x + a\n",
    "        o = h + self.ff_layer(self.ff_norm(h))\n",
    "        return o, kv_cache\n",
    "    \n",
    "class TransformerLM(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.layer_num = config.layer_num\n",
    "        self.head_num = config.head_num\n",
    "        self.seq_len = config.seq_len\n",
    "        # embed raw tokens to a lower dimensional embedding with embed_size\n",
    "        self.token_embedding_table = nn.Embedding(config.vocab_size, config.embed_size)\n",
    "        # Language Modelling (?) Head is a standard linear layer to go from \n",
    "        # embeddings back to logits of vocab_size\n",
    "        self.lm_head = nn.Linear(config.embed_size, config.vocab_size)\n",
    "        # transformer blocks\n",
    "        self.blocks = nn.ModuleList([Block(config) for _ in range(config.layer_num)])\n",
    "\n",
    "    def forward(self, idx, targets=None, kv_cache=None):\n",
    "        B, T = idx.shape\n",
    "        _, _, T_past, _ = kv_cache[0][0].shape if kv_cache is not None and kv_cache[0][0] is not None else (0, 0, 0, 0)\n",
    "        # idx and targets are both (B,T) tensor of integers\n",
    "        tok_embd = self.token_embedding_table(idx) # (B,T,C)\n",
    "        x = tok_embd\n",
    "        # go through blocks\n",
    "        for i, block in enumerate(self.blocks):\n",
    "            x, cache = block(x, None if kv_cache is None else kv_cache[i])\n",
    "            if kv_cache is not None:\n",
    "                kv_cache[i] = cache\n",
    "        # get logits with linear layer\n",
    "        logits = self.lm_head(x) # (B,T,V)\n",
    "        \n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, V = logits.shape\n",
    "            logits = logits.view(B*T, V)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "    \n",
    "    def generate(self, idx, max_new_tokens, temperature=1, use_cache=True):\n",
    "        if use_cache:\n",
    "            # initialize key-value cache\n",
    "            kv_cache = [(None, None) for _ in range(self.layer_num)]\n",
    "            # idx is (B, T) array of indices in the current context\n",
    "            # crop idx to the last seq_len tokens\n",
    "            idx_context = idx[:, -self.seq_len:]\n",
    "            for _ in range(max_new_tokens):\n",
    "                # get the predictions\n",
    "                logits, loss = self(idx_context, kv_cache=kv_cache)\n",
    "                # focus only on the last time step\n",
    "                logits = logits[:, -1, :] # becomes (B, C)\n",
    "                # apply temperature\n",
    "                logits = logits / temperature if temperature > 0 else logits\n",
    "                # apply softmax to get probabilities\n",
    "                probs = F.softmax(logits, dim=-1) # (B, C)\n",
    "                # sample from the distribution\n",
    "                idx_next = torch.multinomial(probs, num_samples=1) if temperature > 0 else torch.argmax(probs, dim=-1, keepdim=True) # (B, 1)\n",
    "                # append sampled index to the running sequence\n",
    "                idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
    "                # since we have kv cache, only need to pass new token\n",
    "                idx_context = idx_next\n",
    "            return idx\n",
    "        else:\n",
    "            # idx is (B, T) array of indices in the current context\n",
    "            for _ in range(max_new_tokens):\n",
    "                #crop idx to the last seq_len tokens\n",
    "                idx_context = idx[:, -self.seq_len:]\n",
    "                # get the predictions\n",
    "                logits, loss = self(idx_context)\n",
    "                # focus only on the last time step\n",
    "                logits = logits[:, -1, :] # becomes (B, C)\n",
    "                # apply temperature\n",
    "                logits = logits / temperature if temperature > 0 else logits\n",
    "                # apply softmax to get probabilities\n",
    "                probs = F.softmax(logits, dim=-1) # (B, C)\n",
    "                # sample from the distribution\n",
    "                idx_next = torch.multinomial(probs, num_samples=1) if temperature > 0 else torch.argmax(probs, dim=-1, keepdim=True) # (B, 1)\n",
    "                # append sampled index to the running sequence\n",
    "                idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
    "            return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4775511"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test forward pass\n",
    "config = TransformerConfig(\n",
    "    vocab_size=vocab_size,\n",
    "    seq_len=seq_len,\n",
    "    embed_size=256,\n",
    "    head_num=4,\n",
    "    layer_num=6\n",
    ")\n",
    "m = TransformerLM(config)\n",
    "m.to(device)\n",
    "xb, yb = get_batch('train', 5, 1)\n",
    "logits, loss = m(xb, yb)\n",
    "total_params = sum(p.numel() for p in m.parameters() if p.requires_grad)\n",
    "total_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize wandb with automated config for classic transformer\n",
    "model = TransformerLM(config)\n",
    "model = torch.compile(model)\n",
    "model.to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=2e-3)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=total_steps, eta_min=1e-6)\n",
    "\n",
    "wandb_config = get_wandb_config(model, optimizer, scheduler, seq_len, batch_size, total_steps)\n",
    "wandb.init(\n",
    "    project=\"transformers-playground\",\n",
    "    config=wandb_config,\n",
    "    name=\"classic-transformer\"\n",
    ")\n",
    "\n",
    "# Train the model with enhanced features\n",
    "losses, val_losses = train(model, optimizer, scheduler, seq_len, batch_size, total_steps)\n",
    "\n",
    "# Finish wandb run\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save\n",
    "torch.save(model.state_dict(), 'models/TransformerLM.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_28159/96251971.py:4: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load('models/TransformerLM.pt'))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "OptimizedModule(\n",
       "  (_orig_mod): TransformerLM(\n",
       "    (token_embedding_table): Embedding(87, 256)\n",
       "    (lm_head): Linear(in_features=256, out_features=87, bias=True)\n",
       "    (blocks): ModuleList(\n",
       "      (0-5): 6 x Block(\n",
       "        (sa_heads): MultiHeadAttention(\n",
       "          (key): Linear(in_features=256, out_features=256, bias=False)\n",
       "          (query): Linear(in_features=256, out_features=256, bias=False)\n",
       "          (value): Linear(in_features=256, out_features=256, bias=False)\n",
       "          (o): Linear(in_features=256, out_features=256, bias=True)\n",
       "        )\n",
       "        (ff_layer): FeedForward(\n",
       "          (lin_1): Linear(in_features=256, out_features=1024, bias=True)\n",
       "          (lin_2): Linear(in_features=1024, out_features=256, bias=True)\n",
       "          (relu): ReLU()\n",
       "        )\n",
       "        (sa_norm): RMSNorm()\n",
       "        (ff_norm): RMSNorm()\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load model\n",
    "model = TransformerLM(config)\n",
    "model = torch.compile(model)\n",
    "model.load_state_dict(torch.load('models/TransformerLM.pt'))\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f1e6d09745c645cab97ce2a92133cf0c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "perplexity: 3.334986925125122 loss: 1.20446875\n"
     ]
    }
   ],
   "source": [
    "# calculate perplexity\n",
    "ppl, loss = perplexity(model, seq_len, seq_len)\n",
    "print(\"perplexity:\", ppl, \"loss:\", loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "00r0pbm3b5eX",
    "outputId": "bdd3b34e-32a0-4724-b528-c162c0fd0c3e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[53, 72, 78,  1, 80, 66, 69, 69,  1, 71, 62, 79, 62, 75]])\n",
      "You will never decide what to do if you will take to see me.\n",
      "\n",
      "I don't mind it.\n",
      "\n",
      "I wonder if it'll be to make it to the bunny today.\n",
      "\n",
      "I wonder if that happens at all.\n",
      "\n",
      "If I can continue fighting something that would be the memories of the moment.\n",
      "\n",
      "But it doesn't matter if I heard that.\n",
      "\n",
      "I don't want to hear that for you.\n",
      "\n",
      "I wonder if I can become this possible.\n",
      "\n",
      "I'm sure that I would like to hear you.\n",
      "\n",
      "I wonder if the things would look like this...\n",
      "\n",
      "I think it's the only one who continues to take a picture of him.\n",
      "\n",
      "Where's he going home?\n",
      "\n",
      "If that's the worst true power to continue the entire training companies,\n",
      "\n",
      "I would have come to the train somewhere seriously.\n",
      "\n",
      "What do you think there are someone who can come to the country of the world line before we met.\n",
      "\n",
      "The next step is the power of the country with the moment I came to the prize again.\n",
      "\n",
      "I am the super bad and she could say that.\n",
      "\n",
      "I can explode the world from the fact that she couldn't make it to the manga she was a man for a manner.\n",
      "\n",
      "I'm a go\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "idx = encode(\"You will never\")\n",
    "print(torch.tensor([idx]))\n",
    "print(decode(model.generate(idx=torch.tensor([idx], dtype=torch.long).to(device), max_new_tokens=1000, temperature=0.5, use_cache=True)[0].tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Diffusion Transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import math\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from torch.nn import functional as F\n",
    "from torch.nn.attention.flex_attention import flex_attention, create_block_mask\n",
    "import triton\n",
    "import triton.language as tl\n",
    "from tqdm.notebook import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython import display\n",
    "import wandb\n",
    "torch.manual_seed(69)\n",
    "torch.set_printoptions(profile=\"short\", sci_mode=False, linewidth=100000)\n",
    "torch.set_float32_matmul_precision('high')\n",
    "# this script is configured to run on a RTX 3060 12GB GPU. you'll want to adjust the model sizes and batch sizes for other devices\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# device = torch.device('cpu')\n",
    "plt.rcParams['figure.figsize'] = [8, 6]\n",
    "plt.rcParams['figure.dpi'] = 50\n",
    "plt.rcParams['axes.grid'] = True\n",
    "plt.rcParams['xtick.minor.visible'] = True\n",
    "plt.rcParams['ytick.minor.visible'] = True\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of dataset in characters:  39526018\n"
     ]
    }
   ],
   "source": [
    "# we use this 40mb file of concatenated anime subtitles as our dataset\n",
    "# just the right size for toy experiments like this I think\n",
    "with open('animesubs.txt', 'r', encoding='latin') as f:\n",
    "    text = f.read()\n",
    "print(\"length of dataset in characters: \", len(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Open your mind. Open your mind.\n",
      "\n",
      "Far beyond the deep blue Earth, you and I shall meet...\n",
      "\n",
      "AH! MY GODDESS\n",
      "\n",
      "A snow-white feather comes fluttering down, swaying gently in the air.\n",
      "\n",
      "Without holding back, I want to envelope you, my one and only love.\n",
      "\n",
      "I know I have the power to protect the one I love, right here in my hands.\n",
      "\n",
      "Open your mind. Just as I've always dreamed.\n",
      "\n",
      "Let the wind carry off your hopes, faraway.\n",
      "\n",
      "I have wings nobody can see. Look, you have them, too.\n",
      "\n",
      "They'll take us to where we ca\n"
     ]
    }
   ],
   "source": [
    "print(text[:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove japanese characters\n",
    "text = ''.join(filter(lambda character:ord(character) < 0x3000, text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unique characters: 86 \n",
      " !'()*+,-./0123456789:;<=>?@ABCDEFGHIJKLMNOPQRSTUVWXYZ[]_abcdefghijklmnopqrstuvwxyz|\n"
     ]
    }
   ],
   "source": [
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "print(\"unique characters:\", vocab_size, ''.join(chars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unique characters with mask token: 87 \n",
      " !'()*+,-./0123456789:;<=>?@ABCDEFGHIJKLMNOPQRSTUVWXYZ[]_abcdefghijklmnopqrstuvwxyz|~\n"
     ]
    }
   ],
   "source": [
    "# Add mask token of ~\n",
    "chars.append('~')\n",
    "chars = sorted(list(set(chars)))\n",
    "vocab_size = len(chars)\n",
    "print(\"unique characters with mask token:\", vocab_size, ''.join(chars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'\\n': 0, ' ': 1, '!': 2, \"'\": 3, '(': 4, ')': 5, '*': 6, '+': 7, ',': 8, '-': 9, '.': 10, '/': 11, '0': 12, '1': 13, '2': 14, '3': 15, '4': 16, '5': 17, '6': 18, '7': 19, '8': 20, '9': 21, ':': 22, ';': 23, '<': 24, '=': 25, '>': 26, '?': 27, '@': 28, 'A': 29, 'B': 30, 'C': 31, 'D': 32, 'E': 33, 'F': 34, 'G': 35, 'H': 36, 'I': 37, 'J': 38, 'K': 39, 'L': 40, 'M': 41, 'N': 42, 'O': 43, 'P': 44, 'Q': 45, 'R': 46, 'S': 47, 'T': 48, 'U': 49, 'V': 50, 'W': 51, 'X': 52, 'Y': 53, 'Z': 54, '[': 55, ']': 56, '_': 57, 'a': 58, 'b': 59, 'c': 60, 'd': 61, 'e': 62, 'f': 63, 'g': 64, 'h': 65, 'i': 66, 'j': 67, 'k': 68, 'l': 69, 'm': 70, 'n': 71, 'o': 72, 'p': 73, 'q': 74, 'r': 75, 's': 76, 't': 77, 'u': 78, 'v': 79, 'w': 80, 'x': 81, 'y': 82, 'z': 83, '|': 84, '~': 85, '\\x94': 86, '': 87}\n",
      "{0: '\\n', 1: ' ', 2: '!', 3: \"'\", 4: '(', 5: ')', 6: '*', 7: '+', 8: ',', 9: '-', 10: '.', 11: '/', 12: '0', 13: '1', 14: '2', 15: '3', 16: '4', 17: '5', 18: '6', 19: '7', 20: '8', 21: '9', 22: ':', 23: ';', 24: '<', 25: '=', 26: '>', 27: '?', 28: '@', 29: 'A', 30: 'B', 31: 'C', 32: 'D', 33: 'E', 34: 'F', 35: 'G', 36: 'H', 37: 'I', 38: 'J', 39: 'K', 40: 'L', 41: 'M', 42: 'N', 43: 'O', 44: 'P', 45: 'Q', 46: 'R', 47: 'S', 48: 'T', 49: 'U', 50: 'V', 51: 'W', 52: 'X', 53: 'Y', 54: 'Z', 55: '[', 56: ']', 57: '_', 58: 'a', 59: 'b', 60: 'c', 61: 'd', 62: 'e', 63: 'f', 64: 'g', 65: 'h', 66: 'i', 67: 'j', 68: 'k', 69: 'l', 70: 'm', 71: 'n', 72: 'o', 73: 'p', 74: 'q', 75: 'r', 76: 's', 77: 't', 78: 'u', 79: 'v', 80: 'w', 81: 'x', 82: 'y', 83: 'z', 84: '|', 85: '~', 86: '\\x94', 87: ''}\n",
      "encoded: [43, 73, 62, 71, 1, 82, 72, 78, 75, 1, 70, 66, 71, 61, 10, 1, 43, 73, 62, 71]\n",
      "decoded: Open your mind. Open\n",
      "vocab size: 88\n"
     ]
    }
   ],
   "source": [
    "# yes, all language models will be character level, which isn't ideal but it's good for simplicity\n",
    "# very simple tokenizer\n",
    "stoi = {ch:i for i,ch in enumerate(chars)}\n",
    "itos = {i:ch for i,ch in enumerate(chars)}\n",
    "# add special token for padding\n",
    "stoi[''] = len(stoi)\n",
    "itos[len(itos)] = ''\n",
    "print(stoi)\n",
    "print(itos)\n",
    "encode = lambda s: [stoi[ch] for ch in s]\n",
    "decode = lambda l: ''.join([itos[i] for i in l])\n",
    "print(\"encoded:\", encode(text[:20]))\n",
    "print(\"decoded:\", decode(encode(text[:20])))\n",
    "vocab_size = len(itos)\n",
    "print(\"vocab size:\", vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "85\n"
     ]
    }
   ],
   "source": [
    "# What token is my mask token?\n",
    "print(stoi[\"~\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([39526018])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = torch.tensor(encode(text), dtype=torch.int64)\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([43, 73, 62, 71,  1, 82, 72, 78, 75,  1, 70, 66, 71, 61, 10,  1, 43, 73, 62, 71,  1, 82, 72, 78, 75,  1, 70, 66, 71, 61, 10,  0,  0, 34, 58, 75,  1, 59, 62, 82, 72, 71, 61,  1, 77, 65, 62,  1, 61, 62, 62, 73,  1, 59, 69, 78, 62,  1, 33, 58, 75, 77, 65,  8,  1, 82, 72, 78,  1, 58, 71, 61,  1, 37,  1, 76, 65, 58, 69, 69,  1, 70, 62, 62, 77, 10, 10, 10,  0,  0, 29, 36,  2,  1, 41, 53,  1, 35, 43, 32])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([39130757]) torch.Size([395261])\n"
     ]
    }
   ],
   "source": [
    "n = int(len(data) * 0.99)\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]\n",
    "print(train_data.shape, val_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs:\n",
      "torch.Size([2, 64])\n",
      "tensor([[64,  1, 72, 78, 75,  1, 66, 71, 77, 62, 75, 71, 76, 65, 66, 73, 76, 10,  0,  0, 48, 65, 62,  1, 71, 66, 64, 65, 77,  1, 37, 66, 61, 58,  1, 64, 72, 77,  1, 77, 65, 62,  1, 75, 62, 76, 78, 69, 77, 76,  1, 72, 63,  1, 77, 65, 62,  1, 62, 81, 58, 70, 66, 71],\n",
      "        [62, 70,  2,  0,  0, 37, 77,  1, 69, 72, 72, 68, 76,  1, 69, 66, 68, 62,  1, 80, 62,  1, 77, 72, 72, 68,  1, 60, 58, 75, 62,  1, 72, 63,  1, 77, 65, 62, 70, 10,  0,  0, 48, 65, 72, 76, 62,  1, 64, 78, 82, 76,  1, 80, 62, 75, 62,  1, 66, 71, 60, 75, 62, 61]], device='cuda:0')\n",
      "targets:\n",
      "torch.Size([2, 64])\n",
      "tensor([[ 1, 72, 78, 75,  1, 66, 71, 77, 62, 75, 71, 76, 65, 66, 73, 76, 10,  0,  0, 48, 65, 62,  1, 71, 66, 64, 65, 77,  1, 37, 66, 61, 58,  1, 64, 72, 77,  1, 77, 65, 62,  1, 75, 62, 76, 78, 69, 77, 76,  1, 72, 63,  1, 77, 65, 62,  1, 62, 81, 58, 70, 66, 71, 58],\n",
      "        [70,  2,  0,  0, 37, 77,  1, 69, 72, 72, 68, 76,  1, 69, 66, 68, 62,  1, 80, 62,  1, 77, 72, 72, 68,  1, 60, 58, 75, 62,  1, 72, 63,  1, 77, 65, 62, 70, 10,  0,  0, 48, 65, 72, 76, 62,  1, 64, 78, 82, 76,  1, 80, 62, 75, 62,  1, 66, 71, 60, 75, 62, 61, 66]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "def get_batch(split, seq_len, batch_size=4):\n",
    "    # generate a small batch of data of inputs x and targets y\n",
    "    # targets are just inputs shifted by 1\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - seq_len, (batch_size,))\n",
    "    x = torch.stack([data[i:i+seq_len] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+seq_len+1] for i in ix])\n",
    "    return x.to(device), y.to(device)\n",
    "\n",
    "xb, yb = get_batch('train', 64, 2)\n",
    "print('inputs:')\n",
    "print(xb.shape)\n",
    "print(xb)\n",
    "print('targets:')\n",
    "print(yb.shape)\n",
    "print(yb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20480000"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Make all steps, sequence lengths, and batch size the same\n",
    "total_steps = 5000\n",
    "seq_len = 16\n",
    "batch_size = 256 # these are small models so we can use large batch sizes to fully utilize the GPU\n",
    "# should cover around 2x the dataset\n",
    "total_steps * seq_len * batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_ids: tensor([[17,  5, 18, 16,  2, 13,  8,  5],\n",
      "        [13, 19,  6, 13, 18, 15,  3,  1]])\n",
      "noisy_batch: tensor([[85,  5, 85, 16, 85, 85,  8,  5],\n",
      "        [13, 85,  6, 13, 18, 15,  3, 85]])\n",
      "masked_indices: tensor([[ True, False,  True, False,  True,  True, False, False],\n",
      "        [False,  True, False, False, False, False, False,  True]])\n",
      "p_mask: tensor([[0.48, 0.48, 0.48, 0.48, 0.48, 0.48, 0.48, 0.48],\n",
      "        [0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15]])\n"
     ]
    }
   ],
   "source": [
    "batch_size = 2\n",
    "seq_len = 8\n",
    "input_ids = torch.randint(20, (batch_size, seq_len))\n",
    "\n",
    "def forward_process(input_ids, eps: float=1e-3):\n",
    "    b, l = input_ids.shape\n",
    "    t = torch.rand(b, device=input_ids.device)\n",
    "    p_mask = (1 - eps) * t + eps\n",
    "    p_mask = p_mask[:, None].repeat(1, l)\n",
    "\n",
    "    masked_indices = torch.rand((b, l), device=input_ids.device) < p_mask\n",
    "    # 126336 is used for [MASK] token - but we use our mask token index\n",
    "    mask_token_idx = stoi[\"~\"]\n",
    "    noisy_batch = torch.where(masked_indices, mask_token_idx, input_ids)\n",
    "    return noisy_batch, masked_indices, p_mask\n",
    "\n",
    "noisy_batch, masked_indices, p_mask = forward_process(input_ids, eps=0.1)\n",
    "print(\"input_ids:\", input_ids)\n",
    "print(\"noisy_batch:\", noisy_batch)\n",
    "print(\"masked_indices:\", masked_indices)\n",
    "print(\"p_mask:\", p_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, optimizer, seq_len, batch_size, total_steps, val_steps=10, val_interval=50):\n",
    "    losses = []\n",
    "    val_losses = []\n",
    "    # live plot\n",
    "    fig, ax = plt.subplots()\n",
    "    dh = display.display(fig, display_id=True)\n",
    "    for steps in (bar := tqdm(range(total_steps))):  # increase number of steps for good results...\n",
    "        # sample a batch of data\n",
    "        xb, yb = get_batch('train', seq_len=seq_len, batch_size=batch_size)\n",
    "\n",
    "        # We set 1% of the pre-training data to a random length that is uniformly sampled from the range [1, 4096].\n",
    "        if torch.rand(1) < 0.01:\n",
    "            random_length = torch.randint(1, xb.shape[1] + 1, (1,))\n",
    "            xb = xb[:, :random_length]\n",
    "\n",
    "        noisy_batch, masked_indices, p_mask = forward_process(xb)\n",
    "\n",
    "        # evaluate the loss using standard next-token prediction\n",
    "        logits = model(noisy_batch)[0]\n",
    "        token_loss = F.cross_entropy(logits[masked_indices], xb[masked_indices], reduction='none') / p_mask[masked_indices]\n",
    "        loss = token_loss.sum() / (xb.shape[0] * xb.shape[1])\n",
    "\n",
    "        # backprop\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        bar.set_description(f\"loss: {loss.item():.2f}, val loss: {val_losses[-1] if val_losses else 0:.2f}\")\n",
    "        losses.append(loss.item())\n",
    "        if steps % val_interval == 0:\n",
    "            # Calculate validation loss\n",
    "            with torch.no_grad():\n",
    "                val_loss = 0\n",
    "                for _ in range(val_steps):\n",
    "                    xb, yb = get_batch('val', seq_len=seq_len, batch_size=batch_size)\n",
    "                    noisy_batch, masked_indices, p_mask = forward_process(xb)\n",
    "                    logits = model(noisy_batch)[0]\n",
    "                    token_loss = F.cross_entropy(logits[masked_indices], xb[masked_indices], reduction='none') / p_mask[masked_indices]\n",
    "                    val_loss_batch = token_loss.sum() / (xb.shape[0] * xb.shape[1])\n",
    "                    val_loss += val_loss_batch.item()\n",
    "                val_loss /= val_steps\n",
    "                val_losses.append(val_loss)\n",
    "            ax.clear()\n",
    "            ax.plot(losses, color='blue', label='train loss', alpha=0.7)\n",
    "            ax.plot(range(0, len(losses), val_interval), val_losses, color='red', label='val loss', alpha=0.7)\n",
    "            ax.set_ylim(1, 4)\n",
    "            ax.legend()\n",
    "            dh.update(fig)\n",
    "    print('final loss:', loss.item(), 'final val loss:', val_loss)\n",
    "\n",
    "def save_checkpoint_diffusion(model, optimizer, scheduler, step, losses, val_losses, seq_len, batch_size, total_steps, save_dir='diffusion_checkpoints'):\n",
    "    \"\"\"Save a complete checkpoint including model, optimizer, scheduler states and training metrics\"\"\"\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    \n",
    "    checkpoint = {\n",
    "        'step': step,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'scheduler_state_dict': scheduler.state_dict(),\n",
    "        'train_losses': losses,\n",
    "        'val_losses': val_losses,\n",
    "        'config': {\n",
    "            'seq_len': seq_len,\n",
    "            'batch_size': batch_size,\n",
    "            'total_steps': total_steps,\n",
    "            'vocab_size': model.token_embedding_table.num_embeddings,\n",
    "            'embed_size': model.blocks[0].sa_heads.head_size * model.head_num,\n",
    "            'head_num': model.head_num,\n",
    "            'layer_num': model.layer_num\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    checkpoint_path = os.path.join(save_dir, f'diffusion_checkpoint_step_{step}.pt')\n",
    "    torch.save(checkpoint, checkpoint_path)\n",
    "    wandb.save(checkpoint_path)\n",
    "    print(f\"Diffusion checkpoint saved at step {step}: {checkpoint_path}\")\n",
    "    \n",
    "    return checkpoint_path\n",
    "\n",
    "def load_checkpoint_diffusion(model, optimizer, scheduler, checkpoint_path):\n",
    "    \"\"\"Load a checkpoint and restore model, optimizer, scheduler states\"\"\"\n",
    "    checkpoint = torch.load(checkpoint_path, map_location=device)\n",
    "    \n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n",
    "    \n",
    "    print(f\"Diffusion checkpoint loaded from step {checkpoint['step']}\")\n",
    "    return checkpoint['step'], checkpoint['train_losses'], checkpoint['val_losses']\n",
    "\n",
    "def train_diffusion_enhanced(model, optimizer, scheduler, seq_len, batch_size, total_steps, val_steps=10, val_interval=50, checkpoint_interval=500, save_dir='diffusion_checkpoints'):\n",
    "    losses = []\n",
    "    val_losses = []\n",
    "    # Create save directory\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    \n",
    "    # live plot\n",
    "    fig, ax = plt.subplots()\n",
    "    dh = display.display(fig, display_id=True)\n",
    "    \n",
    "    for step in (bar := tqdm(range(total_steps))):\n",
    "        # sample a batch of data\n",
    "        xb, yb = get_batch('train', seq_len=seq_len, batch_size=batch_size)\n",
    "\n",
    "        # We set 1% of the pre-training data to a random length that is uniformly sampled from the range [1, 4096].\n",
    "        if torch.rand(1) < 0.01:\n",
    "            random_length = torch.randint(1, xb.shape[1] + 1, (1,))\n",
    "            xb = xb[:, :random_length]\n",
    "\n",
    "        noisy_batch, masked_indices, p_mask = forward_process(xb)\n",
    "\n",
    "        # evaluate the loss using standard next-token prediction\n",
    "        logits = model(noisy_batch)[0]\n",
    "        token_loss = F.cross_entropy(logits[masked_indices], xb[masked_indices], reduction='none') / p_mask[masked_indices]\n",
    "        loss = token_loss.sum() / (xb.shape[0] * xb.shape[1])\n",
    "\n",
    "        # backprop\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "        bar.set_description(f\"loss: {loss.item():.2f}, val loss: {val_losses[-1] if val_losses else 0:.2f}, lr: {scheduler.get_last_lr()[0]:.2e}\")\n",
    "        losses.append(loss.item())\n",
    "        \n",
    "        # Log to wandb\n",
    "        wandb.log({\n",
    "            'train_loss': loss.item(),\n",
    "            'learning_rate': scheduler.get_last_lr()[0],\n",
    "            'step': step\n",
    "        })\n",
    "        \n",
    "        if step % val_interval == 0:\n",
    "            # Calculate validation loss\n",
    "            with torch.no_grad():\n",
    "                val_loss = 0\n",
    "                for _ in range(val_steps):\n",
    "                    xb, yb = get_batch('val', seq_len=seq_len, batch_size=batch_size)\n",
    "                    noisy_batch, masked_indices, p_mask = forward_process(xb)\n",
    "                    logits = model(noisy_batch)[0]\n",
    "                    token_loss = F.cross_entropy(logits[masked_indices], xb[masked_indices], reduction='none') / p_mask[masked_indices]\n",
    "                    val_loss_batch = token_loss.sum() / (xb.shape[0] * xb.shape[1])\n",
    "                    val_loss += val_loss_batch.item()\n",
    "                val_loss /= val_steps\n",
    "                val_losses.append(val_loss)\n",
    "                \n",
    "            # Log validation loss to wandb\n",
    "            wandb.log({\n",
    "                'val_loss': val_loss,\n",
    "                'step': step\n",
    "            })\n",
    "            \n",
    "            ax.clear()\n",
    "            ax.plot(losses, color='blue', label='train loss', alpha=0.7)\n",
    "            ax.plot(range(0, len(losses), val_interval), val_losses, color='red', label='val loss', alpha=0.7)\n",
    "            ax.set_ylim(1, 4)\n",
    "            ax.legend()\n",
    "            dh.update(fig)\n",
    "            \n",
    "        # Save checkpoint\n",
    "        if step % checkpoint_interval == 0 and step > 0:\n",
    "            save_checkpoint_diffusion(model, optimizer, scheduler, step, losses, val_losses, seq_len, batch_size, total_steps, save_dir)\n",
    "            \n",
    "    print('final loss:', loss.item(), 'final val loss:', val_loss)\n",
    "    \n",
    "    # Save final checkpoint\n",
    "    save_checkpoint_diffusion(model, optimizer, scheduler, total_steps, losses, val_losses, seq_len, batch_size, total_steps, save_dir)\n",
    "    \n",
    "    return losses, val_losses\n",
    "\n",
    "def get_wandb_config_diffusion(model, optimizer, scheduler, seq_len, batch_size, total_steps):\n",
    "    \"\"\"Automatically extract wandb config from model and training parameters for diffusion transformer\"\"\"\n",
    "    config = {\n",
    "        \"architecture\": \"DiffusionTransformer\",\n",
    "        \"dataset\": \"anime-subs\",\n",
    "        \"seq_len\": seq_len,\n",
    "        \"batch_size\": batch_size,\n",
    "        \"total_steps\": total_steps,\n",
    "        \"optimizer\": optimizer.__class__.__name__,\n",
    "        \"scheduler\": scheduler.__class__.__name__,\n",
    "        \"initial_lr\": optimizer.param_groups[0]['lr'],\n",
    "        \"model_type\": \"diffusion\"\n",
    "    }\n",
    "    \n",
    "    # Add model configuration\n",
    "    config.update({\n",
    "        \"vocab_size\": model.token_embedding_table.num_embeddings,\n",
    "        \"embed_size\": model.blocks[0].sa_heads.head_size * model.head_num,\n",
    "        \"head_num\": model.head_num,\n",
    "        \"layer_num\": model.layer_num,\n",
    "        \"total_params\": sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    })\n",
    "    \n",
    "    # Add scheduler-specific config\n",
    "    if hasattr(scheduler, 'T_max'):\n",
    "        config['scheduler_T_max'] = scheduler.T_max\n",
    "    if hasattr(scheduler, 'eta_min'):\n",
    "        config['scheduler_eta_min'] = scheduler.eta_min\n",
    "    \n",
    "    return config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Measure post training perplexity on validation set\n",
    "# Create function that receives a model, context length, and PPL sequence length, and returns the perplexity\n",
    "# The PPL sequence length is the number of characters the function uses to calculate the perplexity\n",
    "# We take the logits and calculate the cross entropy loss from scratch, then exponentiate it to get the perplexity\n",
    "# not only that, but we want the models to do this in actual inference\n",
    "def perplexity(model, seq_len, ppl_seq_len, batch_size=128, val_steps=1000):\n",
    "    with torch.no_grad():\n",
    "        val_loss = 0\n",
    "        for _ in tqdm(range(val_steps)):\n",
    "            xb, yb = get_batch('val', seq_len=seq_len, batch_size=batch_size)\n",
    "            with torch.autocast(device_type=\"cuda\", dtype=torch.bfloat16):\n",
    "                logits, _ = model(xb)\n",
    "            logits = logits.reshape(batch_size, seq_len, vocab_size)\n",
    "            logits = logits[:, :ppl_seq_len]\n",
    "            yb = yb[:, :ppl_seq_len]\n",
    "            # flatten logits and targets\n",
    "            logits = logits.reshape(batch_size*ppl_seq_len, vocab_size)\n",
    "            yb = yb.reshape(batch_size*ppl_seq_len)\n",
    "            # calculate cross entropy loss from scratch\n",
    "            loss = F.cross_entropy(logits, yb)\n",
    "            val_loss += loss.item()\n",
    "        val_loss /= val_steps\n",
    "        ppl = torch.exp(torch.tensor(val_loss))\n",
    "        return ppl.item(), val_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classic Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiffusionTransformerConfig:\n",
    "    def __init__(self, vocab_size, seq_len, embed_size, head_num, layer_num):\n",
    "        self.vocab_size = vocab_size\n",
    "        self.seq_len = seq_len\n",
    "        self.embed_size = embed_size\n",
    "        self.head_num = head_num\n",
    "        self.layer_num = layer_num\n",
    "\n",
    "def get_num_transfer_tokens(mask_index, steps):\n",
    "    '''\n",
    "    In the reverse process, the interval [0, 1] is uniformly discretized into steps intervals.\n",
    "    Furthermore, because LLaDA employs a linear noise schedule (as defined in Eq. (8)),\n",
    "    the expected number of tokens transitioned at each step should be consistent.\n",
    "\n",
    "    This function is designed to precompute the number of tokens that need to be transitioned at each step.\n",
    "    '''\n",
    "    mask_num = mask_index.sum(dim=1, keepdim=True)\n",
    "\n",
    "    base = mask_num // steps\n",
    "    remainder = mask_num % steps\n",
    "\n",
    "    num_transfer_tokens = torch.zeros(mask_num.size(0), steps, device=mask_index.device, dtype=torch.int64) + base\n",
    "\n",
    "    for i in range(mask_num.size(0)):\n",
    "        num_transfer_tokens[i, :remainder[i]] += 1\n",
    "\n",
    "    return num_transfer_tokens\n",
    "\n",
    "def add_gumbel_noise(logits, temperature: float):\n",
    "    '''\n",
    "    The Gumbel max is a method for sampling categorical distributions.\n",
    "    According to arXiv:2409.02908, for MDM, low-precision Gumbel Max improves perplexity score but reduces generation quality.\n",
    "    Thus, we use float64.\n",
    "    '''\n",
    "    if temperature == 0:\n",
    "        return logits\n",
    "    logits = logits.to(torch.float64)\n",
    "    noise = torch.rand_like(logits, dtype=torch.float64)\n",
    "    gumbel_noise = (- torch.log(noise)) ** temperature\n",
    "    return logits.exp() / gumbel_noise\n",
    "\n",
    "def precompute_freqs_cis(dim: int, end: int, theta: float = 10000.0):\n",
    "    freqs = 1.0 / (theta ** (torch.arange(0, dim, 2)[: (dim // 2)].float() / dim))\n",
    "    t = torch.arange(end, device=freqs.device, dtype=torch.float32)\n",
    "    freqs = torch.outer(t, freqs)\n",
    "    freqs_cis = torch.polar(torch.ones_like(freqs), freqs)  # complex64\n",
    "    return freqs_cis.to(device)\n",
    "\n",
    "def apply_rotary_emb(\n",
    "    xq: torch.Tensor,\n",
    "    xk: torch.Tensor,\n",
    "    freqs_cis: torch.Tensor,\n",
    "):\n",
    "    xq_ = torch.view_as_complex(xq.float().reshape(*xq.shape[:-1], -1, 2))\n",
    "    xk_ = torch.view_as_complex(xk.float().reshape(*xk.shape[:-1], -1, 2))\n",
    "    # freqs_cis = reshape_for_broadcast(freqs_cis, xq_)\n",
    "    q_shape = [d if i == xq_.ndim - 2 or i == xq_.ndim - 1 else 1 for i, d in enumerate(xq_.shape)]\n",
    "    k_shape = [d if i == xq_.ndim - 2 or i == xk_.ndim - 1 else 1 for i, d in enumerate(xk_.shape)]\n",
    "    T_q = xq_.shape[-2] \n",
    "    q_freqs_cis = freqs_cis[-T_q:].view(*q_shape)\n",
    "    k_freqs_cis = freqs_cis.view(*k_shape)\n",
    "    xq_out = torch.view_as_real(xq_ * q_freqs_cis).flatten(xq.dim() - 1)\n",
    "    xk_out = torch.view_as_real(xk_ * k_freqs_cis).flatten(xq.dim() - 1)\n",
    "    return xq_out.type_as(xq), xk_out.type_as(xk)\n",
    "\n",
    "class RMSNorm(torch.nn.Module):\n",
    "    def __init__(self, dim, eps=1e-6):\n",
    "        super().__init__()\n",
    "        self.eps = eps\n",
    "        self.weight = nn.Parameter(torch.ones(dim))\n",
    "\n",
    "    def _norm(self, x):\n",
    "        return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)\n",
    "\n",
    "    def forward(self, x):\n",
    "        output = self._norm(x.float()).type_as(x)\n",
    "        return output * self.weight\n",
    "    \n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.lin_1 = nn.Linear(config.embed_size, config.embed_size*4)\n",
    "        self.lin_2 = nn.Linear(config.embed_size*4, config.embed_size)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.lin_1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.lin_2(x)\n",
    "        return x\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\" multiple heads of self-attention with AliBi in parallel \"\"\"\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.seq_len = config.seq_len\n",
    "        self.head_num = config.head_num\n",
    "        self.head_size = config.embed_size // config.head_num\n",
    "        self.key = nn.Linear(config.embed_size, config.embed_size, bias=False)\n",
    "        self.query = nn.Linear(config.embed_size, config.embed_size, bias=False)\n",
    "        self.value = nn.Linear(config.embed_size, config.embed_size, bias=False)\n",
    "        self.o = nn.Linear(config.embed_size, config.embed_size)\n",
    "        # block_mask for FlexAttention\n",
    "        def causal(b, h, q_idx, kv_idx):\n",
    "            causal_mask = q_idx >= kv_idx\n",
    "            return causal_mask\n",
    "        self.causal_mask = create_block_mask(causal, B=None, H=None, Q_LEN=config.seq_len, KV_LEN=config.seq_len)\n",
    "        self.freqs_cis = precompute_freqs_cis(config.embed_size//config.head_num, config.seq_len)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(config.seq_len, config.seq_len)))\n",
    "\n",
    "    def forward(self, x, kv_cache=None):\n",
    "        B, T, C = x.shape\n",
    "        _, _, T_past, _ = kv_cache[0].shape if kv_cache is not None and kv_cache[0] is not None else (0, 0, 0, 0)\n",
    "        q = self.query(x) # (B,T,C)\n",
    "        k = self.key(x)   # (B,T,C)\n",
    "        v = self.value(x) # (B,T,C)\n",
    "\n",
    "        # Split into heads\n",
    "        q = q.view(B, T, self.head_num, self.head_size).transpose(1, 2) # (B, H, T, C/H)\n",
    "        k = k.view(B, T, self.head_num, self.head_size).transpose(1, 2) # (B, H, T, C/H)\n",
    "        v = v.view(B, T, self.head_num, self.head_size).transpose(1, 2) # (B, H, T, C/H)\n",
    "\n",
    "        if kv_cache is not None:\n",
    "            k_past, v_past = kv_cache\n",
    "            if k_past is not None:\n",
    "                k = torch.cat((k_past, k), dim=2)\n",
    "                v = torch.cat((v_past, v), dim=2)\n",
    "            if k.shape[-2] > self.seq_len:\n",
    "                k = k[:, :, -self.seq_len:]\n",
    "                v = v[:, :, -self.seq_len:]\n",
    "            kv_cache = (k, v)\n",
    "        T_k = k.shape[-2]\n",
    "        q, k = apply_rotary_emb(q, k, self.freqs_cis[:T_k])\n",
    "\n",
    "        if T == self.seq_len:\n",
    "            out = flex_attention(q, k, v, block_mask=self.causal_mask)\n",
    "        else:\n",
    "            # compute attention scores (\"affinities\")\n",
    "            wei = q @ k.transpose(-2,-1) # (B, H, 1, C/H) @ (B, H, C/H, T) -> (B, H, 1, T)\n",
    "            wei = wei * self.head_size ** -0.5 # scaled attention\n",
    "            wei = wei.masked_fill(self.tril[T_k-T:T_k, T_k-T:T_k] == 0, float('-inf')) # (B, T, T)\n",
    "            wei = F.softmax(wei, dim=-1) # (B, H, T, T)\n",
    "            # apply attention to values\n",
    "            out = wei @ v # (B, H, 1, T) @ (B, H, T, C/H) -> (B, H, 1, C/H)\n",
    "\n",
    "        out = out.transpose(1, 2).contiguous().view(B, T, C) # (B, H, T, C/H) -> (B, T, H, C/H) -> (B, T, C)\n",
    "        out = self.o(out)\n",
    "        return out, kv_cache\n",
    "\n",
    "class Block(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.sa_heads = MultiHeadAttention(config)\n",
    "        self.ff_layer = FeedForward(config)\n",
    "        self.sa_norm = RMSNorm(config.embed_size)\n",
    "        self.ff_norm = RMSNorm(config.embed_size)\n",
    "    \n",
    "    def forward(self, x, kv_cache=None):\n",
    "        a, kv_cache = self.sa_heads(self.sa_norm(x), kv_cache)\n",
    "        h = x + a\n",
    "        o = h + self.ff_layer(self.ff_norm(h))\n",
    "        return o, kv_cache\n",
    "    \n",
    "class DiffusionTransformerLM(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.layer_num = config.layer_num\n",
    "        self.head_num = config.head_num\n",
    "        self.seq_len = config.seq_len\n",
    "        # embed raw tokens to a lower dimensional embedding with embed_size\n",
    "        self.token_embedding_table = nn.Embedding(config.vocab_size, config.embed_size)\n",
    "        # Language Modelling (?) Head is a standard linear layer to go from \n",
    "        # embeddings back to logits of vocab_size\n",
    "        self.lm_head = nn.Linear(config.embed_size, config.vocab_size)\n",
    "        # transformer blocks\n",
    "        self.blocks = nn.ModuleList([Block(config) for _ in range(config.layer_num)])\n",
    "\n",
    "    def forward(self, idx, masked_indices=None, p_mask=None, kv_cache=None):\n",
    "        B, T = idx.shape\n",
    "        _, _, T_past, _ = kv_cache[0][0].shape if kv_cache is not None and kv_cache[0][0] is not None else (0, 0, 0, 0)\n",
    "        # idx and targets are both (B,T) tensor of integers\n",
    "        tok_embd = self.token_embedding_table(idx) # (B,T,C)\n",
    "        x = tok_embd\n",
    "        # go through blocks\n",
    "        for i, block in enumerate(self.blocks):\n",
    "            x, cache = block(x, None if kv_cache is None else kv_cache[i])\n",
    "            if kv_cache is not None:\n",
    "                kv_cache[i] = cache\n",
    "        # get logits with linear layer\n",
    "        logits = self.lm_head(x) # (B,T,V)\n",
    "        \n",
    "        if masked_indices is None or p_mask is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, V = logits.shape\n",
    "            logits = logits[masked_indices]\n",
    "            targets = idx[masked_indices]\n",
    "            # logits = logits.view(B*T, V)\n",
    "            # targets = targets.view(B*T)\n",
    "            token_loss = F.cross_entropy(logits, targets, reduction=\"none\") / p_mask[masked_indices]\n",
    "            loss = token_loss.sum() / (idx.shape[0] * idx.shape[1])\n",
    "\n",
    "        return logits, loss\n",
    "    \n",
    "    @torch.no_grad\n",
    "    def generate(\n",
    "        self, \n",
    "        prompt, \n",
    "        steps: int=128, \n",
    "        gen_length: int=128, \n",
    "        block_length: int=128, \n",
    "        temperature=1, \n",
    "        cfg_scale: float=0.0, \n",
    "        remasking: str=\"low_confidence\", \n",
    "        mask_id: int=85\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            idx: A tensor of shape (1, L)\n",
    "        \"\"\"\n",
    "        x = torch.full((1, prompt.shape[1] + gen_length), mask_id, dtype=torch.long, device=prompt.device)\n",
    "        x[:, :prompt.shape[1]] = prompt.clone()\n",
    "\n",
    "        prompt_index = (x != mask_id)\n",
    "        assert gen_length % block_length == 0\n",
    "        num_blocks = gen_length // block_length\n",
    "\n",
    "        assert steps % num_blocks == 0\n",
    "        steps = steps // num_blocks\n",
    "\n",
    "        for num_block in range(num_blocks):\n",
    "            block_mask_index = (x[:, prompt.shape[1] + num_block * block_length: prompt.shape[1] + (num_block + 1) * block_length:] == mask_id)\n",
    "            num_transfer_tokens = get_num_transfer_tokens(block_mask_index, steps)\n",
    "            for i in range(steps):\n",
    "                mask_index = (x == mask_id)\n",
    "                if cfg_scale > 0.0:\n",
    "                    un_x = x.clone()\n",
    "                    un_x[prompt_index] = mask_id\n",
    "                    x_ = torch.cat([x, un_x], dim=0)\n",
    "                    logits, _ = self(x_)\n",
    "                    logits, un_logits = torch.chunk(logits, 2, dim=0)\n",
    "                    logits = un_logits + (cfg_scale + 1) * (logits - un_logits)\n",
    "                else:\n",
    "                    logits, _ = self(x)\n",
    "\n",
    "                logits_with_noise = add_gumbel_noise(logits, temperature=temperature)\n",
    "                x0 = torch.argmax(logits_with_noise, dim=-1) # B, S\n",
    "\n",
    "                if remasking == \"low_confidence\":\n",
    "                    p = F.softmax(logits, dim=-1)\n",
    "                    x0_p = torch.squeeze(\n",
    "                        torch.gather(p, dim=-1, index=torch.unsqueeze(x0, dim=-1)), dim=-1\n",
    "                    )\n",
    "                elif remasking == \"random\":\n",
    "                    x0_p = torch.rand((x0.shape[0], x0.shape[1]), device=x0.device)\n",
    "                else:\n",
    "                    raise NotImplementedError\n",
    "\n",
    "                x0_p[:, prompt.shape[1] + (num_block + 1) * block_length:] = -np.inf\n",
    "\n",
    "                x0 = torch.where(mask_index, x0, x)\n",
    "                confidence = torch.where(mask_index, x0_p, -np.inf)\n",
    "\n",
    "                transfer_index = torch.zeros_like(x0, dtype=torch.bool, device=x0.device)\n",
    "\n",
    "                for j in range(confidence.shape[0]):\n",
    "                    _, select_index = torch.topk(confidence[j], k=num_transfer_tokens[j, i])\n",
    "                    transfer_index[j, select_index] = True\n",
    "\n",
    "                x[transfer_index] = x0[transfer_index]\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[53, 72, 78,  1, 80, 66, 69, 69,  1, 71, 62, 79, 62, 75]])\n",
      "You will never________________________________5555555555555555555555555555555555555555555555i55i5ii5ii5ii5i5i5ii5ii5i5i555i5555i55i5i55i5i555i\n"
     ]
    }
   ],
   "source": [
    "# Test generate \n",
    "seq_len = 512\n",
    "config = DiffusionTransformerConfig(\n",
    "    vocab_size=vocab_size,\n",
    "    seq_len=seq_len,\n",
    "    embed_size=256,\n",
    "    head_num=4,\n",
    "    layer_num=6\n",
    ")\n",
    "m = DiffusionTransformerLM(config).to(device)\n",
    "m.eval()\n",
    "idx = encode(\"You will never\")\n",
    "print(torch.tensor([idx]))\n",
    "print(decode(m.generate(prompt=torch.tensor([idx], dtype=torch.long).to(device), steps=128, gen_length=128, block_length=32, temperature=0., cfg_scale=0., remasking='low_confidence')[0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4776024"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m.to(device)\n",
    "xb, yb = get_batch('train', 5, 1)\n",
    "logits, loss = m(xb)\n",
    "total_params = sum(p.numel() for p in m.parameters() if p.requires_grad)\n",
    "total_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize wandb with automated config for diffusion transformer\n",
    "model = DiffusionTransformerLM(config)\n",
    "model = torch.compile(model)\n",
    "model.to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=2e-3)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=total_steps, eta_min=1e-6)\n",
    "\n",
    "wandb_config = get_wandb_config_diffusion(model, optimizer, scheduler, seq_len, batch_size, total_steps)\n",
    "wandb.init(\n",
    "    project=\"transformers-playground\",\n",
    "    config=wandb_config,\n",
    "    name=\"diffusion-transformer\"\n",
    ")\n",
    "\n",
    "# Train the model with enhanced features\n",
    "losses, val_losses = train_diffusion_enhanced(model, optimizer, scheduler, seq_len, batch_size, total_steps)\n",
    "\n",
    "# Finish wandb run\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Continue training with enhanced features\n",
    "wandb.init(\n",
    "    project=\"transformers-playground\",\n",
    "    config=wandb_config,\n",
    "    name=\"diffusion-transformer-continued\",\n",
    "    resume=True\n",
    ")\n",
    "\n",
    "train_diffusion_enhanced(model, optimizer, scheduler, seq_len, batch_size, total_steps=5000 * 3)\n",
    "\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[53, 72, 78,  1, 80, 66, 69, 69,  1, 71, 62, 79, 62, 75]])\n",
      "You will never bet aour Yny  y\n",
      "\n",
      "I'l sou whe  you wee  to sour ca dour aane \n",
      "\n",
      "You'le be  aee  yo r  ta s  ao t wour sese.\n",
      "\n",
      "Iou're no sood th  i\n"
     ]
    }
   ],
   "source": [
    "idx = encode(\"You will never\")\n",
    "print(torch.tensor([idx]))\n",
    "print(decode(model.generate(prompt=torch.tensor([idx], dtype=torch.long).to(device), steps=256, gen_length=128, block_length=32, temperature=1, cfg_scale=0.9, remasking='low_confidence')[0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save\n",
    "torch.save(model.state_dict(), 'models/DiffusionTransformerLM_20000.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 400x300 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a26a43575aa42ad9d267e9bd6c190be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/15000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final loss: 2.123497724533081 final val loss: 2.1927693367004393\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 400x300 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train(model, optimizer, seq_len, batch_size, total_steps=5000 * 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[53, 72, 78,  1, 80, 66, 69, 69,  1, 71, 62, 79, 62, 75]])\n",
      "You will never be able to seal   to  the t ao ao toes the  wor w ta   th   th tour tr toes\n",
      "\n",
      "  wo t take  to   to  th t   to the  to    the tr \n"
     ]
    }
   ],
   "source": [
    "idx = encode(\"You will never\")\n",
    "print(torch.tensor([idx]))\n",
    "print(decode(model.generate(prompt=torch.tensor([idx], dtype=torch.long).to(device), steps=128, gen_length=128, block_length=32, temperature=0., cfg_scale=0., remasking='random')[0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'models/DiffusionTransformerLM_35000.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "for g in optimizer.param_groups:\n",
    "    g['lr'] = 1e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final training phase with enhanced features\n",
    "for g in optimizer.param_groups:\n",
    "    g['lr'] = 1e-4\n",
    "\n",
    "wandb.init(\n",
    "    project=\"transformers-playground\",\n",
    "    config=wandb_config,\n",
    "    name=\"diffusion-transformer-final\",\n",
    "    resume=True\n",
    ")\n",
    "\n",
    "train_diffusion_enhanced(model, optimizer, scheduler, seq_len, batch_size, total_steps=5000)\n",
    "\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[53, 72, 78,  1, 80, 66, 69, 69,  1, 71, 62, 79, 62, 75]])\n",
      "You will never be able to see  t iane ao tou. You wan o lelp you oo tet yoe to dee you as m ce so hou won't help yeu?\n",
      "\n",
      "Iou wou no w  yo te ao \n"
     ]
    }
   ],
   "source": [
    "idx = encode(\"You will never\")\n",
    "print(torch.tensor([idx]))\n",
    "print(decode(model.generate(prompt=torch.tensor([idx], dtype=torch.long).to(device), steps=256, gen_length=128, block_length=32, temperature=1, cfg_scale=0.9, remasking='low_confidence')[0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_28159/96251971.py:4: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load('models/TransformerLM.pt'))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "OptimizedModule(\n",
       "  (_orig_mod): TransformerLM(\n",
       "    (token_embedding_table): Embedding(87, 256)\n",
       "    (lm_head): Linear(in_features=256, out_features=87, bias=True)\n",
       "    (blocks): ModuleList(\n",
       "      (0-5): 6 x Block(\n",
       "        (sa_heads): MultiHeadAttention(\n",
       "          (key): Linear(in_features=256, out_features=256, bias=False)\n",
       "          (query): Linear(in_features=256, out_features=256, bias=False)\n",
       "          (value): Linear(in_features=256, out_features=256, bias=False)\n",
       "          (o): Linear(in_features=256, out_features=256, bias=True)\n",
       "        )\n",
       "        (ff_layer): FeedForward(\n",
       "          (lin_1): Linear(in_features=256, out_features=1024, bias=True)\n",
       "          (lin_2): Linear(in_features=1024, out_features=256, bias=True)\n",
       "          (relu): ReLU()\n",
       "        )\n",
       "        (sa_norm): RMSNorm()\n",
       "        (ff_norm): RMSNorm()\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load model\n",
    "model = DiffusionTransformerLM(config)\n",
    "model = torch.compile(model)\n",
    "model.load_state_dict(torch.load('models/TransformerLM.pt'))\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f1e6d09745c645cab97ce2a92133cf0c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "perplexity: 3.334986925125122 loss: 1.20446875\n"
     ]
    }
   ],
   "source": [
    "# calculate perplexity\n",
    "ppl, loss = perplexity(model, seq_len, seq_len)\n",
    "print(\"perplexity:\", ppl, \"loss:\", loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[20]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mmodel\u001b[49m.eval()\n\u001b[32m      2\u001b[39m idx = encode(\u001b[33m\"\u001b[39m\u001b[33mYou will never\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      3\u001b[39m \u001b[38;5;28mprint\u001b[39m(torch.tensor([idx]))\n",
      "\u001b[31mNameError\u001b[39m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "idx = encode(\"You will never\")\n",
    "print(torch.tensor([idx]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[53, 72, 78,  1, 80, 66, 69, 69,  1, 71, 62, 79, 62, 75]])\n",
      "You will never decide what to do if you will take to see me.\n",
      "\n",
      "I don't mind it.\n",
      "\n",
      "I wonder if it'll be to make it to the bunny today.\n",
      "\n",
      "I wonder if that happens at all.\n",
      "\n",
      "If I can continue fighting something that would be the memories of the moment.\n",
      "\n",
      "But it doesn't matter if I heard that.\n",
      "\n",
      "I don't want to hear that for you.\n",
      "\n",
      "I wonder if I can become this possible.\n",
      "\n",
      "I'm sure that I would like to hear you.\n",
      "\n",
      "I wonder if the things would look like this...\n",
      "\n",
      "I think it's the only one who continues to take a picture of him.\n",
      "\n",
      "Where's he going home?\n",
      "\n",
      "If that's the worst true power to continue the entire training companies,\n",
      "\n",
      "I would have come to the train somewhere seriously.\n",
      "\n",
      "What do you think there are someone who can come to the country of the world line before we met.\n",
      "\n",
      "The next step is the power of the country with the moment I came to the prize again.\n",
      "\n",
      "I am the super bad and she could say that.\n",
      "\n",
      "I can explode the world from the fact that she couldn't make it to the manga she was a man for a manner.\n",
      "\n",
      "I'm a go\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "idx = encode(\"You will never\")\n",
    "print(torch.tensor([idx]))\n",
    "print(decode(model.generate(prompt=torch.tensor([idx], dtype=torch.long).to(device), steps=128, gen_length=1000, block_length=128, temperature=0.5, cfg_scale=0., remasking='low_confidence')[0].tolist()))\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
