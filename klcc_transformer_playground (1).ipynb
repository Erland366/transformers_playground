{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "sdtDsu1Y0EqL"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import os\n",
        "import re\n",
        "import math\n",
        "import random\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "from torch.nn.attention.flex_attention import flex_attention, create_block_mask\n",
        "import triton\n",
        "import triton.language as tl\n",
        "from tqdm.notebook import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython import display\n",
        "torch.manual_seed(69)\n",
        "torch.set_printoptions(profile=\"short\", sci_mode=False, linewidth=100000)\n",
        "torch.set_float32_matmul_precision('high')\n",
        "# this script is configured to run on a RTX 3060 12GB GPU. you'll want to adjust the model sizes and batch sizes for other devices\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "# device = torch.device('cpu')\n",
        "plt.rcParams['figure.figsize'] = [8, 6]\n",
        "plt.rcParams['figure.dpi'] = 50\n",
        "plt.rcParams['axes.grid'] = True\n",
        "plt.rcParams['xtick.minor.visible'] = True\n",
        "plt.rcParams['ytick.minor.visible'] = True\n",
        "# make 'models' folder to save trained models if it doesn't exist\n",
        "os.makedirs('models', exist_ok=True)\n",
        "device"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Data Prep"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ANr5dn7W0EqR",
        "outputId": "a00cbe8d-e1c6-454b-b552-4ffd17ce17e5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "length of dataset in characters:  39526018\n"
          ]
        }
      ],
      "source": [
        "# we use this 40mb file of concatenated anime subtitles as our dataset\n",
        "# just the right size for toy experiments like this I think\n",
        "with open('animesubs.txt', 'r', encoding='latin') as f:\n",
        "    text = f.read()\n",
        "print(\"length of dataset in characters: \", len(text))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pANiObIZ0EqU",
        "outputId": "98f70469-1c89-4341-89e8-c142a14331b1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Open your mind. Open your mind.\n",
            "\n",
            "Far beyond the deep blue Earth, you and I shall meet...\n",
            "\n",
            "AH! MY GODDESS\n",
            "\n",
            "A snow-white feather comes fluttering down, swaying gently in the air.\n",
            "\n",
            "Without holding back, I want to envelope you, my one and only love.\n",
            "\n",
            "I know I have the power to protect the one I love, right here in my hands.\n",
            "\n",
            "Open your mind. Just as I've always dreamed.\n",
            "\n",
            "Let the wind carry off your hopes, faraway.\n",
            "\n",
            "I have wings nobody can see. Look, you have them, too.\n",
            "\n",
            "They'll take us to where we ca\n"
          ]
        }
      ],
      "source": [
        "print(text[:500])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "WvM5h6_i3KsM"
      },
      "outputs": [],
      "source": [
        "# remove japanese characters\n",
        "text = ''.join(filter(lambda character:ord(character) < 0x3000, text))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U7SOcWJM0EqW",
        "outputId": "a82a4da8-a8ed-47bf-cfb5-2e4c59dee2cf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "unique characters: 86 \n",
            " !'()*+,-./0123456789:;<=>?@ABCDEFGHIJKLMNOPQRSTUVWXYZ[]_abcdefghijklmnopqrstuvwxyz|Â”\n"
          ]
        }
      ],
      "source": [
        "chars = sorted(list(set(text)))\n",
        "vocab_size = len(chars)\n",
        "print(\"unique characters:\", vocab_size, ''.join(chars))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yobmmaeK0EqX",
        "outputId": "26669f38-4b26-4b53-f642-ee7543e7c578"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'\\n': 0, ' ': 1, '!': 2, \"'\": 3, '(': 4, ')': 5, '*': 6, '+': 7, ',': 8, '-': 9, '.': 10, '/': 11, '0': 12, '1': 13, '2': 14, '3': 15, '4': 16, '5': 17, '6': 18, '7': 19, '8': 20, '9': 21, ':': 22, ';': 23, '<': 24, '=': 25, '>': 26, '?': 27, '@': 28, 'A': 29, 'B': 30, 'C': 31, 'D': 32, 'E': 33, 'F': 34, 'G': 35, 'H': 36, 'I': 37, 'J': 38, 'K': 39, 'L': 40, 'M': 41, 'N': 42, 'O': 43, 'P': 44, 'Q': 45, 'R': 46, 'S': 47, 'T': 48, 'U': 49, 'V': 50, 'W': 51, 'X': 52, 'Y': 53, 'Z': 54, '[': 55, ']': 56, '_': 57, 'a': 58, 'b': 59, 'c': 60, 'd': 61, 'e': 62, 'f': 63, 'g': 64, 'h': 65, 'i': 66, 'j': 67, 'k': 68, 'l': 69, 'm': 70, 'n': 71, 'o': 72, 'p': 73, 'q': 74, 'r': 75, 's': 76, 't': 77, 'u': 78, 'v': 79, 'w': 80, 'x': 81, 'y': 82, 'z': 83, '|': 84, '\\x94': 85, '': 86}\n",
            "{0: '\\n', 1: ' ', 2: '!', 3: \"'\", 4: '(', 5: ')', 6: '*', 7: '+', 8: ',', 9: '-', 10: '.', 11: '/', 12: '0', 13: '1', 14: '2', 15: '3', 16: '4', 17: '5', 18: '6', 19: '7', 20: '8', 21: '9', 22: ':', 23: ';', 24: '<', 25: '=', 26: '>', 27: '?', 28: '@', 29: 'A', 30: 'B', 31: 'C', 32: 'D', 33: 'E', 34: 'F', 35: 'G', 36: 'H', 37: 'I', 38: 'J', 39: 'K', 40: 'L', 41: 'M', 42: 'N', 43: 'O', 44: 'P', 45: 'Q', 46: 'R', 47: 'S', 48: 'T', 49: 'U', 50: 'V', 51: 'W', 52: 'X', 53: 'Y', 54: 'Z', 55: '[', 56: ']', 57: '_', 58: 'a', 59: 'b', 60: 'c', 61: 'd', 62: 'e', 63: 'f', 64: 'g', 65: 'h', 66: 'i', 67: 'j', 68: 'k', 69: 'l', 70: 'm', 71: 'n', 72: 'o', 73: 'p', 74: 'q', 75: 'r', 76: 's', 77: 't', 78: 'u', 79: 'v', 80: 'w', 81: 'x', 82: 'y', 83: 'z', 84: '|', 85: '\\x94', 86: ''}\n",
            "encoded: [43, 73, 62, 71, 1, 82, 72, 78, 75, 1, 70, 66, 71, 61, 10, 1, 43, 73, 62, 71]\n",
            "decoded: Open your mind. Open\n",
            "vocab size: 87\n"
          ]
        }
      ],
      "source": [
        "# yes, all language models will be character level, which isn't ideal but it's good for simplicity\n",
        "# very simple tokenizer\n",
        "stoi = {ch:i for i,ch in enumerate(chars)}\n",
        "itos = {i:ch for i,ch in enumerate(chars)}\n",
        "# add special token for padding\n",
        "stoi[''] = len(stoi)\n",
        "itos[len(itos)] = ''\n",
        "print(stoi)\n",
        "print(itos)\n",
        "encode = lambda s: [stoi[ch] for ch in s]\n",
        "decode = lambda l: ''.join([itos[i] for i in l])\n",
        "print(\"encoded:\", encode(text[:20]))\n",
        "print(\"decoded:\", decode(encode(text[:20])))\n",
        "vocab_size = len(itos)\n",
        "print(\"vocab size:\", vocab_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6Pnf9KfP0EqY",
        "outputId": "ff581168-335a-4f0f-efeb-0d4bd5b225ca"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([39526018])"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "data = torch.tensor(encode(text), dtype=torch.int64)\n",
        "data.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lZ2fY1pR0EqY",
        "outputId": "5eb599e0-fb79-4cdb-c4b9-52a781d67151"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([43, 73, 62, 71,  1, 82, 72, 78, 75,  1, 70, 66, 71, 61, 10,  1, 43, 73, 62, 71,  1, 82, 72, 78, 75,  1, 70, 66, 71, 61, 10,  0,  0, 34, 58, 75,  1, 59, 62, 82, 72, 71, 61,  1, 77, 65, 62,  1, 61, 62, 62, 73,  1, 59, 69, 78, 62,  1, 33, 58, 75, 77, 65,  8,  1, 82, 72, 78,  1, 58, 71, 61,  1, 37,  1, 76, 65, 58, 69, 69,  1, 70, 62, 62, 77, 10, 10, 10,  0,  0, 29, 36,  2,  1, 41, 53,  1, 35, 43, 32])"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "data[:100]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oIaYesPh0Eqa",
        "outputId": "caa27cbb-5ae3-4406-8194-646264d4ba99"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([39130757]) torch.Size([395261])\n"
          ]
        }
      ],
      "source": [
        "n = int(len(data) * 0.99)\n",
        "train_data = data[:n]\n",
        "val_data = data[n:]\n",
        "print(train_data.shape, val_data.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4bFhizcI0Eqa",
        "outputId": "c93a4d43-6fb2-4de7-aefc-8fed47e4a861"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([43, 73, 62, 71,  1, 82, 72, 78, 75])"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "seq_len = 8\n",
        "train_data[:seq_len+1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "skFCPvQC0Eqc",
        "outputId": "08990c5b-88af-4a51-ce37-3c59989d6d74"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "inputs:\n",
            "torch.Size([2, 64])\n",
            "tensor([[64,  1, 72, 78, 75,  1, 66, 71, 77, 62, 75, 71, 76, 65, 66, 73, 76, 10,  0,  0, 48, 65, 62,  1, 71, 66, 64, 65, 77,  1, 37, 66, 61, 58,  1, 64, 72, 77,  1, 77, 65, 62,  1, 75, 62, 76, 78, 69, 77, 76,  1, 72, 63,  1, 77, 65, 62,  1, 62, 81, 58, 70, 66, 71],\n",
            "        [62, 70,  2,  0,  0, 37, 77,  1, 69, 72, 72, 68, 76,  1, 69, 66, 68, 62,  1, 80, 62,  1, 77, 72, 72, 68,  1, 60, 58, 75, 62,  1, 72, 63,  1, 77, 65, 62, 70, 10,  0,  0, 48, 65, 72, 76, 62,  1, 64, 78, 82, 76,  1, 80, 62, 75, 62,  1, 66, 71, 60, 75, 62, 61]], device='cuda:0')\n",
            "targets:\n",
            "torch.Size([2, 64])\n",
            "tensor([[ 1, 72, 78, 75,  1, 66, 71, 77, 62, 75, 71, 76, 65, 66, 73, 76, 10,  0,  0, 48, 65, 62,  1, 71, 66, 64, 65, 77,  1, 37, 66, 61, 58,  1, 64, 72, 77,  1, 77, 65, 62,  1, 75, 62, 76, 78, 69, 77, 76,  1, 72, 63,  1, 77, 65, 62,  1, 62, 81, 58, 70, 66, 71, 58],\n",
            "        [70,  2,  0,  0, 37, 77,  1, 69, 72, 72, 68, 76,  1, 69, 66, 68, 62,  1, 80, 62,  1, 77, 72, 72, 68,  1, 60, 58, 75, 62,  1, 72, 63,  1, 77, 65, 62, 70, 10,  0,  0, 48, 65, 72, 76, 62,  1, 64, 78, 82, 76,  1, 80, 62, 75, 62,  1, 66, 71, 60, 75, 62, 61, 66]], device='cuda:0')\n"
          ]
        }
      ],
      "source": [
        "def get_batch(split, seq_len, batch_size=4):\n",
        "    # generate a small batch of data of inputs x and targets y\n",
        "    # targets are just inputs shifted by 1\n",
        "    data = train_data if split == 'train' else val_data\n",
        "    ix = torch.randint(len(data) - seq_len, (batch_size,))\n",
        "    x = torch.stack([data[i:i+seq_len] for i in ix])\n",
        "    y = torch.stack([data[i+1:i+seq_len+1] for i in ix])\n",
        "    return x.to(device), y.to(device)\n",
        "\n",
        "xb, yb = get_batch('train', 64, 2)\n",
        "print('inputs:')\n",
        "print(xb.shape)\n",
        "print(xb)\n",
        "print('targets:')\n",
        "print(yb.shape)\n",
        "print(yb)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Training Prep"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "327680000"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Make all steps, sequence lengths, and batch size the same\n",
        "total_steps = 5000\n",
        "seq_len = 256\n",
        "batch_size = 256 # these are small models so we can use large batch sizes to fully utilize the GPU\n",
        "# should cover around 2x the dataset\n",
        "total_steps * seq_len * batch_size"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [],
      "source": [
        "def train(model, optimizer, seq_len, batch_size, total_steps, val_steps=10, val_interval=50):\n",
        "    losses = []\n",
        "    val_losses = []\n",
        "    # live plot\n",
        "    fig, ax = plt.subplots()\n",
        "    dh = display.display(fig, display_id=True)\n",
        "    for steps in (bar := tqdm(range(total_steps))):  # increase number of steps for good results...\n",
        "        # sample a batch of data\n",
        "        xb, yb = get_batch('train', seq_len=seq_len, batch_size=batch_size)\n",
        "\n",
        "        # evaluate the loss\n",
        "        logits, loss = model(xb, yb)\n",
        "\n",
        "        # backprop\n",
        "        optimizer.zero_grad(set_to_none=True)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        bar.set_description(f\"loss: {loss.item():.2f}, val loss: {val_losses[-1] if val_losses else 0:.2f}\")\n",
        "        losses.append(loss.item())\n",
        "        if steps % val_interval == 0:\n",
        "            # Calculate validation loss\n",
        "            with torch.no_grad():\n",
        "                val_loss = 0\n",
        "                for _ in range(val_steps):\n",
        "                    xb, yb = get_batch('val', seq_len=seq_len, batch_size=batch_size)\n",
        "                    _, loss = model(xb, yb)\n",
        "                    val_loss += loss.item()\n",
        "                val_loss /= val_steps\n",
        "                val_losses.append(val_loss)\n",
        "            ax.clear()\n",
        "            ax.plot(losses, color='blue', label='train loss', alpha=0.7)\n",
        "            ax.plot(range(0, len(losses), val_interval), val_losses, color='red', label='val loss', alpha=0.7)\n",
        "            ax.set_ylim(0, 4)\n",
        "            ax.legend()\n",
        "            dh.update(fig)\n",
        "    print('final loss:', loss.item(), 'final val loss:', val_loss)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Measure post training perplexity on validation set\n",
        "# Create function that receives a model, context length, and PPL sequence length, and returns the perplexity\n",
        "# The PPL sequence length is the number of characters the function uses to calculate the perplexity\n",
        "# We take the logits and calculate the cross entropy loss from scratch, then exponentiate it to get the perplexity\n",
        "# not only that, but we want the models to do this in actual inference\n",
        "def perplexity(model, seq_len, ppl_seq_len, batch_size=128, val_steps=1000):\n",
        "    with torch.no_grad():\n",
        "        val_loss = 0\n",
        "        for _ in tqdm(range(val_steps)):\n",
        "            xb, yb = get_batch('val', seq_len=seq_len, batch_size=batch_size)\n",
        "            with torch.autocast(device_type=\"cuda\", dtype=torch.bfloat16):\n",
        "                logits, _ = model(xb, yb)\n",
        "            logits = logits.reshape(batch_size, seq_len, vocab_size)\n",
        "            logits = logits[:, :ppl_seq_len]\n",
        "            yb = yb[:, :ppl_seq_len]\n",
        "            # flatten logits and targets\n",
        "            logits = logits.reshape(batch_size*ppl_seq_len, vocab_size)\n",
        "            yb = yb.reshape(batch_size*ppl_seq_len)\n",
        "            # calculate cross entropy loss from scratch\n",
        "            loss = F.cross_entropy(logits, yb)\n",
        "            val_loss += loss.item()\n",
        "        val_loss /= val_steps\n",
        "        ppl = torch.exp(torch.tensor(val_loss))\n",
        "        return ppl.item(), val_loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Transformers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Classic Transformer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k6cA2WbrayyL"
      },
      "outputs": [],
      "source": [
        "class TransformerConfig:\n",
        "    def __init__(self, vocab_size, seq_len, embed_size, head_num, layer_num, hidden_mult=4):\n",
        "        self.vocab_size = vocab_size\n",
        "        self.seq_len = seq_len\n",
        "        self.embed_size = embed_size\n",
        "        self.head_num = head_num\n",
        "        self.layer_num = layer_num\n",
        "        self.hidden_mult = hidden_mult\n",
        "\n",
        "def precompute_freqs_cis(dim: int, end: int, theta: float = 10000.0):\n",
        "    freqs = 1.0 / (theta ** (torch.arange(0, dim, 2)[: (dim // 2)].float() / dim))\n",
        "    t = torch.arange(end, device=freqs.device, dtype=torch.float32)\n",
        "    freqs = torch.outer(t, freqs)\n",
        "    freqs_cis = torch.polar(torch.ones_like(freqs), freqs)  # complex64\n",
        "    return freqs_cis.to(device)\n",
        "\n",
        "def apply_rotary_emb(\n",
        "    xq: torch.Tensor,\n",
        "    xk: torch.Tensor,\n",
        "    freqs_cis: torch.Tensor,\n",
        "):\n",
        "    xq_ = torch.view_as_complex(xq.float().reshape(*xq.shape[:-1], -1, 2))\n",
        "    xk_ = torch.view_as_complex(xk.float().reshape(*xk.shape[:-1], -1, 2))\n",
        "    # freqs_cis = reshape_for_broadcast(freqs_cis, xq_)\n",
        "    q_shape = [d if i == xq_.ndim - 2 or i == xq_.ndim - 1 else 1 for i, d in enumerate(xq_.shape)]\n",
        "    k_shape = [d if i == xq_.ndim - 2 or i == xk_.ndim - 1 else 1 for i, d in enumerate(xk_.shape)]\n",
        "    T_q = xq_.shape[-2] \n",
        "    q_freqs_cis = freqs_cis[-T_q:].view(*q_shape)\n",
        "    k_freqs_cis = freqs_cis.view(*k_shape)\n",
        "    xq_out = torch.view_as_real(xq_ * q_freqs_cis).flatten(xq.dim() - 1)\n",
        "    xk_out = torch.view_as_real(xk_ * k_freqs_cis).flatten(xq.dim() - 1)\n",
        "    return xq_out.type_as(xq), xk_out.type_as(xk)\n",
        "\n",
        "class RMSNorm(torch.nn.Module):\n",
        "    def __init__(self, dim, eps=1e-6):\n",
        "        super().__init__()\n",
        "        self.eps = eps\n",
        "        self.weight = nn.Parameter(torch.ones(dim))\n",
        "\n",
        "    def _norm(self, x):\n",
        "        return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)\n",
        "\n",
        "    def forward(self, x):\n",
        "        output = self._norm(x.float()).type_as(x)\n",
        "        return output * self.weight\n",
        "    \n",
        "class FeedForward(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.lin_1 = nn.Linear(config.embed_size, config.embed_size*config.hidden_mult, bias=False)\n",
        "        self.lin_2 = nn.Linear(config.embed_size, config.embed_size*config.hidden_mult, bias=False)\n",
        "        self.lin_3 = nn.Linear(config.embed_size*config.hidden_mult, config.embed_size, bias=False)\n",
        "        self.silu = nn.SiLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x1 = self.lin_1(x)\n",
        "        x2 = self.lin_2(x)\n",
        "        x = self.silu(x1) * x2\n",
        "        x = self.lin_3(x)\n",
        "        return x\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    \"\"\" multiple heads of self-attention in parallel \"\"\"\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.seq_len = config.seq_len\n",
        "        self.head_num = config.head_num\n",
        "        self.head_size = config.embed_size // config.head_num\n",
        "        self.query = nn.Linear(config.embed_size, config.embed_size, bias=False)\n",
        "        self.key = nn.Linear(config.embed_size, config.embed_size, bias=False)\n",
        "        self.value = nn.Linear(config.embed_size, config.embed_size, bias=False)\n",
        "        self.o = nn.Linear(config.embed_size, config.embed_size, bias=False)\n",
        "        # block_mask for FlexAttention\n",
        "        def causal(b, h, q_idx, kv_idx):\n",
        "            causal_mask = q_idx >= kv_idx\n",
        "            return causal_mask\n",
        "        self.causal_mask = create_block_mask(causal, B=None, H=None, Q_LEN=config.seq_len, KV_LEN=config.seq_len)\n",
        "        self.freqs_cis = precompute_freqs_cis(config.embed_size//config.head_num, config.seq_len)\n",
        "        self.register_buffer('tril', torch.tril(torch.ones(config.seq_len, config.seq_len)))\n",
        "\n",
        "    def forward(self, x, kv_cache=None):\n",
        "        B, T, C = x.shape\n",
        "        _, _, T_past, _ = kv_cache[0].shape if kv_cache is not None and kv_cache[0] is not None else (0, 0, 0, 0)\n",
        "        q = self.query(x) # (B, T, C)\n",
        "        k = self.key(x)   # (B, T, C)\n",
        "        v = self.value(x) # (B, T, C)\n",
        "\n",
        "        # Split into heads\n",
        "        q = q.view(B, T, self.head_num, self.head_size).transpose(1, 2) # (B, H, T, C/H)\n",
        "        k = k.view(B, T, self.head_num, self.head_size).transpose(1, 2) # (B, H, T, C/H)\n",
        "        v = v.view(B, T, self.head_num, self.head_size).transpose(1, 2) # (B, H, T, C/H)\n",
        "\n",
        "        if kv_cache is not None:\n",
        "            k_past, v_past = kv_cache\n",
        "            if k_past is not None:\n",
        "                k = torch.cat((k_past, k), dim=2)\n",
        "                v = torch.cat((v_past, v), dim=2)\n",
        "            if k.shape[-2] > self.seq_len:\n",
        "                k = k[:, :, -self.seq_len:]\n",
        "                v = v[:, :, -self.seq_len:]\n",
        "            kv_cache = (k, v)\n",
        "        T_k = k.shape[-2]\n",
        "        q, k = apply_rotary_emb(q, k, self.freqs_cis[:T_k])\n",
        "\n",
        "        if T == self.seq_len: # only in training\n",
        "            out = flex_attention(q, k, v, block_mask=self.causal_mask)\n",
        "        else:\n",
        "            # compute attention scores (\"affinities\")\n",
        "            wei = q @ k.transpose(-2,-1) # (B, H, Tq, C/H) @ (B, H, C/H, Tk) -> (B, H, Tq, Tk)\n",
        "            wei = wei * self.head_size ** -0.5 # scaled attention\n",
        "            wei = wei.masked_fill(self.tril[T_k-T:T_k, :T_k] == 0, float('-inf')) # (B, Tq, Tk)\n",
        "            wei = F.softmax(wei, dim=-1) # (B, H, Tq, Tk)\n",
        "            # apply attention to values\n",
        "            out = wei @ v # (B, H, Tq, Tk) @ (B, H, Tk, C/H) -> (B, H, Tq, C/H)\n",
        "\n",
        "        out = out.transpose(1, 2).contiguous().view(B, T, C) # (B, H, Tq, C/H) -> (B, Tq, H, C/H) -> (B, Tq, C)\n",
        "        out = self.o(out)\n",
        "        return out, kv_cache\n",
        "\n",
        "class Block(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.sa_heads = MultiHeadAttention(config)\n",
        "        self.ff_layer = FeedForward(config)\n",
        "        self.sa_norm = RMSNorm(config.embed_size)\n",
        "        self.ff_norm = RMSNorm(config.embed_size)\n",
        "    \n",
        "    def forward(self, x, kv_cache=None):\n",
        "        a, kv_cache = self.sa_heads(self.sa_norm(x), kv_cache)\n",
        "        h = x + a\n",
        "        o = h + self.ff_layer(self.ff_norm(h))\n",
        "        return o, kv_cache\n",
        "    \n",
        "class TransformerLM(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.layer_num = config.layer_num\n",
        "        self.head_num = config.head_num\n",
        "        self.seq_len = config.seq_len\n",
        "        # embed raw tokens to a lower dimensional embedding with embed_size\n",
        "        self.token_embedding_table = nn.Embedding(config.vocab_size, config.embed_size)\n",
        "        # Language Modelling (?) Head is a standard linear layer to go from \n",
        "        # embeddings back to logits of vocab_size\n",
        "        self.lm_head = nn.Linear(config.embed_size, config.vocab_size, bias=False)\n",
        "        # transformer blocks\n",
        "        self.blocks = nn.ModuleList([Block(config) for _ in range(config.layer_num)])\n",
        "\n",
        "    def forward(self, idx, targets=None, kv_cache=None):\n",
        "        B, T = idx.shape\n",
        "        # print(\"Input shape:\", idx.shape)\n",
        "        # print(\"KV cache shape:\", kv_cache[0][0].shape if kv_cache is not None and kv_cache[0][0] is not None else None)\n",
        "        _, _, T_past, _ = kv_cache[0][0].shape if kv_cache is not None and kv_cache[0][0] is not None else (0, 0, 0, 0)\n",
        "        # idx and targets are both (B,T) tensor of integers\n",
        "        tok_embd = self.token_embedding_table(idx) # (B,T,C)\n",
        "        x = tok_embd\n",
        "        # go through blocks\n",
        "        for i, block in enumerate(self.blocks):\n",
        "            x, cache = block(x, None if kv_cache is None else kv_cache[i])\n",
        "            if kv_cache is not None:\n",
        "                kv_cache[i] = cache\n",
        "        # get logits with linear layer\n",
        "        logits = self.lm_head(x) # (B,T,V)\n",
        "        \n",
        "        if targets is None:\n",
        "            loss = None\n",
        "        else:\n",
        "            B, T, V = logits.shape\n",
        "            logits = logits.view(B*T, V)\n",
        "            targets = targets.view(B*T)\n",
        "            loss = F.cross_entropy(logits, targets)\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    def generate(\n",
        "        self, \n",
        "        idx, \n",
        "        max_new_tokens, \n",
        "        temperature=1, \n",
        "        use_cache=True, \n",
        "        use_klcc=False, \n",
        "        klcc_cutoff=None, \n",
        "        klcc_window_size=None, \n",
        "        klcc_steps=None, \n",
        "        klcc_steps_window_size=None, \n",
        "        klcc_lr=1e-5\n",
        "    ):\n",
        "        if use_cache:\n",
        "            # initialize key-value cache\n",
        "            kv_cache = [(None, None) for _ in range(self.layer_num)]\n",
        "            # idx is (B, T) array of indices in the current context\n",
        "            # crop idx to the last seq_len tokens\n",
        "            idx_context = idx[:, -self.seq_len:]\n",
        "            if use_klcc:\n",
        "                all_logits = None\n",
        "            for _ in (bar := tqdm(range(max_new_tokens))):\n",
        "                # get the predictions\n",
        "                logits, loss = self(idx_context, kv_cache=kv_cache)\n",
        "                if use_klcc:\n",
        "                    all_logits = logits if all_logits is None else torch.cat((all_logits, logits), dim=1)\n",
        "                # focus only on the last time step\n",
        "                logits = logits[:, -1, :] # becomes (B, C)\n",
        "                # apply temperature\n",
        "                logits = logits / temperature if temperature > 0 else logits\n",
        "                # apply softmax to get probabilities\n",
        "                probs = F.softmax(logits, dim=-1) # (B, C)\n",
        "                # sample from the distribution\n",
        "                idx_next = torch.multinomial(probs, num_samples=1) if temperature > 0 else torch.argmax(probs, dim=-1, keepdim=True) # (B, 1)\n",
        "                # do KL context compression\n",
        "                if use_klcc and kv_cache[0][0].shape[2] >= klcc_cutoff:\n",
        "                    self.train()\n",
        "                    klcc_optimizer = torch.optim.AdamW(self.parameters(), lr=klcc_lr)\n",
        "                    klcc_loss = float('inf')\n",
        "                    while klcc_loss > 1e-2:\n",
        "                        for i in range(0, ((klcc_window_size // klcc_steps_window_size)) - 1):\n",
        "                            klcc_logits, _ = self(idx[:, -(((i+1) * klcc_steps_window_size) + 1):-(((i) * klcc_steps_window_size) + 1)], kv_cache=None)\n",
        "                            klcc_loss = F.kl_div(F.log_softmax(klcc_logits, dim=-1), F.softmax(all_logits[:,  -(((i+1) * klcc_steps_window_size) + 1):-(((i) * klcc_steps_window_size) + 1)].detach(), dim=-1), reduction='batchmean')\n",
        "                            # add L1 regularization\n",
        "                            l1_lambda = 1e-7\n",
        "                            l1_norm = sum(p.abs().sum() for p in self.parameters())\n",
        "                            # klcc_loss_reg = klcc_loss + l1_lambda * l1_norm\n",
        "                            klcc_loss_reg = klcc_loss\n",
        "                            # print(\"KLCC loss:\", klcc_loss.item())\n",
        "                            bar.set_description(f\"KLCC loss: {klcc_loss.item():.4f}\")\n",
        "                            klcc_optimizer.zero_grad(set_to_none=True)\n",
        "                            klcc_loss_reg.backward()\n",
        "                            klcc_optimizer.step()\n",
        "                            if klcc_loss.item() < 1e-2:\n",
        "                                break\n",
        "                    less_kv_cache = []\n",
        "                    for k, v in kv_cache:\n",
        "                        less_k = k[:, :, -klcc_window_size:, :] if k is not None else None\n",
        "                        less_v = v[:, :, -klcc_window_size:, :] if v is not None else None\n",
        "                        less_kv_cache.append((less_k, less_v))\n",
        "                    kv_cache = less_kv_cache\n",
        "                    self.eval()\n",
        "                    all_logits = None\n",
        "                # append sampled index to the running sequence\n",
        "                idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
        "                # since we have kv cache, only need to pass new token\n",
        "                idx_context = idx_next\n",
        "                        \n",
        "            return idx\n",
        "        else:\n",
        "            raise NotImplementedError(\"generate without kv cache is not implemented\")\n",
        "        "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "6339072"
            ]
          },
          "execution_count": 28,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Test forward pass\n",
        "seq_len = 256\n",
        "config = TransformerConfig(\n",
        "    vocab_size=vocab_size,\n",
        "    seq_len=seq_len,\n",
        "    embed_size=256,\n",
        "    head_num=4,\n",
        "    layer_num=6\n",
        ")\n",
        "m = TransformerLM(config)\n",
        "m.to(device)\n",
        "xb, yb = get_batch('train', 5, 1)\n",
        "logits, loss = m(xb, yb)\n",
        "total_params = sum(p.numel() for p in m.parameters() if p.requires_grad)\n",
        "total_params"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 400x300 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "75bdd2ca52f34bea9eb90d5734405503",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/5000 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/coder/Python_project/transformers_playground/.venv/lib/python3.12/site-packages/torch/_inductor/lowering.py:1917: UserWarning: Torchinductor does not support code generation for complex operators. Performance may be worse than eager.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "final loss: 1.0041760206222534 final val loss: 1.1770459413528442\n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 400x300 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# training!\n",
        "model = TransformerLM(config)\n",
        "model = torch.compile(model)\n",
        "model.to(device)\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3)\n",
        "train(model, optimizer, seq_len, batch_size, total_steps)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save\n",
        "# torch.save(model.state_dict(), 'models/TransformerLM.pt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "OptimizedModule(\n",
              "  (_orig_mod): TransformerLM(\n",
              "    (token_embedding_table): Embedding(87, 256)\n",
              "    (lm_head): Linear(in_features=256, out_features=87, bias=False)\n",
              "    (blocks): ModuleList(\n",
              "      (0-5): 6 x Block(\n",
              "        (sa_heads): MultiHeadAttention(\n",
              "          (query): Linear(in_features=256, out_features=256, bias=False)\n",
              "          (key): Linear(in_features=256, out_features=256, bias=False)\n",
              "          (value): Linear(in_features=256, out_features=256, bias=False)\n",
              "          (o): Linear(in_features=256, out_features=256, bias=False)\n",
              "        )\n",
              "        (ff_layer): FeedForward(\n",
              "          (lin_1): Linear(in_features=256, out_features=1024, bias=False)\n",
              "          (lin_2): Linear(in_features=256, out_features=1024, bias=False)\n",
              "          (lin_3): Linear(in_features=1024, out_features=256, bias=False)\n",
              "          (silu): SiLU()\n",
              "        )\n",
              "        (sa_norm): RMSNorm()\n",
              "        (ff_norm): RMSNorm()\n",
              "      )\n",
              "    )\n",
              "  )\n",
              ")"
            ]
          },
          "execution_count": 29,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
            "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
            "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
            "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
          ]
        }
      ],
      "source": [
        "# Load model\n",
        "model = TransformerLM(config)\n",
        "model = torch.compile(model)\n",
        "model.load_state_dict(torch.load('models/TransformerLM.pt'))\n",
        "model.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 153,
      "metadata": {},
      "outputs": [],
      "source": [
        "# # calculate perplexity\n",
        "# ppl, loss = perplexity(model, seq_len, seq_len)\n",
        "# print(\"perplexity:\", ppl, \"loss:\", loss)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 154,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "00r0pbm3b5eX",
        "outputId": "bdd3b34e-32a0-4724-b528-c162c0fd0c3e"
      },
      "outputs": [],
      "source": [
        "# model.eval()\n",
        "# idx = encode(\"You will never\")\n",
        "# print(torch.tensor([idx]))\n",
        "# print(decode(model.generate(idx=torch.tensor([idx], dtype=torch.long).to(device), max_new_tokens=1000, temperature=0.5, use_cache=True)[0].tolist()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([71, 62, 79, 62])"
            ]
          },
          "execution_count": 35,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "torch.tensor(idx)[-5:-1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[53, 72, 78,  1, 80, 66, 69, 69,  1, 71, 62, 79, 62, 75]])\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b9e8b921562f4cdca2d5ac962d112b7d",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/3000 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[22]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      2\u001b[39m idx = encode(\u001b[33m\"\u001b[39m\u001b[33mYou will never\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      3\u001b[39m \u001b[38;5;28mprint\u001b[39m(torch.tensor([idx]))\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m \u001b[38;5;28mprint\u001b[39m(decode(\u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43midx\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlong\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m3000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_klcc\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mklcc_cutoff\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m512\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mklcc_window_size\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m256\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mklcc_steps\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mklcc_lr\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1e-5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mklcc_steps_window_size\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m64\u001b[39;49m\u001b[43m)\u001b[49m[\u001b[32m0\u001b[39m].tolist()))\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 197\u001b[39m, in \u001b[36mTransformerLM.generate\u001b[39m\u001b[34m(self, idx, max_new_tokens, temperature, use_cache, use_klcc, klcc_cutoff, klcc_window_size, klcc_steps, klcc_steps_window_size, klcc_lr)\u001b[39m\n\u001b[32m    194\u001b[39m     all_logits = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    195\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m (bar := tqdm(\u001b[38;5;28mrange\u001b[39m(max_new_tokens))):\n\u001b[32m    196\u001b[39m     \u001b[38;5;66;03m# get the predictions\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m197\u001b[39m     logits, loss = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43midx_context\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkv_cache\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkv_cache\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    198\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m use_klcc:\n\u001b[32m    199\u001b[39m         all_logits = logits \u001b[38;5;28;01mif\u001b[39;00m all_logits \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m torch.cat((all_logits, logits), dim=\u001b[32m1\u001b[39m)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Python_project/transformers_playground/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Python_project/transformers_playground/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 158\u001b[39m, in \u001b[36mTransformerLM.forward\u001b[39m\u001b[34m(self, idx, targets, kv_cache)\u001b[39m\n\u001b[32m    156\u001b[39m \u001b[38;5;66;03m# go through blocks\u001b[39;00m\n\u001b[32m    157\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i, block \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mself\u001b[39m.blocks):\n\u001b[32m--> \u001b[39m\u001b[32m158\u001b[39m     x, cache = \u001b[43mblock\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mkv_cache\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mkv_cache\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    159\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m kv_cache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    160\u001b[39m         kv_cache[i] = cache\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Python_project/transformers_playground/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Python_project/transformers_playground/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 129\u001b[39m, in \u001b[36mBlock.forward\u001b[39m\u001b[34m(self, x, kv_cache)\u001b[39m\n\u001b[32m    128\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, kv_cache=\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m--> \u001b[39m\u001b[32m129\u001b[39m     a, kv_cache = \u001b[38;5;28mself\u001b[39m.sa_heads(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msa_norm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m, kv_cache)\n\u001b[32m    130\u001b[39m     h = x + a\n\u001b[32m    131\u001b[39m     o = h + \u001b[38;5;28mself\u001b[39m.ff_layer(\u001b[38;5;28mself\u001b[39m.ff_norm(h))\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Python_project/transformers_playground/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Python_project/transformers_playground/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 44\u001b[39m, in \u001b[36mRMSNorm.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m     43\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[32m---> \u001b[39m\u001b[32m44\u001b[39m     output = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_norm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfloat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m.type_as(x)\n\u001b[32m     45\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m output * \u001b[38;5;28mself\u001b[39m.weight\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 41\u001b[39m, in \u001b[36mRMSNorm._norm\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m     40\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_norm\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[32m---> \u001b[39m\u001b[32m41\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m x * \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrsqrt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpow\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmean\u001b[49m\u001b[43m(\u001b[49m\u001b[43m-\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeepdim\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43meps\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[31mKeyboardInterrupt\u001b[39m: "
          ]
        }
      ],
      "source": [
        "model.eval()\n",
        "idx = encode(\"You will never\")\n",
        "print(torch.tensor([idx]))\n",
        "print(decode(model.generate(idx=torch.tensor([idx], dtype=torch.long).to(device), max_new_tokens=5000, temperature=0.5, use_cache=True, use_klcc=True, klcc_cutoff=128, klcc_window_size=64, klcc_steps=5, klcc_lr=1e-6, klcc_steps_window_size=16)[0].tolist()))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.11"
    },
    "orig_nbformat": 4
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
